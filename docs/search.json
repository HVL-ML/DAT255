[
  {
    "objectID": "slides/old-11-tabular-data.html#this-week",
    "href": "slides/old-11-tabular-data.html#this-week",
    "title": "DAT255: Deep learning engineering",
    "section": "This week",
    "text": "This week\n\n\n\nToday:\n\nDeep learning on tabular data\nData preprocessing with Keras\n\nFeature scaling\nConverting text and categorical values\nQuick look at embeddings\n\n\nThursday:\n\nCustom Keras objects\nSome ML experiment monitoring tools"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#tabular-data",
    "href": "slides/old-11-tabular-data.html#tabular-data",
    "title": "DAT255: Deep learning engineering",
    "section": "Tabular data",
    "text": "Tabular data\nData that makes sense to put in a table:\n\n\n\n\n\n\n\n\n\n\n\npatient_id\nage\nvisits\nblood_type\ndiag_code\nsymptoms\n\n\n\n\n321\n28\n1\n“A”\nnone\n“headache, fatigue”\n\n\n602\n64\n4\n“AB”\n32, 12\n“complains about headache, joint pains”\n\n\n201\n62\n2\n“0”\n12\n“dizzyness, headache”\n\n\n491\n57\n1\n“A”\n6\n“headache, fever”\n\n\n\n\nCompare to e.g. images and time series:"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#feature-normalisation",
    "href": "slides/old-11-tabular-data.html#feature-normalisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Feature normalisation",
    "text": "Feature normalisation\nConsider optimisation where\n\nFeatures have similar scale\nFeatures have different scale"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#feature-normalisation-1",
    "href": "slides/old-11-tabular-data.html#feature-normalisation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Feature normalisation",
    "text": "Feature normalisation\nConsider optimisation where\n\nFeatures have similar scale\nFeatures have different scale\n\nRemember also relation between input and output variances:"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#feature-normalisation-2",
    "href": "slides/old-11-tabular-data.html#feature-normalisation-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Feature normalisation",
    "text": "Feature normalisation\n\n\n\nNormalisation:\nMake features look like a normal distribution with mean = 0 and variance = 1\nUsual approach:\nimport numpy as np\nx_normalised = (x - np.mean(x)) / np.std(x)\n\n\n\nProvided by\n\nsklearn.preprocessing.StandardScaler()\nkeras.layers.Normalization()"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#feature-transformations",
    "href": "slides/old-11-tabular-data.html#feature-transformations",
    "title": "DAT255: Deep learning engineering",
    "section": "Feature transformations",
    "text": "Feature transformations"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#dealing-with-text-and-categorical-data",
    "href": "slides/old-11-tabular-data.html#dealing-with-text-and-categorical-data",
    "title": "DAT255: Deep learning engineering",
    "section": "Dealing with text and categorical data",
    "text": "Dealing with text and categorical data\n\n\n\n\n\n\n\n\n\n\n\npatient_id\nage\nvisits\nblood_type\ndiag_code\nsymptoms\n\n\n\n\n321\n28\n1\n“A”\nnone\n“headache, fatigue”\n\n\n602\n64\n4\n“AB”\n32, 12\n“complains about headache, joint pains”\n\n\n201\n62\n2\n“0”\n12\n“dizzyness, headache”\n\n\n491\n57\n1\n“A”\n6\n“headache, fever”\n\n\n\nEncoding categorical data, like blood_type:\n\n\n\n\nOption 1: Ordinal encoding\n{\"0\": 0, \"A\": 1, \"B\": 2, \"AB\": 3}\n-&gt; imposes an ordering which may not be meaningful\n\n\n\n\n\nOption 2: “One-hot” encoding\n# patient_id    0  A  B  AB\n  321         [ 0  1  0  0 ]\n  602         [ 0  0  0  1 ]\n  201         [ 1  0  0  0 ]\n  491         [ 0  1  0  0 ]\n-&gt; no implicit assumption about ordering"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#dealing-with-text-and-categorical-data-1",
    "href": "slides/old-11-tabular-data.html#dealing-with-text-and-categorical-data-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Dealing with text and categorical data",
    "text": "Dealing with text and categorical data\n\n\n\n\n\n\n\n\n\n\n\npatient_id\nage\nvisits\nblood_type\ndiag_code\nsymptoms\n\n\n\n\n321\n28\n1\n“A”\nnone\n“headache, fatigue”\n\n\n602\n64\n4\n“AB”\n32, 12\n“complains about headache, joint pains”\n\n\n201\n62\n2\n“0”\n12\n“dizzyness, headache”\n\n\n491\n57\n1\n“A”\n6\n“headache, fever”\n\n\n\nEncoding categorical data with vector entries, like diag_code:\n\n\n\n\nOption 1: Ordinal encoding, with one category for each combination\n{\"none\": 0, \"32, 12\": 1, \"12\": 2, \"6\": 3}\n-&gt; assumes combination is distinct from the separate entries\n\n\n\n\n\nOption 2: “One-hot” encoding\n# patient_id   32  12   6\n  321         [ 0   0   0 ]\n  602         [ 1   1   0 ]\n  201         [ 0   1   0 ]\n  491         [ 0   0   1 ]\n-&gt; model will have to learn correlations between columns"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#dealing-with-text-data",
    "href": "slides/old-11-tabular-data.html#dealing-with-text-data",
    "title": "DAT255: Deep learning engineering",
    "section": "Dealing with text data",
    "text": "Dealing with text data\nSophisticated text processing will be the topic for next week\nFor “simple” text we can extend the one-hot encoding scheme and to feature hashing:\n\n\n\n\n\n\n\n\n\n\n\npatient_id\n…\nsymptoms\n\n\n\n\n321\n\n“headache, fatigue”\n\n\n602\n\n“complains about headache, joint pains”\n\n\n201\n\n“dizzyness, headache”\n\n\n491\n\n“headache, fever”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheadache\nfatigue\ncomplains\nabout\njoint\npains\ndizzyness\nfever\n\n\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n1\n1\n1\n1\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data",
    "href": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data",
    "title": "DAT255: Deep learning engineering",
    "section": "Keras convenience layers for categorical data",
    "text": "Keras convenience layers for categorical data\nEncoding integers, when knowing how many categories exist: CategoryEncoding\n\n\n\n&gt;&gt;&gt; layer = keras.layers.CategoryEncoding(num_tokens=3, output_mode=\"one_hot\")\n&gt;&gt;&gt; data = [2,0,1]\n&gt;&gt;&gt; layer(data)\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])\nEncoding data that can be vectors of integers:\n\n\n\n&gt;&gt;&gt; layer = keras.layers.CategoryEncoding(num_tokens=3, output_mode=\"one_hot\")\n&gt;&gt;&gt; data = tf.ragged.constant([[2],[0,1],[0,1,2]])  # uequal lengths -&gt; use tf.ragged\n&gt;&gt;&gt; layer(data)\narray([[0., 0., 1.],\n       [1., 1., 0.],\n       [1., 1., 1.]])"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data-1",
    "href": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Keras convenience layers for categorical data",
    "text": "Keras convenience layers for categorical data\nEncoding (single) strings: StringLookup\n(For sentences we will use TextVectorization next week)\nCan either provide the list of known strings (aka the vocabulary), of learn it from data:\n\n\n\n&gt;&gt;&gt; layer = keras.layers.StringLookup()\n&gt;&gt;&gt; data = ['a', 'b', 'c']\n&gt;&gt;&gt; layer.adapt(data)\n&gt;&gt;&gt; layer.get_vocabulary()\n['[UNK]', 'c', 'b', 'a']\n&gt;&gt;&gt; layer(data)\narray([3, 2, 1])\n&gt;&gt;&gt; other_data = ['b', 'c', 'd']\n&gt;&gt;&gt; layer(other_data)\narray([2, 1, 0])\nOut-Of-Vocabulary (OOV) gets mapped to [UNK], meaning unknown"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data-2",
    "href": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Keras convenience layers for categorical data",
    "text": "Keras convenience layers for categorical data\nCan also use the same approach for integers, when we don’t know how many categories exist: IntegerLookup\n\n\n\n&gt;&gt;&gt; layer = keras.layers.IntegerLookup()\n&gt;&gt;&gt; data = [12, 14, 46]\n&gt;&gt;&gt; layer.adapt(data)\n&gt;&gt;&gt; layer.get_vocabulary()\n[-1, np.int64(46), np.int64(14), np.int64(12)]\n&gt;&gt;&gt; layer([14, 12, 317])\n&lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 3, 0])&gt;\n\n\n\nOut-Of-Vocabulary (OOV) numbers get mapped to -1\nCan also have multiple OOV tokens. Check the documentation"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data-3",
    "href": "slides/old-11-tabular-data.html#keras-convenience-layers-for-categorical-data-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Keras convenience layers for categorical data",
    "text": "Keras convenience layers for categorical data\nOften your dataset will contain a mix of diffent types, requiring different preprocessing.\nA do-it-all solution is to use keras.utils.FeatureSpace. From example in docs:\nfeature_space = FeatureSpace(\n    features={\n        # Categorical features encoded as integers\n        \"fbs\": \"integer_categorical\",\n        \"restecg\": \"integer_categorical\",\n        # Categorical feature encoded as string\n        \"thal\": \"string_categorical\",\n        # Numerical features to discretize\n        \"age\": \"float_discretized\",\n        # Numerical features to normalize\n        \"trestbps\": \"float_normalized\",\n        \"chol\": \"float_normalized\",\n        \"thalach\": \"float_normalized\",\n    },\n    # Our utility will one-hot encode all categorical\n    # features and concat all features into a single\n    # vector (one vector per sample).\n    output_mode=\"concat\",\n)\n# Adapt to training data\nfeature_space.adapt(train_ds)"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#embeddings",
    "href": "slides/old-11-tabular-data.html#embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings\nProblem:\nIf one-hot encoding many categorical features, each with many categories, we get very many new feature columns\n-&gt; Our data becomes a sparse matrix, meaning most of the elements are zero.\nIs there a more efficient way to encode our data?\n\nEnter embeddings:\nLet’s create a lookup table that maps categorical entries to numerical vectors\n\n\nInstead of\n{\"0\": 0, \"A\": 1, \"B\": 2, \"AB\": 3}\nwe do (for instance)\n{\"0\": [0.2, 1.1], \"A\": [1.6, -0.2], \"B\": [-0.3, 0.7], \"AB\": [1.4, -0.1]}"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#embeddings-1",
    "href": "slides/old-11-tabular-data.html#embeddings-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings\nNow the magic✨ is:\nThe embeddings are learnt from data\n# These values are updated during training\n# You can have any dimension you like\n{\"0\": [0.2, 1.1], \"A\": [1.6, -0.2], \"B\": [-0.3, 0.7], \"AB\": [1.4, -0.1]}\nWe can treat the embedding space as an Euclidian space where elements close to another are similar or somehow related.\n\nFor categorical data we call it entity embeddings (while next week we’ll look at word embeddings)\nIn Keras:\nkeras.layers.Embedding(input_dim, output_dim)\n\ninput_dim: length of vocabulary\noutput_dim: dimensions of embedding space (you choose)"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data",
    "href": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data",
    "title": "DAT255: Deep learning engineering",
    "section": "Network architectures for tabular data",
    "text": "Network architectures for tabular data\n\n\n\n\n\nMost common: Straight-up Dense network\nBenefits from:\n\nFeature engineering\nEnsembling\n\n\n\n\nTypically the data rows and columns have no specific ordering or neighbouring relation -&gt; convolutions or recurrence make little sense."
  },
  {
    "objectID": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data-1",
    "href": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Network architectures for tabular data",
    "text": "Network architectures for tabular data\n\n\n\nA variation: Wide and deep network"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data-2",
    "href": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Network architectures for tabular data",
    "text": "Network architectures for tabular data\nModern models are often based on the Transformer architecture\n\n\n\n\n\nhttps://arxiv.org/pdf/2410.12034"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data-3",
    "href": "slides/old-11-tabular-data.html#network-architectures-for-tabular-data-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Network architectures for tabular data",
    "text": "Network architectures for tabular data\nA transformer model for tabular data: TabTransformer\n\n\n\n\n\n\n\n\n\nTabTransformer significantly outperforms MLP and recent deep networks for tabular data while matching the performance of tree-based ensemble models (GBDT).\n\n\n\n\nhttps://arxiv.org/pdf/2012.06678"
  },
  {
    "objectID": "slides/old-11-tabular-data.html#the-best-model-for-tabular-data",
    "href": "slides/old-11-tabular-data.html#the-best-model-for-tabular-data",
    "title": "DAT255: Deep learning engineering",
    "section": "The best model for tabular data",
    "text": "The best model for tabular data\n\nhttps://arxiv.org/pdf/2207.08815"
  },
  {
    "objectID": "slides/old-09-gradients.html#topics-for-today",
    "href": "slides/old-09-gradients.html#topics-for-today",
    "title": "DAT255: Deep learning engineering",
    "section": "Topics for today",
    "text": "Topics for today\nSplit topics for this lecture:\n\n\n\n\nTime series\n\nA different time series forecasting task: Anomaly detection\nModern sequence prediction models: A peek on the Transformer\n\n\n\n\n\n\nInvestigating models though gradients\n\nExplaining predictions\nHacking machine learning models ☠️"
  },
  {
    "objectID": "slides/old-09-gradients.html#anomaly-detection",
    "href": "slides/old-09-gradients.html#anomaly-detection",
    "title": "DAT255: Deep learning engineering",
    "section": "Anomaly detection",
    "text": "Anomaly detection\nLet’s have a look at an self-supervised machine learning task:\nDetecting anomalies or outliers.\n\n\n\nSetting: Have either\n\nfew examples, or\nno examples\n\nof the positive class."
  },
  {
    "objectID": "slides/old-09-gradients.html#anomaly-detection-1",
    "href": "slides/old-09-gradients.html#anomaly-detection-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Anomaly detection",
    "text": "Anomaly detection\nApproach:\nTrain a model to reconstruct normal data.\n\nNeed an encoder-decoder type model, for instance an autoencoder:"
  },
  {
    "objectID": "slides/old-09-gradients.html#autoencoders",
    "href": "slides/old-09-gradients.html#autoencoders",
    "title": "DAT255: Deep learning engineering",
    "section": "Autoencoders",
    "text": "Autoencoders\nAutoencoders are trained to reconstruct their input \\(\\boldsymbol{x}\\), i.e. approximate a composite function \\(g(f(\\boldsymbol{x}))\\) so that\n\\[\n\\small\n\\begin{align}\n\\boldsymbol{x} - \\boldsymbol{\\hat{x}} &= \\\\\n\\boldsymbol{x}- g(f(\\boldsymbol{x, \\theta}), \\boldsymbol{\\theta'}) &\\to 0\n\\end{align}\n\\]\n\n\\(f\\) learns to compress data into an efficient representation\n\\(g\\) learns to do the inverse operation (decompress)\n\nNo target \\(y\\) involved!\n\nUsually choose the loss function to be mean squared error, but this depends on the task."
  },
  {
    "objectID": "slides/old-09-gradients.html#autoencoder-applications",
    "href": "slides/old-09-gradients.html#autoencoder-applications",
    "title": "DAT255: Deep learning engineering",
    "section": "Autoencoder applications",
    "text": "Autoencoder applications\n\n\nCompression:\nRun encoder \\(f\\) Transmit encoded representation  Run decoder \\(g\\), recover data\n\n\n\n\nDenoising:\nEncoder looks for learnt patterns in noisy data (encoded in \\(\\boldsymbol{\\theta}\\)) Decoder reconstructs learnt patterns (encoded in \\(\\boldsymbol{\\theta'}\\))\n\n\n\n\nAnomaly detection:\nEncoder looks for learnt patterns Can’t find any Decoder doesn’t know what to do??"
  },
  {
    "objectID": "slides/old-09-gradients.html#anomaly-detection-with-autoencoders",
    "href": "slides/old-09-gradients.html#anomaly-detection-with-autoencoders",
    "title": "DAT255: Deep learning engineering",
    "section": "Anomaly detection with autoencoders",
    "text": "Anomaly detection with autoencoders\n\n\nSince we know the true \\(\\boldsymbol{x}\\), we can estimate how “surprising” it is by comparing to the reconstructed \\(\\boldsymbol{\\hat{x}}\\)\n(basically use models as test if it looks like training data)\n\nContains expected features -&gt; Good reconstruction (\\(|\\boldsymbol{x}-\\boldsymbol{\\hat{x}}|\\) small)\nContains unexpected features -&gt; Poor reconstruction (\\(|\\boldsymbol{x}-\\boldsymbol{\\hat{x}}|\\) large)\n\n\n\n\n\n\nMSE = 0.002\n\n\nMSE = 0.003\n\n\nMSE = 0.001\n\n\nMSE = 0.026\n\n\nMSE = 0.002"
  },
  {
    "objectID": "slides/old-09-gradients.html#anomaly-detection-for-time-series",
    "href": "slides/old-09-gradients.html#anomaly-detection-for-time-series",
    "title": "DAT255: Deep learning engineering",
    "section": "Anomaly detection for time series",
    "text": "Anomaly detection for time series\nNotebook for this week:\nBuild an encoder-decoder CNN model for 1D data\n\n\n\n\n\n\nNormal data\n\n\n\n\n\n\n\nAnomalous data"
  },
  {
    "objectID": "slides/old-09-gradients.html#anomaly-detection-for-time-series-1",
    "href": "slides/old-09-gradients.html#anomaly-detection-for-time-series-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Anomaly detection for time series",
    "text": "Anomaly detection for time series\nNotebook for this week:\nBuild an encoder-decoder CNN model for 1D data\n\n\n\n\n\n\nNormal data\n\n\n\n\n\n\n\nAnomalous data"
  },
  {
    "objectID": "slides/old-09-gradients.html#modern-networks-for-sequences-and-time-series",
    "href": "slides/old-09-gradients.html#modern-networks-for-sequences-and-time-series",
    "title": "DAT255: Deep learning engineering",
    "section": "Modern networks for sequences and time series",
    "text": "Modern networks for sequences and time series\nRecurrent networks:\n\n+ Maintain context by storing and updating an internal state\n\n\n- Difficult to store entire contect in a fixed-length state vector (\\(\\boldsymbol{z}^{\\ast}\\))\n\n\n- Sequential processing (for loops) slow down training"
  },
  {
    "objectID": "slides/old-09-gradients.html#modern-networks-for-sequences-and-time-series-1",
    "href": "slides/old-09-gradients.html#modern-networks-for-sequences-and-time-series-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Modern networks for sequences and time series",
    "text": "Modern networks for sequences and time series\nA more powerful and scalable approach is the Transformer model"
  },
  {
    "objectID": "slides/old-09-gradients.html#peek-on-transformers",
    "href": "slides/old-09-gradients.html#peek-on-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Peek on transformers",
    "text": "Peek on transformers\nThe core of the transformer architecture is the attention mechanism\n\nLearn to give different weights to different inputs\nThese weights thmeselves depend on the input values"
  },
  {
    "objectID": "slides/old-09-gradients.html#transformer-tasks",
    "href": "slides/old-09-gradients.html#transformer-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformer tasks",
    "text": "Transformer tasks\nCan use the encoder-decoder structure in parts, or combined:\n\nEncoder:\n\nTake a sequence as input, and output fixed-length vectors (like class labels)"
  },
  {
    "objectID": "slides/old-09-gradients.html#transformer-tasks-1",
    "href": "slides/old-09-gradients.html#transformer-tasks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformer tasks",
    "text": "Transformer tasks\nCan use the encoder-decoder structure in parts, or combined:\nEncoder:\n\nTake a sequence as input, and output fixed-length vectors (like class labels)\n\nDecoder:\n\nTake a sequence as input, then produce next element in sequence, in an autoregressive fashion\n(ChatGPT and friends)"
  },
  {
    "objectID": "slides/old-09-gradients.html#transformer-tasks-2",
    "href": "slides/old-09-gradients.html#transformer-tasks-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformer tasks",
    "text": "Transformer tasks\nCan use the encoder-decoder structure in parts, or combined:\nEncoder:\n\nTake a sequence as input, and output fixed-length vectors (like class labels)\n\nDecoder:\n\nTake a sequence as input, then produce next element in sequence, in an autoregressive fashion\n(ChatGPT and friends)\n\nEncoder-decoder (sequence-to-sequence transformer):\n\nUseful for translation tasks, but compute intensive"
  },
  {
    "objectID": "slides/old-09-gradients.html#using-gradients",
    "href": "slides/old-09-gradients.html#using-gradients",
    "title": "DAT255: Deep learning engineering",
    "section": "Using gradients",
    "text": "Using gradients"
  },
  {
    "objectID": "slides/old-09-gradients.html#recap-optimisation-with-gradients",
    "href": "slides/old-09-gradients.html#recap-optimisation-with-gradients",
    "title": "DAT255: Deep learning engineering",
    "section": "Recap: Optimisation with gradients",
    "text": "Recap: Optimisation with gradients\nQuestion:\nIf we vary the parameters \\(\\boldsymbol{\\theta}\\) of model, how does it affect the predictions?\n\n\n\n\n\nQuantify this by computing the gradient of the loss \\(L\\) with respect to the model parameters \\(\\boldsymbol{\\theta}\\):\n\n\\[\n\\nabla \\color{MediumVioletRed}{L}(\\color{teal}{\\boldsymbol{\\theta}}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_0} \\\\\n  \\vdots \\\\\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_n} \\\\\n\\end{bmatrix}\n\\]\nBasically: Vary \\(\\theta\\)  check effect on \\(L\\)"
  },
  {
    "objectID": "slides/old-09-gradients.html#gradients-explainability",
    "href": "slides/old-09-gradients.html#gradients-explainability",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradients: explainability",
    "text": "Gradients: explainability\nLet’s turn it around and ask a different question:\nIf we vary the data \\(\\boldsymbol{x}\\), how does it affect the prediction?\n\n\n\n\nCan use this for per-datapoint explanations of model decision.\nCompute then the gradient of the model output \\(S\\) with respect to input data \\(x\\)."
  },
  {
    "objectID": "slides/old-09-gradients.html#gradients-explainability-1",
    "href": "slides/old-09-gradients.html#gradients-explainability-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradients: explainability",
    "text": "Gradients: explainability\nExample:\n\n\nFor an image classifier we have two classes, cat and dog. Differentiating the score for cat, \\(S_c\\), with respect to the pixel values \\(x_1, \\dots, x_n\\), gives an attribution of each pixels’ importance to the classification\n\\[\n\\small\n\\nabla S_c (\\boldsymbol{x}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\color{MediumVioletRed}{S_c}}{\\partial \\color{teal}{x}_0} \\\\\n  \\vdots \\\\\n  \\frac{\\partial \\color{MediumVioletRed}{S_c}}{\\partial \\color{teal}{x}_n} \\\\\n\\end{bmatrix}\n\\leftarrow \\mathrm{attribution\\; map}\n\\]\n\n\n\n\n\n\nCat\n\n\n\n\n\n\n\nPixel attributions to classification score"
  },
  {
    "objectID": "slides/old-09-gradients.html#gradients-explainability-2",
    "href": "slides/old-09-gradients.html#gradients-explainability-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradients: explainability",
    "text": "Gradients: explainability\nExample:\n\n\nFor an image classifier we have two classes, cat and dog. Differentiating the score for cat, \\(S_c\\), with respect to the pixel values \\(x_1, \\dots, x_n\\), gives an attribution of each pixels’ importance to the classification\n\\[\n\\small\n\\nabla S_c (\\boldsymbol{x}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\color{MediumVioletRed}{S_c}}{\\partial \\color{teal}{x}_0} \\\\\n  \\vdots \\\\\n  \\frac{\\partial \\color{MediumVioletRed}{S_c}}{\\partial \\color{teal}{x}_n} \\\\\n\\end{bmatrix}\n\\leftarrow \\mathrm{attribution\\; map}\n\\]"
  },
  {
    "objectID": "slides/old-09-gradients.html#attribution-methods",
    "href": "slides/old-09-gradients.html#attribution-methods",
    "title": "DAT255: Deep learning engineering",
    "section": "Attribution methods",
    "text": "Attribution methods\nTwo problems with using the (only) the gradient:\n\nGradients looks rather noisy\nIf using ReLU and output activations are 0, the gradient is also 0.\n\n\n\nSome solutions:\n\nAdd noise and average: SmoothGrad"
  },
  {
    "objectID": "slides/old-09-gradients.html#attribution-methods-1",
    "href": "slides/old-09-gradients.html#attribution-methods-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Attribution methods",
    "text": "Attribution methods\nTwo problems with using the (only) the gradient:\n\nGradients looks rather noisy\nIf using ReLU and output activations are 0, the gradient is also 0.\n\n\nSome solutions:\n\nAdd noise and average: SmoothGrad\nMake a baseline and integrate: Integrated gradients"
  },
  {
    "objectID": "slides/old-09-gradients.html#attribution-methods-2",
    "href": "slides/old-09-gradients.html#attribution-methods-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Attribution methods",
    "text": "Attribution methods\nTwo problems with using the (only) the gradient:\n\nGradients looks rather noisy\nIf using ReLU and output activations are 0, the gradient is also 0.\n\n\nSome solutions:\n\nBe clever about how gradients are propagated through the activation functions: DeepLift, Deconvolution, Layerwise relevance propagation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractiveheatmapping"
  },
  {
    "objectID": "slides/old-09-gradients.html#adversarial-attacks",
    "href": "slides/old-09-gradients.html#adversarial-attacks",
    "title": "DAT255: Deep learning engineering",
    "section": "Adversarial attacks",
    "text": "Adversarial attacks\nTwo observations from our pixel attributions:\n\nGradients look noisy\nA few pixels have very large gradients (and thereby a large impact on the classification)"
  },
  {
    "objectID": "slides/old-09-gradients.html#adversarial-attacks-1",
    "href": "slides/old-09-gradients.html#adversarial-attacks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Adversarial attacks",
    "text": "Adversarial attacks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal image\n\n\nPerturbation\n\n\nPerturbed image\n\n\nPrediction\n\n\nOstrich\n\n\nOstrich\n\n\nOstrich\n\n\n\n\n\nOstrich\n\n\nOstrich\n\n\nOstrich"
  },
  {
    "objectID": "slides/old-09-gradients.html#constructing-adversarial-examples",
    "href": "slides/old-09-gradients.html#constructing-adversarial-examples",
    "title": "DAT255: Deep learning engineering",
    "section": "Constructing adversarial examples",
    "text": "Constructing adversarial examples\nFrom the gradient \\(\\nabla S_c\\) we know which pixels were important to make the correct prediction\n Adjust their values in the opposite direction\n\n\n\n\n\n(Many sophisticated variants exist)"
  },
  {
    "objectID": "slides/old-09-gradients.html#irl-adversarial-examples",
    "href": "slides/old-09-gradients.html#irl-adversarial-examples",
    "title": "DAT255: Deep learning engineering",
    "section": "IRL adversarial examples",
    "text": "IRL adversarial examples\n\n\n\n\n\n\n\n\n\nPrediction:Speed limit 45\n\n\n\n\n\n\n\nPrediction:Speed limit 45\n\n\n\n\n\n\n\nPrediction:Speed limit 45\n\n\n\n\n\n\n\nPrediction:Speed limit 45"
  },
  {
    "objectID": "slides/old-09-gradients.html#adversarial-attacks-2",
    "href": "slides/old-09-gradients.html#adversarial-attacks-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Adversarial attacks",
    "text": "Adversarial attacks\nTargeted attacks need access to the model on order to compute gradients\nBlack box attacks: Constructing adversarial examples purely by analysing outputs for given inputs (more difficult)\n\nSome defense strategies:\n\nTraining on adversarial examples\nEnsembling\nAugmentation"
  },
  {
    "objectID": "slides/old-07-timeseries.html#sequences",
    "href": "slides/old-07-timeseries.html#sequences",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequences",
    "text": "Sequences\nSequences are ordered series. For instance\n\n\n\nNatural language\nI only drink coffee (\\(\\neq\\) only I drink coffee)\n\n\n\nTime series\n\n\n\nTime\n11:00\n12:00\n13:00\n14:00\n15:00\n\n\nTemp\n7°C\n8°C\n10°C\n12 °C\n12 °C\n\n\n\n\n\n\n\nAudio\n\n\n\n\n\n\nVideo\nVideo\n\n\n\n\nDNA"
  },
  {
    "objectID": "slides/old-07-timeseries.html#sequence-prediction-tasks",
    "href": "slides/old-07-timeseries.html#sequence-prediction-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence prediction tasks",
    "text": "Sequence prediction tasks\n\n\n\n\nClassification:\n\nSpeech recognition\nFraud detection, network intrusion detection\nFault detection and predictive maintenance\nMedical diagnostics\nSentiment analysis\nTopic classification\n\n\n We already know (most of) the tools needed\n\n\n\n\nForecasting (regression of future values)\n\nPredicting weather, energy prices, stock prices\nText generation\n\n\n Need a model that can remember the past\n\n\n\nSequence-to-sequence learning\n\nLanguage translation\nImage captioning\nText summarisation\n\n\n Need a model that can remember the context"
  },
  {
    "objectID": "slides/old-07-timeseries.html#sequence-classification",
    "href": "slides/old-07-timeseries.html#sequence-classification",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence classification",
    "text": "Sequence classification\n\nFor images we looked for patterns between neighbouring pixels, in 2D\nFor sequences we equivalently look for patterns between neighbouring elements (e.g. points in time), in 1D\n\nWhile 1D, we can still have multiple channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2D (images)\n\n1D (sequences)\n\n\n\n\nkeras.layers.Conv2D\n-&gt;\nkeras.layers.Conv1D\n\n\nkeras.layers.Conv2DTranspose\n-&gt;\nkeras.layers.Conv1DTranspose\n\n\nkeras.layers.MaxPooling2D\n-&gt;\nkeras.layers.MaxPooling1D"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-recap",
    "href": "slides/old-07-timeseries.html#convolutions-recap",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-recap-1",
    "href": "slides/old-07-timeseries.html#convolutions-recap-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-recap-2",
    "href": "slides/old-07-timeseries.html#convolutions-recap-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-recap-3",
    "href": "slides/old-07-timeseries.html#convolutions-recap-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-recap-4",
    "href": "slides/old-07-timeseries.html#convolutions-recap-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-recap-5",
    "href": "slides/old-07-timeseries.html#convolutions-recap-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap",
    "text": "Convolutions recap"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-detour",
    "href": "slides/old-07-timeseries.html#convolutions-detour",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions detour",
    "text": "Convolutions detour\nIf the take the convolution operation\n\\[\n\\small\n(f \\ast g)(t) \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\n\\]\nbut reverse one of the functions (\\(\\small f(t) \\to f(-t)\\)), we get the similar operation called cross-correlation:\n\n\\[\n\\small\nf \\star g \\equiv f(-t) \\ast g(t)\n\\]"
  },
  {
    "objectID": "slides/old-07-timeseries.html#convolutions-detour-1",
    "href": "slides/old-07-timeseries.html#convolutions-detour-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions detour",
    "text": "Convolutions detour\nIf the take the convolution operation\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\n\\]\nbut reverse one of the functions (\\(\\small f(t) \\to f(-t)\\)), we get the similar operation called cross-correlation:\n\n\\[\n\\small\nf \\star g \\equiv f(-t) \\ast g(t)\n\\]"
  },
  {
    "objectID": "slides/old-07-timeseries.html#sequencing-forecasting-predicting-the-future",
    "href": "slides/old-07-timeseries.html#sequencing-forecasting-predicting-the-future",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequencing forecasting: Predicting the future",
    "text": "Sequencing forecasting: Predicting the future\nCNNs are great for classification because of translation invariance\nFor forecasting, we typically don’t want this.\n\nNew assumption:\n\nRecent data is more informative than old data"
  },
  {
    "objectID": "slides/old-07-timeseries.html#recurrent-neural-networks-rnns",
    "href": "slides/old-07-timeseries.html#recurrent-neural-networks-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way:\nLet each node store its previous output"
  },
  {
    "objectID": "slides/old-07-timeseries.html#recurrent-neural-networks-rnns-1",
    "href": "slides/old-07-timeseries.html#recurrent-neural-networks-rnns-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way:\nLet each node store its previous output"
  },
  {
    "objectID": "slides/old-07-timeseries.html#recurrent-neural-networks-rnns-2",
    "href": "slides/old-07-timeseries.html#recurrent-neural-networks-rnns-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nRecall that a regular Dense layer computes its output by\noutputs = activation(tf.dot(W, inputs) + b)\n(where W is the weight matrix, inputs is the vector of features and b is the bias vector)\n\nThe recurrent node has two sets of weights:\n\nThe usual ones, call them W_x\nThose to be applied to the previous output, call them W_y\n\n\n\nThe outputs then become\nstate_t = tf.zeros(shape=(num_output_features))\noutputs = []\nfor input_t in input_sequence:  # loop over inputs at time t\n  output_t = activation(tf.dot(W_y, inputs) + tf.dot(W_y, state_t) + b)\n  outputs.append(output_t)\n  state_t = output_t\noutput_sequence = tf.stack(outputs, axis=0)\nImplemented in Keras as keras.layers.SimpleRNN"
  },
  {
    "objectID": "slides/old-07-timeseries.html#input-and-output-sequences",
    "href": "slides/old-07-timeseries.html#input-and-output-sequences",
    "title": "DAT255: Deep learning engineering",
    "section": "Input and output sequences",
    "text": "Input and output sequences"
  },
  {
    "objectID": "slides/old-07-timeseries.html#intermezzo-autoregressive-models",
    "href": "slides/old-07-timeseries.html#intermezzo-autoregressive-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Intermezzo: Autoregressive models",
    "text": "Intermezzo: Autoregressive models\nSimplest possible forecast:\nThe value tomorrow is the same as the value today. \\[\n\\small\ny_i = y_{t-1}\n\\]\n\n\n\n\n\nNumber of rail and bus passengers in Chicago 2019\n\n\n\n\n\n\n\nPartial autocorrelation"
  },
  {
    "objectID": "slides/old-07-timeseries.html#intermezzo-autoregressive-models-1",
    "href": "slides/old-07-timeseries.html#intermezzo-autoregressive-models-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Intermezzo: Autoregressive models",
    "text": "Intermezzo: Autoregressive models\n\n\nMore advanced forecast:\nThe value tomorrow is given by a weighted sum of the \\(p\\) previous time steps, plus a noise term\n\\[\n\\small\ny_i = \\sum^p \\varphi_i y_{t-i} + \\epsilon_t\n\\]\n(\\(\\varphi_i\\) are the parameters of the model)\n\n\n\n\nCan add moving average to get an ARMA model, look at differences to get ARIMA, add seasonality to get SARIMA, …\nLots of work on (traditional) statistical time series modelling - usually worth trying out before going to deep learning."
  },
  {
    "objectID": "slides/old-07-timeseries.html#improved-memory-cells",
    "href": "slides/old-07-timeseries.html#improved-memory-cells",
    "title": "DAT255: Deep learning engineering",
    "section": "Improved memory cells",
    "text": "Improved memory cells\nIn practice, RNNs suffer from vanishing/exploding gradients during training\n Difficult to make them learn long-term dependencies\n\nCan introduce hidden states which are not the same as the output.\n\n\n\n\n\nTwo most used approaces: LSTMs and GRUs."
  },
  {
    "objectID": "slides/old-07-timeseries.html#the-long-short-term-memory-lstm-cell",
    "href": "slides/old-07-timeseries.html#the-long-short-term-memory-lstm-cell",
    "title": "DAT255: Deep learning engineering",
    "section": "The long short-term memory (LSTM) cell",
    "text": "The long short-term memory (LSTM) cell\nAdd long-term memory by having two states in each cell:\nA short-term state \\(\\small\\boldsymbol{h}_t\\) and a long-term state \\(\\small\\boldsymbol{c}_t\\)\n\n\n\n\n\nGates determine data flow – add small networks inside the cell to act as gate operators\n\nkeras.layers.LSTM"
  },
  {
    "objectID": "slides/old-07-timeseries.html#the-gated-recurrent-unit-gru",
    "href": "slides/old-07-timeseries.html#the-gated-recurrent-unit-gru",
    "title": "DAT255: Deep learning engineering",
    "section": "The gated recurrent unit (GRU)",
    "text": "The gated recurrent unit (GRU)\nSimplified and somewhat more effective variant:\n\n\n\n\n\nkeras.layers.GRU"
  },
  {
    "objectID": "slides/old-07-timeseries.html#stacking-recurrent-layers",
    "href": "slides/old-07-timeseries.html#stacking-recurrent-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Stacking recurrent layers",
    "text": "Stacking recurrent layers\nAs usual, we can increase the capacity by stacking layers.\nNote when building a deep RNN: Intermediate layers should return the entire sequence\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.GRU(32, return_sequences=True)(inputs)\nx = layers.GRU(32, return_sequences=True)(x)\nx = layers.GRU(32)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/old-07-timeseries.html#training-rnns",
    "href": "slides/old-07-timeseries.html#training-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Training RNNs",
    "text": "Training RNNs\nSome tricks to efficiently training recurrent networks:\n\n\nUse saturating activation functions (tanh, sigmoid)\nlayers.LSTM(units, activation=\"tanh\", recurrent_activation=\"sigmoid\")\nUse layer normalisation (keras.layers.LayerNormalization) instead of batch normalisation\nAdd recurrent dropout (potentially in addition to regular dropout)\nx = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n\n\n\n\nTest if training runs faster on CPU than on GPU\n\nNVIDIA backend only available if using default arguments for LSTM/GRU layers\nfor loops in recurrent nodes reduces parallelisability\n\nCan optionally unroll for loops (memory intensive):\nx = layers.LSTM(32, unroll=True)(inputs)"
  },
  {
    "objectID": "slides/old-07-timeseries.html#bonus-trick-1-cnn-processing",
    "href": "slides/old-07-timeseries.html#bonus-trick-1-cnn-processing",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #1: CNN processing",
    "text": "Bonus trick #1: CNN processing\nEven with the previous tricks up our sleeve, getting RNNs to learn patterns over &gt;100 time steps is difficult.\nCan extract small-scale patterns with convolutional layers first, then apply recurrent layers:\nmodel = keras.Sequential([\n  keras.layers.Conv1D(filters=32, kernel_size=4, strides=2, activation=\"relu\"),\n  keras.layers.GRU(32, return_sequences=True)\n  keras.layers.Dense(14)\n])\n(add stride &gt; 1 to downsample)"
  },
  {
    "objectID": "slides/old-07-timeseries.html#bonus-trick-1-cnn-processing-1",
    "href": "slides/old-07-timeseries.html#bonus-trick-1-cnn-processing-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #1: CNN processing",
    "text": "Bonus trick #1: CNN processing\nEven with the previous tricks up out sleeve, getting RNNs to learn patterns over &gt;100 time steps is difficult.\nCan extract small-scale patterns with convolutional layers first, then apply recurrent layers\nOr maybe skip the recurrence altogether? WaveNet architecture:\n\n\n\n\n\nkeras.layers.Conv1D(..., padding=\"casual\")  # Look only backwards"
  },
  {
    "objectID": "slides/old-07-timeseries.html#bonus-trick-2-bidirectional-rnns",
    "href": "slides/old-07-timeseries.html#bonus-trick-2-bidirectional-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #2: bidirectional RNNs",
    "text": "Bonus trick #2: bidirectional RNNs\nFor time series we expect the most recent data points to be most important\n Chronological ordering makes sense\n\nSometimes this is not the case - for instance for text\n\n\n\nI arrived by bike.\n\n\n\nIch bin mit Fahrrad angekommen.\n\n\n\n\n\n\nCan process sequences both forwards and in reverse by using a bidirectional recurrent layer:\n\n\n\ninputs = keras.Input(shape=(...))\nx = layers.Bidirectional(layers.LSTM(16))(inputs)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#section",
    "href": "slides/21-generative-deep-learning.html#section",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "We already did generative deep learning for text generation - key was to sample instead of deterministically picking top prediction.\nWill look at some deep learning methods for converting sampled values into realistic data:\n\nVariational autoencoders (today)\nGenerative adversarial networks (today)\nDiffusion models (Monday)"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#autoencoders-revisited",
    "href": "slides/21-generative-deep-learning.html#autoencoders-revisited",
    "title": "DAT255: Deep learning engineering",
    "section": "Autoencoders revisited",
    "text": "Autoencoders revisited\nTrain a model to reconstruct its input, by\n\nencoding (compressing) data to smaller dimensionality:the latent space (explore in TensorBoard projector)\ndecoding from latent space to original dimensions\n\n\n\n\n\n\n\nCan randomly sample values in the latent space to generate new outputs\n(although not all sampled value are meaningful)"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#vectors-in-latent-space",
    "href": "slides/21-generative-deep-learning.html#vectors-in-latent-space",
    "title": "DAT255: Deep learning engineering",
    "section": "Vectors in latent space",
    "text": "Vectors in latent space\nFor language models we saw that direction was related to concepts / word meaning\n\nCan recognise the same for images:\n\n\n\n\n\n\nGenerated faces (arXiv:1609.04468)\n\n\n\n Less smile\n\n\nMore smile \n\n\nOpen mouth\n\n\nClosed mouth"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#latent-space-sampling",
    "href": "slides/21-generative-deep-learning.html#latent-space-sampling",
    "title": "DAT255: Deep learning engineering",
    "section": "Latent space sampling",
    "text": "Latent space sampling\nCan generalise to different model types\n\n\n\n\n\n\n\n\n\nIn this and next lecture we look at decoders to go from random points in latent space to realistic data\n Our sampling strategy is simply to draw from a normal distribution\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#improving-the-autoencoder",
    "href": "slides/21-generative-deep-learning.html#improving-the-autoencoder",
    "title": "DAT255: Deep learning engineering",
    "section": "Improving the autoencoder",
    "text": "Improving the autoencoder\n\n\n\nIn practice, autoencoder latent spaces are not great for generating realistic output.\n\n\n\nCan do a better by imposing some requirements on the structure of the latent space:\n Try to learn a distribution rather than a fixed encoding\n\n\n\nThis will lead us to the variational autoencoder (in a bit)"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#learning-distributions",
    "href": "slides/21-generative-deep-learning.html#learning-distributions",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning distributions",
    "text": "Learning distributions\nFor ML in general, we usually only to predict a single best value.\nRequires there to be a functional relationship (one-to-one or many-to-one) between the the input features and the target\n\n\n\n\n\n\n\n\n\nNice relationship between x and y\n\n\n\n\n\n\n\nMany possible y values for each x\n\n\n\n\n\n\n● data points\n‒ prediction"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#learning-distributions-1",
    "href": "slides/21-generative-deep-learning.html#learning-distributions-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning distributions",
    "text": "Learning distributions\nCan free ourselves from the functional assumption by rather modelling the distribution that data came from\nExample: Mixture density network predicting parameters of normal distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this we can sample realistic new data points"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#variational-autoencoder",
    "href": "slides/21-generative-deep-learning.html#variational-autoencoder",
    "title": "DAT255: Deep learning engineering",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\nTwist: Let’s model distributions in latent space\nThis is the aim of the variational autoencoder (VAE).\n\n\n\n\n\n\n\n\n\n\nEncode input data into parameters of a probability distribution (for a normal distribution: mean and variance)\nSample a point from this distribution\nDecode this point back to the input format"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#variational-autoencoder-1",
    "href": "slides/21-generative-deep-learning.html#variational-autoencoder-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\n\n\nSince encoder and decoder layers are nonlinear transformations, we can go from complicated input distributions to a normal (Gaussian) looking latent space and back\nTo force this latent space distribution, we modify the loss function:\nCombine\n\nreconstruction error (e.g. MSE), and\ndifference between the latent distribution and a normal distribution (measured by KL divergence)"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#variational-autoencoder-2",
    "href": "slides/21-generative-deep-learning.html#variational-autoencoder-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\n\n\n\n\n\nThe latent space becomes continuous\n We can sample along the different latent space dimensions to combine (or remove) concepts\n\n\n\n\n\nMNIST digits with 2 latent dimensions"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#generative-adversarial-networks",
    "href": "slides/21-generative-deep-learning.html#generative-adversarial-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\n\n\n\n\n\n\n\n\nGANs: The coolest thing in 2014 (now superseded by diffusion models)\nPopularised by thispersondoesnotexist.com\n\n\n\n\n\nBased StyleGAN, arXiv:182.04948"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#generative-adversarial-networks-1",
    "href": "slides/21-generative-deep-learning.html#generative-adversarial-networks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\nHave two models work in tandem:\n\nA generator that decodes random values into an image\nA discriminator that takes in images and predicts whether it is real or made by the generator\n\n\n\n\n\n\nTrain so that\n\nGenerator gets better at producing realistic output\nDiscriminator gets better at distinguishing fake from real"
  },
  {
    "objectID": "slides/21-generative-deep-learning.html#generative-adversarial-networks-2",
    "href": "slides/21-generative-deep-learning.html#generative-adversarial-networks-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\nTraining procedure is a little tricky, see the book for details\nWe can still do vector arithmetic in the latent space:\n\n\n\n\n\n\narXiv:1511.06434\n\n\nNote: GANs are well studied in research but seldom used in practice.\nThey are very sensitive to configuration of the training procedure, and are generally just difficult"
  },
  {
    "objectID": "slides/19-deployment.html#deployment-options",
    "href": "slides/19-deployment.html#deployment-options",
    "title": "DAT255: Deep learning engineering",
    "section": "Deployment options",
    "text": "Deployment options\n\nAs REST API (central)\nOn a device (local)\nIn browser / app (local)"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-a-model-as-a-rest-api",
    "href": "slides/19-deployment.html#deploying-a-model-as-a-rest-api",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying a model as a REST API",
    "text": "Deploying a model as a REST API\nThe most common approach:\n\n\n\n\nWebpage / App\n\n\n\n\n\n\n\nServer\n\n\n\n\n{\n  \"signature_name\": \"serving_default\",\n  \"instances\": [0.42, 1.31, 2.50]\n}\n\n\n{\n  \"predictions\": [0.99, 0.01]\n}"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-a-model-as-a-rest-api-1",
    "href": "slides/19-deployment.html#deploying-a-model-as-a-rest-api-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying a model as a REST API",
    "text": "Deploying a model as a REST API\nInference server:\n\nHas relevant hardware\nOptimised for inference\n\nRuns e.g. TensorFlow Serving\n\nNo user accounts, databases etc (stateless)\n\nWebsite server / App:\n\nMinimal compute need\nImplements user account and other persistent info\n\nBoth can be scaled independently\n\n\n\n\n✨"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-a-model-as-a-rest-api-2",
    "href": "slides/19-deployment.html#deploying-a-model-as-a-rest-api-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying a model as a REST API",
    "text": "Deploying a model as a REST API\nCan use TensorFlow Serving to serve Keras models\n\n# Download the TensorFlow Serving Docker imageo\ndocker pull tensorflow/serving\n\n# Start TensorFlow Serving container and open the REST API port\ndocker run -t --rm -p 8501:8501 \\\n    -v \"my_saved_model:/models/my_saved_model\" \\\n    -e MODEL_NAME=my_model \\\n    tensorflow/serving &\n\n# Query the model using the predict API\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n    -X POST http://localhost:8501/v1/models/my_model:predict\n\n# Returns =&gt; { \"predictions\": [2.5, 3.0, 4.5] }\nThings to consider:\n\nInference + network roundtrip typically takes &gt; 0.5 s\nAlthough request can be encrypted, data must be decrypted on the remote server"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-a-model-as-a-rest-api-3",
    "href": "slides/19-deployment.html#deploying-a-model-as-a-rest-api-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying a model as a REST API",
    "text": "Deploying a model as a REST API\n\n\nFor serving LLMs, JSON elements can differ per model, but typically looks like\n\n\n\ncurl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    --data '{\n        \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is the capital of France?\"\n            }\n        ]\n    }'"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-on-a-specific-device",
    "href": "slides/19-deployment.html#deploying-on-a-specific-device",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying on a (specific) device",
    "text": "Deploying on a (specific) device\n\n\n\nFor low-latency applications, the model typically resides on the same device that reads directly from sensor/instrument\nThis comes with specific requirements:\n\nLow memory\nLow compute capacity\nLow power\nSpecific runtime environment"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-on-a-specific-device-1",
    "href": "slides/19-deployment.html#deploying-on-a-specific-device-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying on a (specific) device",
    "text": "Deploying on a (specific) device\n\n\nCan still develop and train on regular hardware, and then optimise the model for the target device\nPreferred way to run Keras models is through Lite Runtime (LiteRT)*, which supports\n\nAndroid / iOS\nEmbedded Linux\nVarious microcontrollers\n\nConverts and optimises Keras (or other) models to run in any of the above environments\n\n\n*Used to be called TensorFlow Lite"
  },
  {
    "objectID": "slides/19-deployment.html#very-low-latency-dl",
    "href": "slides/19-deployment.html#very-low-latency-dl",
    "title": "DAT255: Deep learning engineering",
    "section": "Very low-latency DL",
    "text": "Very low-latency DL\n\n\n\n\n\nIf you want a small-ish model to be really fast, run it to a Field-Programmable Gate Array (FPGA)\nInference times down to between \\(\\small 10^{-9}\\) – \\(\\small 10^{-6}\\) s (depending on models size)\n\n\n\nTools like hls4ml can do the high-level synthesis of Keras models into hardware-specific language"
  },
  {
    "objectID": "slides/19-deployment.html#deploying-locally-in-browser-or-app",
    "href": "slides/19-deployment.html#deploying-locally-in-browser-or-app",
    "title": "DAT255: Deep learning engineering",
    "section": "Deploying locally in browser or app",
    "text": "Deploying locally in browser or app\n\n\nA good midway between the two previous options: Implement in browser\nTensorFlow.js provides JavaScript interface to run Keras models (with WebGPU acceleration)\nIdeal if:\n\nModel is not too big\nModel does not have to be kept secret\nUser data should stay on the users device\nYou don’t want to pay for a compute server"
  },
  {
    "objectID": "slides/19-deployment.html#optimisation",
    "href": "slides/19-deployment.html#optimisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Optimisation",
    "text": "Optimisation\nWhen your models is developed and ready to be deployed, it pays off to optimise it\nMinimise computing cost and memory footprint by:\n\nPruning\nWeight quantisation\nWeight sharing / clustering\n\nThis is implemented in LiteRT(and in other tools too)\n\n\n\n\nLiteRT optimisation flowchart"
  },
  {
    "objectID": "slides/19-deployment.html#some-deployment-options",
    "href": "slides/19-deployment.html#some-deployment-options",
    "title": "DAT255: Deep learning engineering",
    "section": "Some deployment options",
    "text": "Some deployment options\nStreamlit and Gradio"
  },
  {
    "objectID": "slides/19-deployment.html#streamlit",
    "href": "slides/19-deployment.html#streamlit",
    "title": "DAT255: Deep learning engineering",
    "section": "Streamlit",
    "text": "Streamlit\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "slides/19-deployment.html#section",
    "href": "slides/19-deployment.html#section",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Simple public deployment on Streamlit cloud\n\nAutomatic updates triggered when code pushed to GitHub\nFree compute resources rather limited"
  },
  {
    "objectID": "slides/19-deployment.html#section-2",
    "href": "slides/19-deployment.html#section-2",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Gradio apps are easily deployed on Hugging Face Spaces\n\nAI focused\nFree compute resources are somewhat limited"
  },
  {
    "objectID": "slides/17-llms.html#training-decoder-models",
    "href": "slides/17-llms.html#training-decoder-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Training decoder models",
    "text": "Training decoder models\nDecoder-type language models are next word predictors.\n\n\nSo we train them to do exactly this:\n\nMask the end of sentences\nUse the next token as the prediction target\nReveal token and move to next one\nContinue until end of text\n\n\nUsually call the procedure masked attention or causal attention, when model is only allowed to look “backwards”.\nCan train on large, unlabelled data in a self-supervised approach"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nAs for other classification models, last layer looks like\n\nDense(units=vocabulary_size, activation=\"softmax\")\n\n-&gt; output is a vector of softmax scores for all possible tokens.\n\n\n\nRun the model in an autoregressive loop:\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n(Stop if predicting end-of-sequence token)"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-1",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nAs for other classification models, last layer looks like\n\nDense(units=vocabulary_size, activation=\"softmax\")\n\n-&gt; output is a vector of softmax scores for all possible tokens.\n\n\n\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-2",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the _____\n\n\nfloor       7.72% bed         6.82% couch       5.70% ground      4.71% edge        4.66%"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-3",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the floor _____\n\n\n,           25.08% and         13.56% .            7.38% of           7.07% with         6.58%"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-4",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the floor, _____\n\n\nand         9.41% looking     3.23% the         1.78% he          1.63% his         1.48%"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-5",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the floor, and _____\n\n\nthe         8.40% he          5.72% I           4.55% she         3.65% his         3.56%"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-6",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-6",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\n\nMakes model deterministic, always outputs same output for a given input\nOften get stuck in sequence loops"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-7",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-7",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\n\nComputationally expensive\nCan also get stuck in loops\n\n\n\n\n\n\n\n\n\n\n\narXiv:1904.09751"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-8",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-8",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\nSampling: Use scores as probabilities and sample randomly\n\nWith large vocabularies the results can be nonsensical"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-9",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-9",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\nSampling: Use scores as probabilities and sample randomly\nTop-K sampling: Sample among the K tokens with highest score"
  },
  {
    "objectID": "slides/17-llms.html#generating-text-from-a-decoder-model-10",
    "href": "slides/17-llms.html#generating-text-from-a-decoder-model-10",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\nSampling: Use scores as probabilities and sample randomly\nTop-K sampling: Sample among the K tokens with highest score\nAdjusted softmax sampling: Add a parameter T called temperature in the softmax function applied to the outputAdjusts how tokens are sampled"
  },
  {
    "objectID": "slides/17-llms.html#softmax-with-temperature",
    "href": "slides/17-llms.html#softmax-with-temperature",
    "title": "DAT255: Deep learning engineering",
    "section": "Softmax with temperature",
    "text": "Softmax with temperature\nDivide the logits a by temperature T in the softmax function:\n\\[\n\\small\ny_i = \\frac{\\exp(a_i/T)}{\\sum_j \\exp(a_j/T)}\n\\]\nChanges the distribution we sample from.\n\n\n\nLow T: Give highest score to the most likely token\n\nMore determinism\n\n\n\n\n\n\nHigh T: Give more equals scores to all tokens\n\nMore randomness / creativity"
  },
  {
    "objectID": "slides/17-llms.html#large-language-models-llms",
    "href": "slides/17-llms.html#large-language-models-llms",
    "title": "DAT255: Deep learning engineering",
    "section": "Large language models (LLMs)",
    "text": "Large language models (LLMs)\nLLM: language model with \\(\\small\\gtrsim 10^9\\) (1 billion) parameters"
  },
  {
    "objectID": "slides/17-llms.html#training-llms",
    "href": "slides/17-llms.html#training-llms",
    "title": "DAT255: Deep learning engineering",
    "section": "Training LLMs",
    "text": "Training LLMs\nTypical training procedure:\n\n\n\nSelf-supervised pre-training\nA general model that can be subsequently fine-tuned on different tasks is called a foundation model\nCost: $1 million to $100 million\n\n\n\nSupervised fine-tuning\nAdapt to more specific uses, such as\n\nChat / Question answering\nChain-of-thought reasoning\nDomain-specific used\n\nCost: Depends\n\n\n\n\n\nContinuous fine-tuning\nFine-tune an already fine-tuned model\nCost: $1 to $1000"
  },
  {
    "objectID": "slides/17-llms.html#training-data",
    "href": "slides/17-llms.html#training-data",
    "title": "DAT255: Deep learning engineering",
    "section": "Training data",
    "text": "Training data\nAs always, key to a good model is data.\n\n\nPre-training:\nData scraped from …\n\nWikipedia (multi-language)\nGitHub (code)\nArXiv (academic text)\nStackOverflow (Q&A)\nReddit etc. (forums)\nProject Gutenberg (books)\n\nSee e.g. Common Crawl"
  },
  {
    "objectID": "slides/17-llms.html#training-data-1",
    "href": "slides/17-llms.html#training-data-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Training data",
    "text": "Training data\nAs always, key to a good model is data.\n\n\nPre-training:\nData scraped from …\n\nWikipedia (multi-language)\nGitHub (code)\nArXiv (academic text)\nStackOverflow (Q&A)\nReddit etc. (forums)\nProject Gutenberg (books)\n\nSee e.g. Common Crawl\n\nFine-tuning:\nNow we need annotated data\n\nSpecific datasets for question answering\nReinforcement learning from human feedback (RLHF)\n\n\nExact training data are business secrets (even for open-sourced models)"
  },
  {
    "objectID": "slides/17-llms.html#distributed-training",
    "href": "slides/17-llms.html#distributed-training",
    "title": "DAT255: Deep learning engineering",
    "section": "Distributed training",
    "text": "Distributed training\nOnce the model can’t fit on a single GPU, things get more complicated\n\n(for instance the full Llama 3.1 needs 3.3TB VRAM for training)\n\n\nWhat can be split and parallelised?\n\nData: Run different batches in parallel\nWeights: Distribute weight matrices over separate GPUs\nLayers: Distribute different layers over separate GPUs\nSequences: Partition the input data sequences\n\nUpdate entire model after each training step\nTensorFlow implements a set of methods for distributed training, while Hugging Face offers advanced ones"
  },
  {
    "objectID": "slides/17-llms.html#supervised-fine-tuning",
    "href": "slides/17-llms.html#supervised-fine-tuning",
    "title": "DAT255: Deep learning engineering",
    "section": "Supervised fine-tuning",
    "text": "Supervised fine-tuning\nA pre-trained model can only append text to an input.\n\nMaking a useful chatbot requires instruction training\n{\n  \"instruction\": \"Translate 'Good night' into Spanish.\",\n  \"solution\": \"Buenas noches\"\n}\n{\n  \"instruction\": \"Name primary colors.\",\n  \"solution\": \"Red, blue, yellow\"\n}\n\n\n\nOn the Hugging Face model hub you will often find two variants of the same model:\n\n\n\n\nmeta-llama/Llama-3.2-3B meta-llama/Llama-3.2-3B-Instruct\n\n\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (…)"
  },
  {
    "objectID": "slides/17-llms.html#llm-fine-tuning-with-limited-resources",
    "href": "slides/17-llms.html#llm-fine-tuning-with-limited-resources",
    "title": "DAT255: Deep learning engineering",
    "section": "LLM fine-tuning with limited resources",
    "text": "LLM fine-tuning with limited resources\nFull model fine-tuning can be problematic for two reasons:\n\nHardware requirements (mainly VRAM)\nRisk of catastrophic forgetting (https://arxiv.org/abs/1312.6211)\n\n\nUseful techniques:\n\n\n\n\n\nPrompt tuning:\nAdd a small trainable model before the LLM, which outputs learned, task-specific tokens\n\n\n\n\nLow-rank adaptation (LoRA):\nAdd small traininable layers in parallel with the existing attention layers"
  },
  {
    "objectID": "slides/17-llms.html#low-rank-adaptation-lora",
    "href": "slides/17-llms.html#low-rank-adaptation-lora",
    "title": "DAT255: Deep learning engineering",
    "section": "Low-rank adaptation (LoRA)",
    "text": "Low-rank adaptation (LoRA)\n\n\n\n\n\nKeep original weight matrix \\(\\small W\\) frozen\nTrain new (small) matrices \\(\\small A\\) and \\(\\small B\\)\n\nAdds \\(\\small 2\\cdot R\\cdot D\\) new parameters (compared to \\(\\small D^2\\) in \\(\\small W\\))"
  },
  {
    "objectID": "slides/17-llms.html#quantization",
    "href": "slides/17-llms.html#quantization",
    "title": "DAT255: Deep learning engineering",
    "section": "Quantization",
    "text": "Quantization\nReduce memory cost of running inference by reducing numerical precision in weights and activations\n\nCan store float32 as float16 without much modification\nCan store float32 as int8 for use on embedded systems (requires more modification)\n\n\nModern quantisation schemes for LLMs are more extreme:\n\n\nGo down to anywhere between 6 to 2 bit\nExample:\nDeepSeek-R1-Q4_K_M.gguf\nQ{bits per weight}_{type}_{variant}\n\n\n\n\n\nScheme\nCompression ratio(relative to f32)\nPerformance\n\n\n\n\nQ8_0\n1:4\nHigh quality\n\n\nQ4_K_M\n1:8\nMedium quality\n\n\nQ3_K_M\n1:11\nLow quality"
  },
  {
    "objectID": "slides/17-llms.html#knowledge-distillation",
    "href": "slides/17-llms.html#knowledge-distillation",
    "title": "DAT255: Deep learning engineering",
    "section": "Knowledge distillation",
    "text": "Knowledge distillation\nTrain a small model to mimic the output of a bigger one:\nUse the output of the bigger model as labels to supervise the smaller model\n\n\n\n\n\n\n\n\n\nhttps://arxiv.org/abs/2305.02301"
  },
  {
    "objectID": "slides/17-llms.html#knowledge-distillation-1",
    "href": "slides/17-llms.html#knowledge-distillation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Knowledge distillation",
    "text": "Knowledge distillation\nTrain a small model to mimic the output of a bigger one:\nUse the output of the bigger model as labels to supervise the smaller model"
  },
  {
    "objectID": "slides/15-transformers.html#vector-embeddings",
    "href": "slides/15-transformers.html#vector-embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Vector embeddings",
    "text": "Vector embeddings\nTokenise and embed the words:\n\n“you are right”"
  },
  {
    "objectID": "slides/15-transformers.html#transformers-basic-idea",
    "href": "slides/15-transformers.html#transformers-basic-idea",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformers: Basic idea",
    "text": "Transformers: Basic idea\n\n\n\nTokenise and embed the words:\n\n“you are right”\n\n\n“turn right here”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntransform! \n\n\ntransform! \n\n\nEmbedding space\n\n\nNew representation"
  },
  {
    "objectID": "slides/15-transformers.html#transformers-the-basic-idea",
    "href": "slides/15-transformers.html#transformers-the-basic-idea",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformers: The basic idea",
    "text": "Transformers: The basic idea\n\n\n\nAim:\n\nTake a sequence of vectors of dimensionality \\(\\small N \\times D\\)\nCompute some relation between the vectors\nUse these relations to transform the vectors into new ones (also \\(\\small N \\times D\\))\nNew representation is better suited to solve the task\n\nCritical operation is to incorporate the relation between vectors\n-&gt; the neural attention mechanism"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention",
    "href": "slides/15-transformers.html#self-attention",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\n\nWhat service does a “station” provide?\n\n\n\n\nHe stopped at the station to fill petrol.\nThe meteorological station is unmanned.\nThe train left the station on time.\nI prefer BBC Radio 4 over other stations.\n\nOf course dependent on context."
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-1",
    "href": "slides/15-transformers.html#self-attention-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\nExample sentence: “The train left the station on time”\n\n\n\n\n\n\nRepeatfor all tokens\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-the-steps-involved",
    "href": "slides/15-transformers.html#self-attention-the-steps-involved",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention: The steps involved",
    "text": "Self-attention: The steps involved\n\n\n\n\n\n\n\n1: Text vectorisation\nCompute embedding vectors."
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-the-steps-involved-1",
    "href": "slides/15-transformers.html#self-attention-the-steps-involved-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention: The steps involved",
    "text": "Self-attention: The steps involved\n\n\n\n\n\n\n\n\n\n\n2: Compute attention scores\nDot-product self-attention"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-the-steps-involved-2",
    "href": "slides/15-transformers.html#self-attention-the-steps-involved-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention: The steps involved",
    "text": "Self-attention: The steps involved\n\n\n\n\n\n\n\n\n\n\n\n\n\n3: Compute weighted token vectors\nScale, multiply, and normalise"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-the-steps-involved-3",
    "href": "slides/15-transformers.html#self-attention-the-steps-involved-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention: The steps involved",
    "text": "Self-attention: The steps involved\n\n\n\n\n\n\n\n\n\n\n4: Create the new, context-aware vector\nSum up attention-weighted token vectors"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-2",
    "href": "slides/15-transformers.html#self-attention-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-3",
    "href": "slides/15-transformers.html#self-attention-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\n\n\nCompute dot product with all other token vectors"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-4",
    "href": "slides/15-transformers.html#self-attention-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\nScale by input length and apply softmax\nSoftmax ensures all attention scores are in the interval [0, 1]."
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-5",
    "href": "slides/15-transformers.html#self-attention-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\nMultiply each token vector by the score, and sum all of them"
  },
  {
    "objectID": "slides/15-transformers.html#self-attention-6",
    "href": "slides/15-transformers.html#self-attention-6",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n \n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\nReturn the new vector representation"
  },
  {
    "objectID": "slides/15-transformers.html#the-query-key-value-model",
    "href": "slides/15-transformers.html#the-query-key-value-model",
    "title": "DAT255: Deep learning engineering",
    "section": "The query-key-value model",
    "text": "The query-key-value model\n\n\n\nWe can generalise the attention mechanism by using concepts from information retrieval.\nWhat we computed was basically\n\n\n\n\n\n\n\noutput = sum(inputs * pairwise_scores(inputs, inputs))\n\n\nbut we could be doing this with three different sequences:\n\n\n\n\n\n\n\noutput = sum(values * pairwise_scores(query, keys))\n\n\nFor each element in a query, compute how much it is related to every key, and use these scores to weight a sum of values"
  },
  {
    "objectID": "slides/15-transformers.html#multi-head-attention",
    "href": "slides/15-transformers.html#multi-head-attention",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\nNow we want to increase the complexity by computing attention several times in parallel.\nBut: Our transformation so far has no learnable parameters\n it’s just a stateless operation, we get the same result each time.\n\nKey idea from the “Attention is all you need” paper:\n\nPass each input (query, key and value) though a separate Dense layer\n\nEach with their own parameters\n\n\nThis we can do in parallel, and get different features for each attention “head”\n Call it multi-head attention."
  },
  {
    "objectID": "slides/15-transformers.html#multi-head-attention-1",
    "href": "slides/15-transformers.html#multi-head-attention-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/15-transformers.html#multi-head-attention-2",
    "href": "slides/15-transformers.html#multi-head-attention-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\nThe parameters of the Dense layers that form the Q, K, V matrices, is where the attention head actually learns something.\n\nIf \\(\\small D\\) is the dimensionality of the embedding space, and \\(\\small N\\) is the number of tokens in our data matrix \\(\\small X\\),\nthis introduces 3 weight matrices \\(\\small W\\), of dimension \\(\\small D \\times D\\):\n\\[\n\\small\n\\begin{align}\nQ &= X W^{(q)} \\\\\nK &= X W^{(k)} \\\\\nV &= X W^{(v)} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/15-transformers.html#multi-head-attention-3",
    "href": "slides/15-transformers.html#multi-head-attention-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\n\n\n\nIn the end, we can write the scaled dot-product self-attention for a single head as\n\\[\n\\small\n\\mathrm{attention}(Q, K, V) = \\mathrm{softmax}\\left[\\frac{QK^T}{\\sqrt{D}} \\right] V\n\\]\nAnd to get the final output from all the attention heads, we simply concatenate the individial outputs.\n\n\n\n\nDeep Learning: Foundations and Concepts, C. Bishop"
  },
  {
    "objectID": "slides/15-transformers.html#multi-head-attention-4",
    "href": "slides/15-transformers.html#multi-head-attention-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\n\n\n\nIn the end, we can write the scaled dot-product self-attention for a single head as\n\\[\n\\small\n\\mathrm{attention}(Q, K, V) = \\mathrm{softmax}\\left[\\frac{QK^T}{\\sqrt{D}} \\right] V\n\\]\nAnd to get the final output from all the attention heads, we simply concatenate the individial outputs."
  },
  {
    "objectID": "slides/15-transformers.html#bonus-feature",
    "href": "slides/15-transformers.html#bonus-feature",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus feature",
    "text": "Bonus feature\n\n\nBefore we started involving Dense layers and the Q, K, V matrices, we had this:\n\n\n\n\n\n\n\nNotice it’s symmetric.\nThe output from\n\\[\\small\\mathrm{attention}(Q, K, V)\\]\non the other hand, need not be symmetric.\n\nCan then encode asymmetric relations:\nA hammer is a tool, but not all tools are hammers."
  },
  {
    "objectID": "slides/15-transformers.html#the-transformer-architecture",
    "href": "slides/15-transformers.html#the-transformer-architecture",
    "title": "DAT255: Deep learning engineering",
    "section": "The Transformer architecture",
    "text": "The Transformer architecture\n\n\n\n\n\nWith the multi-head attention in place, we add a few extra layers to form a block:\n\nA residual connection (Add & Norm) going around the attention layer\nA stack of Dense (Feed Forward) layers\nA residual connection going around the feed forward layers\n\n\n\nThis almost completes our transformer encoder."
  },
  {
    "objectID": "slides/15-transformers.html#positional-encodings",
    "href": "slides/15-transformers.html#positional-encodings",
    "title": "DAT255: Deep learning engineering",
    "section": "Positional encodings",
    "text": "Positional encodings\nSo far, the position of each token in a sentencedoesn’t affect the computation of the attention scores.\n-&gt; If we permute the word order, we still get thesame result.\n\nThe food was bad, not good at all.\n\\[\n\\small\n\\neq\n\\]\nThe food was good, not bad at all.\n\n\n\n\n\nTransformer models solve this by encoding token position into the data itself:\ntoken embedding with position = token embedding + position encoding"
  },
  {
    "objectID": "slides/15-transformers.html#positional-encodings-1",
    "href": "slides/15-transformers.html#positional-encodings-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Positional encodings",
    "text": "Positional encodings\n\nOption 1: Count token positions, then embed them\n\n\n\n\n\n\n\n\n\n\n\ntoken\nThe\nfood\nwas\ngood\n\n\ntoken embedding\n\\(\\mathbf{x}_1\\)\n\\(\\mathbf{x}_2\\)\n\\(\\mathbf{x}_3\\)\n\\(\\mathbf{x}_4\\)\n\n\ntoken position\n1\n2\n3\n4\n\n\nposition embedding\n\\(\\mathbf{r}_1\\)\n\\(\\mathbf{r}_2\\)\n\\(\\mathbf{r}_3\\)\n\\(\\mathbf{r}_4\\)\n\n\nfinal embedding\n\\(\\mathbf{x}_1 + \\mathbf{r}_1\\)\n\\(\\mathbf{x}_2 + \\mathbf{r}_2\\)\n\\(\\mathbf{x}_3 + \\mathbf{r}_3\\)\n\\(\\mathbf{x}_4 + \\mathbf{r}_4\\)\n\n\n\n\n\n\n\n\nOption 2: Sinusoidal encoding (see textbook p 613)"
  },
  {
    "objectID": "slides/13-nlp-basics.html#natural-language-processing-nlp",
    "href": "slides/13-nlp-basics.html#natural-language-processing-nlp",
    "title": "DAT255: Deep learning engineering",
    "section": "Natural language processing (NLP)",
    "text": "Natural language processing (NLP)\n(as opposed to machine language)\nSince most human knowledge is stored as text, NLP is an important field of study\n\n\nKnowing grammar, syntax and language structure (i.e. linguistics), can we write down the set of rules required to do a language task like translation?\n\n\n\nNot really, no.\n\n\n\nKnowing basically nothing, but having a library of example text, can we statistically infer the rules required to do a language task?\n\n\n\nYes!\n\n\nDisclaimer: There’s loads of interesting NLP stuff that we will skip, because it has been obsoleted by transformer models. Look at the supplemental reading on Canvas."
  },
  {
    "objectID": "slides/13-nlp-basics.html#nlp-tasks",
    "href": "slides/13-nlp-basics.html#nlp-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "NLP tasks",
    "text": "NLP tasks\n\nText classification:\nWhat is the topic of this text?\nContent filtering:\nIs this email spam? Does this post contain swearing?\nSentiment analysis:\nIs this review positive or negative?\nTranslation:\nWhat is this text in French?\nSummarisation:\nCan you give a short summary of this article?"
  },
  {
    "objectID": "slides/13-nlp-basics.html#our-toolbox-so-far",
    "href": "slides/13-nlp-basics.html#our-toolbox-so-far",
    "title": "DAT255: Deep learning engineering",
    "section": "Our toolbox so far:",
    "text": "Our toolbox so far:\nWe know how to do\n\nSequence processing:\nKnow how to construct RNNs, CNNs\nInput encoding:\nLooked at embeddings last week\n\nWith this, we can build an NLP model at 2017 level (will do so this week)\nNext week we will learn about\n\nTransformers (not the electrical kind)\n\nWith this, we can build an NLP model at 2022 level\n\n\n\n\nMMLU benchmark"
  },
  {
    "objectID": "slides/13-nlp-basics.html#getting-text-into-our-model",
    "href": "slides/13-nlp-basics.html#getting-text-into-our-model",
    "title": "DAT255: Deep learning engineering",
    "section": "Getting text into our model",
    "text": "Getting text into our model\n\n\nText vectorisation: Converting text to numeric data.\nTypical approach consists of several steps\n\nStandardisation:Remove diacritics, punctuation, convert to lowercase\nTokenisation:Split text into tokens which can be words, subwords, or groups of words\nIndexing:Convert tokens to integer values\nEncoding:Convert indices into embeddings or one-hot encoding\n\n\n\n\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/13-nlp-basics.html#text-vectorisation",
    "href": "slides/13-nlp-basics.html#text-vectorisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Text vectorisation",
    "text": "Text vectorisation\nCan do all the processing steps with the keras.layers.TextVectorization layer\n\nA preprocessing layer which maps text features to integer sequences.\n\n\n\n\nkeras.layers.TextVectorization(\n  max_tokens=None,\n  standardize=\"lower_and_strip_punctuation\",\n  split=\"whitespace\",\n  ngrams=None,\n  output_mode=\"int\",\n  output_sequence_length=None,\n  vocabulary=None,\n  ...\n)\nstandardize and split can be custom functions."
  },
  {
    "objectID": "slides/13-nlp-basics.html#text-vectorisation-1",
    "href": "slides/13-nlp-basics.html#text-vectorisation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Text vectorisation",
    "text": "Text vectorisation\nCan do all the processing steps with the keras.layers.TextVectorization layer\n\n\n\nThe vocabulary is built automatically from data by calling adapt():\n\n\n\nvectorize_layer = keras.layers.TextVectorization(\n  max_tokens=1000,\n  standardize=\"lower_and_strip_punctuation\",\n  output_sequence_length=250\n)\n\nvectorize_layer.adapt(dataset)\nvectorize_layer.get_vocabulary()[:10]\n# ['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n\n\n\n\nWe reserve index 1 ('[UNK]') for unknown, out-of-vocabulary tokens\nWe reserve index 0 ('') for the mask token, which used to pad sequences"
  },
  {
    "objectID": "slides/13-nlp-basics.html#text-vectorisation-2",
    "href": "slides/13-nlp-basics.html#text-vectorisation-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Text vectorisation",
    "text": "Text vectorisation\nTest sentence from the IMDb movie review dataset:\n\n\n\nText:     I   am  shocked.  Shocked  and  dismayed  that  the   428   of   you   IMDB  users\nEncoded: 10  238     2355      2355    3         1    12    2     1    5    23    933   5911\nDecoded:  i   am  shocked   shocked  and     [UNK]  that  the  [UNK]  of   you   imdb  users\n\nCan remove HTML tags (or other uninteresting stuff) by using regexes:\n\n\n\ndef custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  without_html = tf.strings.regex_replace(lowercase, '&lt;[^&gt;]*&gt;', ' ')\n  without_punctuation = tf.strings.regex_replace(without_html, '[{}]'.format(string.punctuation), '')\n  return without_punctuation\n\nvectorize_layer = keras.layers.TextVectorization(\n  standardize=custom_standardization\n)\nNote: TextVectorization is based on TensorFlow operations, so using it with other backend frameworks is complicated."
  },
  {
    "objectID": "slides/13-nlp-basics.html#detour-n-grams",
    "href": "slides/13-nlp-basics.html#detour-n-grams",
    "title": "DAT255: Deep learning engineering",
    "section": "Detour: N-grams",
    "text": "Detour: N-grams\nSo far all words receive a separate index; we assume no relation between them.\nSince text is not strictly ordered, how about training a Dense model on the presence of words, and just ignore the ordering?\nConsider the IMDb review dataset again:\n\n\n\n\n\n\n\n\n\n\nReview\nSentiment\n\n\n\n\nProbably my all-time favorite movie, a story of selflessness, sacrifice and dedication to …\npositive\n\n\nIf you like original gut wrenching laughter you will like this movie. If you are young …\npositive\n\n\nThis movie made it into one of my top 10 most awful movies. Horrible. There wasn’t …\nnegative\n\n\n\n\n\n\nCan classify decently well by learning correlations between single words and sentiment.\nIn this case we treat the text as a set and not a sequence."
  },
  {
    "objectID": "slides/13-nlp-basics.html#detour-n-grams-1",
    "href": "slides/13-nlp-basics.html#detour-n-grams-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Detour: N-grams",
    "text": "Detour: N-grams\nThere are still some word combinations that throw us off:\n\n\"good\" is very different from \"not good\".\n\n\n\n\n\nSuch modifiers are placed close, so we can extend our vocabulary with pairs:\n\n“… not really expecting much,”\n   {“not”, “not really”, “really”, “really expecting”, “expecting”, “expecting much”]}\n\n\n\n\nThis approach of keeping some ordering, popular around 2010, is called\n\nbigrams (for pairs for words)\ntrigrams (for triplets of words)\nN-grams (for N words)"
  },
  {
    "objectID": "slides/13-nlp-basics.html#detour-n-grams-2",
    "href": "slides/13-nlp-basics.html#detour-n-grams-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Detour: N-grams",
    "text": "Detour: N-grams\n\nN-grams apply to other data as well, for instance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence\n1-gram\n2-gram\n3-gram\n\n\n\n\nProtein sequencing\nCys-Gly-Leu-Ser-Trp\nCys, Gly, Leu, Ser, Trp,\nCys-Gly, Gly-Leu, Leu-Ser, Ser-Trp\nCys-Gly-Leu, Gly-Leu-Ser, Leu-Ser-Trp\n\n\nDNA sequencing\nAGCTTCGA\nA, G, C, T, T, C, G, A\nAG, GC, CT, TT, TC, CG, GA\nAGC, GCT, CTT, TTC, TCG, CGA,\n\n\n\n\n\n\nFor language research purpuses N-grams are still useful; have a look atthe Google N-gram Viewer"
  },
  {
    "objectID": "slides/13-nlp-basics.html#embeddings",
    "href": "slides/13-nlp-basics.html#embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings\nLet’s return to single words.\nOur text is converted into numbers – We can convert them further into good numbers, by creating word embeddings.\nRecall we could map categorical values to vectors of floating-point values:\n\n\n\nmy_embedding = {\n  'the': [-1.46, -0.86,  0.09],\n  'and': [-0.27,  1.15,  1.19],\n  'a':   [ 1.17,  0.06, -0.16],\n  'of':  [ 0.60,  0.10,  0.22],\n  ...\n}\nAt first, these values are randomly initialised\nThen, we optimise them during training."
  },
  {
    "objectID": "slides/13-nlp-basics.html#embeddings-1",
    "href": "slides/13-nlp-basics.html#embeddings-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings\nEmbedding layers are practically the same as one-hot encoding, followed by a linear Dense layer (without the bias term):\n\n\n\n\n\n\nthe  [ 0     1     0     0 ]\n\n\n[ -1.46 -0.86  0.09 ]\n\n\n\n\n\nembedding = keras.Sequential([\n  keras.layers.StringLookup(output_mode=\"one-hot\"),\n  keras.layers.Dense(\n    units=embedding_dim,\n    use_bias=False,\n    activation=None\n  )\n])\n\n\nDifference is that Embedding layers are implemented in a much more efficient way."
  },
  {
    "objectID": "slides/13-nlp-basics.html#demo",
    "href": "slides/13-nlp-basics.html#demo",
    "title": "DAT255: Deep learning engineering",
    "section": "Demo",
    "text": "Demo\n\n\n\nLet’s visualise some embeddings"
  },
  {
    "objectID": "slides/13-nlp-basics.html#embeddings-2",
    "href": "slides/13-nlp-basics.html#embeddings-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings"
  },
  {
    "objectID": "slides/13-nlp-basics.html#computing-similarity-in-embedding-space",
    "href": "slides/13-nlp-basics.html#computing-similarity-in-embedding-space",
    "title": "DAT255: Deep learning engineering",
    "section": "Computing similarity in embedding space",
    "text": "Computing similarity in embedding space\nWe’ve seen there is a relation between semantics and position in embedding space.\nHow can we measure token similarity?\n\n\nOption 1: Euclidian (L2) norm\n-&gt; distance between positions \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\)\n\n\\[\n\\small\n|| (\\mathbf{a} - \\mathbf{b}) ||_2 = \\sqrt{(a_1 - b_1)^2 + ... \\ (a_n - b_n)^2}\n\\]\n\n\n\nOption 2: Cosine similarity\n-&gt; angular difference between positions \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) (ignores vector lengths)\n\n\\[\n\\small\n\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||}\n\\]"
  },
  {
    "objectID": "slides/13-nlp-basics.html#computing-similarity-in-embedding-space-1",
    "href": "slides/13-nlp-basics.html#computing-similarity-in-embedding-space-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Computing similarity in embedding space",
    "text": "Computing similarity in embedding space\nHow to interpret Euclidian vs cosine similarity?\nConsider this example from StackOverflow:\n\nYou run an online shop and want to compare customers.\n\nUser 1 bought 1x eggs, 1x flour and 1x sugar.\nUser 2 bought 100x eggs, 100x flour and 100x sugar.\nUser 3 bought 1x eggs, 1x Vodka and 1x Red Bull.\n\n\n\n-&gt; By cosine similarity, user 1 and user 2 are more similar\n-&gt; By Euclidian similarity, user 1 and user 3 are more similar\nWord embeddings are affected by word frequency, so cosine similarity is often preferred."
  },
  {
    "objectID": "slides/13-nlp-basics.html#embeddings-3",
    "href": "slides/13-nlp-basics.html#embeddings-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings"
  },
  {
    "objectID": "slides/13-nlp-basics.html#better-text-tokenisation",
    "href": "slides/13-nlp-basics.html#better-text-tokenisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Better text tokenisation",
    "text": "Better text tokenisation\n\n\n\nSome words are obviously variants of the same; but conjugated, in plural, etc.\n\nam, is, are are all variants of the lemma that is be\n\n\n\n\nWe can collapse these in order to simplify the vocabulary our model needs to learn:\n\nHe is reading books -&gt; He be read book\n\n\n\n\nManually composed algorithms for this lemmatization have been developed over long time."
  },
  {
    "objectID": "slides/13-nlp-basics.html#better-text-tokenisation-1",
    "href": "slides/13-nlp-basics.html#better-text-tokenisation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Better text tokenisation",
    "text": "Better text tokenisation\nModern tokenisation algorithms are not inspired by linguistics, but rather substring frequencies in data.\nDifferent LLMs use different algorithms, but they are mostly based on splitting words into individual characters and recombing them into common subwords.\n\nThe final vocabulary is under no obligation to make sense to us 🤷\nRandomly selected tokens from the GPT-2 tokeniser vocabulary:\n\n\n\n\n objective         stacked             USB                Energy\n 306               booster            Bird                learn\n stationary        nighttime          85                  rice\n tensions         mission             iency               quitting\nagging             hypers             OOOOOOOO           Typ\n reopen           finding              Spoon              Plate\n nat              Ïĥ                   climates           Druid\ndownload          isition             æĦ                  partic\n predis            calf                Object            annie\n\nDifferent tokenisers example"
  },
  {
    "objectID": "slides/13-nlp-basics.html#pretrained-embeddings-and-models",
    "href": "slides/13-nlp-basics.html#pretrained-embeddings-and-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Pretrained embeddings and models",
    "text": "Pretrained embeddings and models\n\n\nFor image classification tasks we could use take pretrained models and fine-tune, without having to retrain the first layers\n\nFirst layers were general enough to transfer well to basically all other tasks\n\nSame for NLP models:\nEmbeddings are general and can be reused for other models or tasks."
  },
  {
    "objectID": "slides/13-nlp-basics.html#nlp-resources",
    "href": "slides/13-nlp-basics.html#nlp-resources",
    "title": "DAT255: Deep learning engineering",
    "section": "NLP resources",
    "text": "NLP resources\n\nTensorflow Text"
  },
  {
    "objectID": "slides/13-nlp-basics.html#nlp-resources-1",
    "href": "slides/13-nlp-basics.html#nlp-resources-1",
    "title": "DAT255: Deep learning engineering",
    "section": "NLP resources",
    "text": "NLP resources\n\nTensorflow Text\nHuggingface models and ML library"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#sequences",
    "href": "slides/11-timeseries-cont.html#sequences",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequences",
    "text": "Sequences\nSequences are ordered series. For instance\n\n\n\nNatural language\nI only drink coffee (\\(\\neq\\) only I drink coffee)\n\n\n\nTime series\n\n\n\nTime\n11:00\n12:00\n13:00\n14:00\n15:00\n\n\nTemp\n7°C\n8°C\n10°C\n12 °C\n12 °C\n\n\n\n\n\n\n\nAudio\n\n\n\n\n\n\nVideo\nVideo\n\n\n\n\nDNA"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#sequence-prediction-tasks",
    "href": "slides/11-timeseries-cont.html#sequence-prediction-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence prediction tasks",
    "text": "Sequence prediction tasks\n\n\n\n\nClassification:\n\nSpeech recognition\nFraud detection, network intrusion detection\nFault detection and predictive maintenance\nMedical diagnostics\nSentiment analysis\nTopic classification\n\n\n We already know (most of) the tools needed\n\n\n\n\nForecasting (regression of future values)\n\nPredicting weather, energy prices, stock prices\nText generation\n\n\n Need a model that can remember the past\n\n\n\nSequence-to-sequence learning\n\nLanguage translation\nImage captioning\nText summarisation\n\n\n Need a model that can remember the context"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#sequence-classification",
    "href": "slides/11-timeseries-cont.html#sequence-classification",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence classification",
    "text": "Sequence classification\n\nFor images we looked for patterns between neighbouring pixels, in 2D\nFor sequences we equivalently look for patterns between neighbouring elements (e.g. points in time), in 1D\n\nWhile 1D, we can still have multiple channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2D (images)\n\n1D (sequences)\n\n\n\n\nkeras.layers.Conv2D\n\nkeras.layers.Conv1D\n\n\nkeras.layers.Conv2DTranspose\n\nkeras.layers.Conv1DTranspose\n\n\nkeras.layers.MaxPooling2D\n\nkeras.layers.MaxPooling1D"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-1",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-2",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-3",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-4",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-5",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-6",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-6",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-7",
    "href": "slides/11-timeseries-cont.html#convolutions-recap-but-1d-7",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolution-appreciation-slide",
    "href": "slides/11-timeseries-cont.html#convolution-appreciation-slide",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolution appreciation slide",
    "text": "Convolution appreciation slide\nRecall we use the convolution* operation as a translationally invariant pattern detector\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\n\\]\n\n\n\n\n* Again, in signal processing we would call it cross-correlation"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#convolution-appreciation-slide-1",
    "href": "slides/11-timeseries-cont.html#convolution-appreciation-slide-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolution appreciation slide",
    "text": "Convolution appreciation slide\nRecall we use the convolution* operation as a translationally invariant pattern detector\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\n\\]\n\n\n\n\n* Again, in signal processing we would call it cross-correlation"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#sequence-forecasting-predicting-the-future",
    "href": "slides/11-timeseries-cont.html#sequence-forecasting-predicting-the-future",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence forecasting: Predicting the future",
    "text": "Sequence forecasting: Predicting the future\nCNNs are great for classification because of translation invariance\nFor forecasting, we often don’t want this.\n\nNew assumption:\n\nRecent data is more informative than old data"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#seasonality",
    "href": "slides/11-timeseries-cont.html#seasonality",
    "title": "DAT255: Deep learning engineering",
    "section": "Seasonality",
    "text": "Seasonality\nMany timeseries have recurring patters, caused by some physical phenomenon  Collectively talk about this as seasonality\nExample from the textbook: Temperature\n\n\n\n\n\n\nEight years (Fig 13.1)\n\n\n\n\n\n\n\nTen days (Fig 13.2)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#recurrent-neural-networks-rnns",
    "href": "slides/11-timeseries-cont.html#recurrent-neural-networks-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way: Let each node store its previous output"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#recurrent-neural-networks-rnns-1",
    "href": "slides/11-timeseries-cont.html#recurrent-neural-networks-rnns-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way: Let each node store its previous output"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#recurrent-neural-networks-rnns-2",
    "href": "slides/11-timeseries-cont.html#recurrent-neural-networks-rnns-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nRecall that a regular Dense layer computes its output by\noutputs = activation(tf.dot(W, inputs) + b)\n(where W is the weight matrix, inputs is the vector of features and b is the bias vector)\n\nThe recurrent node has two sets of weights:\n\nThe usual ones, call them W_x\nThose to be applied to the previous output, call them W_y\n\n\n\nThe outputs then become\nstate_t = tf.zeros(shape=(num_output_features))\noutputs = []\nfor input_t in input_sequence:  # loop over inputs at time t\n  output_t = activation(tf.dot(W_y, inputs) + tf.dot(W_y, state_t) + b)\n  outputs.append(output_t)\n  state_t = output_t\noutput_sequence = tf.stack(outputs, axis=0)\nImplemented in Keras as keras.layers.SimpleRNN"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#input-and-output-sequences",
    "href": "slides/11-timeseries-cont.html#input-and-output-sequences",
    "title": "DAT255: Deep learning engineering",
    "section": "Input and output sequences",
    "text": "Input and output sequences\n\n\n\n\n\n\nA. Geron: Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#intermezzo-autoregressive-models",
    "href": "slides/11-timeseries-cont.html#intermezzo-autoregressive-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Intermezzo: Autoregressive models",
    "text": "Intermezzo: Autoregressive models\nSimplest possible forecast:\nThe value tomorrow is the same as the value today. \\[\n\\small\ny_i = y_{t-1}\n\\]\n\n\n\n\n\nNumber of rail and bus passengers in Chicago 2019\n\n\n\n\n\n\n\nPartial autocorrelation"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#intermezzo-autoregressive-models-1",
    "href": "slides/11-timeseries-cont.html#intermezzo-autoregressive-models-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Intermezzo: Autoregressive models",
    "text": "Intermezzo: Autoregressive models\n\n\nMore advanced forecast:\nThe value tomorrow is given by a weighted sum of the \\(p\\) previous time steps, plus a noise term\n\\[\n\\small\ny_i = \\sum^p \\varphi_i y_{t-i} + \\epsilon_t\n\\]\n(\\(\\varphi_i\\) are the parameters of the model)\n\n\n\n\nCan add moving average to get an ARMA model, look at differences to get ARIMA, add seasonality to get SARIMA, …\nLots of work on (traditional) statistical time series modelling - usually worth trying out before going to deep learning."
  },
  {
    "objectID": "slides/11-timeseries-cont.html#improved-memory-cells",
    "href": "slides/11-timeseries-cont.html#improved-memory-cells",
    "title": "DAT255: Deep learning engineering",
    "section": "Improved memory cells",
    "text": "Improved memory cells\nIn practice, RNNs suffer from vanishing/exploding gradients during training\n Difficult to make them learn long-term dependencies\n\nCan introduce hidden states which are not the same as the output.\n\n\n\n\n\nTwo most used approaces: LSTMs and GRUs."
  },
  {
    "objectID": "slides/11-timeseries-cont.html#the-long-short-term-memory-lstm-cell",
    "href": "slides/11-timeseries-cont.html#the-long-short-term-memory-lstm-cell",
    "title": "DAT255: Deep learning engineering",
    "section": "The long short-term memory (LSTM) cell",
    "text": "The long short-term memory (LSTM) cell\nAdd long-term memory by having two states in each cell:\nA short-term state \\(\\small\\boldsymbol{h}_t\\) and a long-term state \\(\\small\\boldsymbol{c}_t\\)\n\n\n\n\n\nGates determine data flow – add small networks inside the cell to act as gate operators\n\nkeras.layers.LSTM"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#the-gated-recurrent-unit-gru",
    "href": "slides/11-timeseries-cont.html#the-gated-recurrent-unit-gru",
    "title": "DAT255: Deep learning engineering",
    "section": "The gated recurrent unit (GRU)",
    "text": "The gated recurrent unit (GRU)\nSimplified and somewhat more effective variant:\n\n\n\n\n\nkeras.layers.GRU"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#stacking-recurrent-layers",
    "href": "slides/11-timeseries-cont.html#stacking-recurrent-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Stacking recurrent layers",
    "text": "Stacking recurrent layers\nAs usual, we can increase the capacity by stacking layers.\nNote when building a deep RNN: Intermediate layers should return the entire sequence\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.GRU(32, return_sequences=True)(inputs)\nx = layers.GRU(32, return_sequences=True)(x)\nx = layers.GRU(32)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#training-rnns",
    "href": "slides/11-timeseries-cont.html#training-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Training RNNs",
    "text": "Training RNNs\nSome tricks to efficiently training recurrent networks:\n\n\nUse saturating activation functions (tanh, sigmoid)\nlayers.LSTM(units, activation=\"tanh\", recurrent_activation=\"sigmoid\")\nUse layer normalisation (keras.layers.LayerNormalization) instead of batch normalisation\nAdd recurrent dropout (potentially in addition to regular dropout)\nx = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n\n\n\n\nTest if training runs faster on CPU than on GPU\n\nNVIDIA backend only available if using default arguments for LSTM/GRU layers\nfor loops in recurrent nodes reduces parallelisability\n\nCan optionally unroll for loops (memory intensive):\nx = layers.LSTM(32, unroll=True)(inputs)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#bonus-trick-1-cnn-processing",
    "href": "slides/11-timeseries-cont.html#bonus-trick-1-cnn-processing",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #1: CNN processing",
    "text": "Bonus trick #1: CNN processing\nEven with the previous tricks up our sleeve, getting RNNs to learn patterns over &gt;100 time steps is difficult.\nCan extract small-scale patterns with convolutional layers first, then apply recurrent layers:\nmodel = keras.Sequential([\n  keras.layers.Conv1D(filters=32, kernel_size=4, strides=2, activation=\"relu\"),\n  keras.layers.GRU(32, return_sequences=True)\n  keras.layers.Dense(14)\n])\n(add stride &gt; 1 to downsample)"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#bonus-trick-1-cnn-processing-1",
    "href": "slides/11-timeseries-cont.html#bonus-trick-1-cnn-processing-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #1: CNN processing",
    "text": "Bonus trick #1: CNN processing\nEven with the previous tricks up out sleeve, getting RNNs to learn patterns over &gt;100 time steps is difficult.\nCan extract small-scale patterns with convolutional layers first, then apply recurrent layers\nOr maybe skip the recurrence altogether? WaveNet architecture:\n\n\n\n\n\nkeras.layers.Conv1D(..., padding=\"casual\")  # Look only backwards"
  },
  {
    "objectID": "slides/11-timeseries-cont.html#bonus-trick-2-bidirectional-rnns",
    "href": "slides/11-timeseries-cont.html#bonus-trick-2-bidirectional-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #2: bidirectional RNNs",
    "text": "Bonus trick #2: bidirectional RNNs\nFor time series we expect the most recent data points to be most important\n Chronological ordering makes sense\n\nSometimes this is not the case - for instance for text\n\n\n\nI arrived by bike.\n\n\n\nIch bin mit Fahrrad angekommen.\n\n\n\n\n\n\nCan process sequences both forwards and in reverse by using a bidirectional recurrent layer:\n\n\n\ninputs = keras.Input(shape=(...))\nx = layers.Bidirectional(layers.LSTM(16))(inputs)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/08-object-detection.html#improving-generalisation",
    "href": "slides/08-object-detection.html#improving-generalisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Improving generalisation",
    "text": "Improving generalisation\nRealisation:\n\nWe don’t have infinite training data\nTraining data don’t cover the space of all realistic examples\n\n\nMitigation:\n\nAdd artificially modified duplicates of the training data (while preserving information)\n\n\n\n\n\n\n\n\nCat\n\n\n\n\n\n\n\nRotated cat is still cat\n\n\n\n\n\n\n\nFlipped cat is also cat???"
  },
  {
    "objectID": "slides/08-object-detection.html#augmentation",
    "href": "slides/08-object-detection.html#augmentation",
    "title": "DAT255: Deep learning engineering",
    "section": "Augmentation",
    "text": "Augmentation\nThe benefit of augmentation greatly exceeds the effort involved\n Always recommended to use for computer vision.\n\n\n\n\n\nNo augmentation\n\n\n\n\n\n\n\nkeras.layers.RandomFlip\n\n\n\n\n\n\n\nkeras.layers.RandomRotation\n\n\n\n\n\n\n\nkeras.layers.RandomCrop\n\n\n\n\n\n\n\nkeras.layers.RandomBrightness\n\n\n\n\n\n\n\nkeras.layers.RandomHue\n\n\n\n\n\n\n\nkeras.layers.RandomTranslation\n\n\n\n\n\n\n\nkeras.layers.RandomShear\n\n\n\n\n\n\n\nkeras.layers.Equalization\n\n\n\n\n\n\n\nkeras.layers.RandAugment (“do it all”)"
  },
  {
    "objectID": "slides/08-object-detection.html#more-advanced-network-configurations",
    "href": "slides/08-object-detection.html#more-advanced-network-configurations",
    "title": "DAT255: Deep learning engineering",
    "section": "More advanced network configurations",
    "text": "More advanced network configurations\nGoing beyond the Sequential model"
  },
  {
    "objectID": "slides/08-object-detection.html#the-keras-functional-api",
    "href": "slides/08-object-detection.html#the-keras-functional-api",
    "title": "DAT255: Deep learning engineering",
    "section": "The Keras functional API",
    "text": "The Keras functional API\nConsider the equivalent ways of defining a network:\n\n\nThe Sequential API:\nfrom keras import layers\nmodel = keras.Sequential([\n  layers.Input(shape=input_shape),\n  layers.Conv2D(64, 3, activation=\"relu\"),\n  layers.MaxPooling2D(),\n  layers.Conv2D(128, 3, activation=\"relu\"),\n  layers.Flatten(),\n  layers.Dropout(0.5),\n  layers.Dense(\n    num_classes, activation=\"softmax\"\n  ),\n])\nLayers are stacked in a list\n\nThe Functional API\n\ninputs = layers.Input(shape=input_shape)\nx = layers.Conv2D(64, 3, activation=\"relu\")(inputs)\nx = layers.MaxPooling2D()(x)\nx = layers.Conv2D(128, 3, activation=\"relu\")(x)\nx = layers.Flatten()(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(\n  num_classes, activation=\"softmax\"\n)(x)\n\nmodel = keras.Model(\n  inputs=inputs,\n  outputs=outputs\n)\nLayers are used as functions taking the previous layer as input\nLastly we need to specify the input and output in a Model"
  },
  {
    "objectID": "slides/08-object-detection.html#example-bird-classifier",
    "href": "slides/08-object-detection.html#example-bird-classifier",
    "title": "DAT255: Deep learning engineering",
    "section": "Example: Bird classifier",
    "text": "Example: Bird classifier\n\n\nSay that you for obvious reasons want to classify observations of birds.\nEach data point contains:\n\nPicture of bird\nSize of bird\n(other numerical values related to observation of bird) \n\n\n\n\n\n\n\n\nBird 1. Size: 50 cm\n\n\n\n\n\n\n\nBird 2. Size: 14 cm\n\n\n\n\n\n\n\nBird 3. Size: 63 cm\n\n\n\n\n\n\n\n\nHow to combine this information?"
  },
  {
    "objectID": "slides/08-object-detection.html#example-bird-classifier-1",
    "href": "slides/08-object-detection.html#example-bird-classifier-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Example: Bird classifier",
    "text": "Example: Bird classifier\n\n\nIn the functional API we can easily have two input sources:\ninput1 = keras.layers.Input(shape=(128,128,3))\n\nx1 = keras.layers.Conv2D(64, 3, activation='relu')(input1)\nx1 = keras.layers.MaxPooling2D(2)(x1)\nx1 = keras.layers.Conv2D(64, 3, activation='relu')(x1)\nx1 = keras.layers.MaxPooling2D(2)(x1)\nx1 = keras.layers.Flatten()(x1)\n\ninput2 = keras.layers.Input(shape=(10,))\n\nx2 = keras.layers.Dense(32, activation='relu')(input2)\nx2 = keras.layers.Dense(32, activation='relu')(x2)\n\nconcat = keras.layers.Concatenate()([x1, x2])\n\nx = keras.layers.Dense(64, activation='relu')(concat)\nout = keras.layers.Dense(3, activation='softmax')(x)\n\nmodel = keras.Model(\n  inputs=[input1, input2],\n  outputs=out\n)"
  },
  {
    "objectID": "slides/08-object-detection.html#non-sequential-networks",
    "href": "slides/08-object-detection.html#non-sequential-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Non-sequential networks",
    "text": "Non-sequential networks\n\n\n\n\n\nCan have networks with\n\nMultiple inputs\nMultiple outputs\nArbitrary layer connections\nNetworks-inside-the-network\nLoops (will come to this later)\n\n\n\n\n\n\nLet’s look at some noteworthy architectures for computer vision."
  },
  {
    "objectID": "slides/08-object-detection.html#residual-connections",
    "href": "slides/08-object-detection.html#residual-connections",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual connections",
    "text": "Residual connections\nIn deep learning we are computing composite functions: \\[\ny = f_3(f_2(f_1(x_1, x_2, \\dots, x_n | \\theta_1) | \\theta_2) | \\theta_3)\n\\]\nThese are prone to training problems like exploding or vanishing gradients.\n\nWe can stabilise training with good parameter initialisation, but going beyond ~16 Conv layers (the VGG models *) makes the model worse :/\nWith too many layers, the model gets dominated by noise.\n\n* Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556.\n\n\n\nA method for solving this is residual connections:"
  },
  {
    "objectID": "slides/08-object-detection.html#residual-connections-1",
    "href": "slides/08-object-detection.html#residual-connections-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual connections",
    "text": "Residual connections\nImplementing this branching and recombination can be done with Keras’ functional API:\n\n\n\n\n\n\n\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n# Set aside the residual\nresidual = x\n# Conv block\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nx = layers.MaxPooling2D(2, padding=\"same\")(x)\n# Conv layer with strides=2, so that the shapes match\nresidual = layers.Conv2D(64, 1, strides=2)(residual)\n# Add the block output with the residual \nx = layers.add([x, residual])\n\n\nIf we need to this multiple times, we can write it in a\n\nfor loop\na function\nor as a subclass of keras.layers.Layer"
  },
  {
    "objectID": "slides/08-object-detection.html#residual-networks",
    "href": "slides/08-object-detection.html#residual-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual networks",
    "text": "Residual networks\nWith residual connections in place, we can continue learning even if the gradients in a layer vanishes:\n\n\n\n\n\n\nA. Geron: Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow"
  },
  {
    "objectID": "slides/08-object-detection.html#residual-networks-1",
    "href": "slides/08-object-detection.html#residual-networks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual networks",
    "text": "Residual networks\n\n\n…which enables us to train networks with 100+ layers\n\n\n\n\n\n\n\n\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition."
  },
  {
    "objectID": "slides/08-object-detection.html#densely-connected-convolutional-networks",
    "href": "slides/08-object-detection.html#densely-connected-convolutional-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Densely connected convolutional networks",
    "text": "Densely connected convolutional networks\nHow about adding skip connections (almost) everywhere?\nEnter the DenseNet:\n\n\n\n\n\n\nHuang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition."
  },
  {
    "objectID": "slides/08-object-detection.html#inception-networks",
    "href": "slides/08-object-detection.html#inception-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Inception networks",
    "text": "Inception networks\n\n\nWe can also add width in addition to depth:\nThe inception module features parallel convolutional layers with different kernel size\n\n\n\n\n\nOutput is concatenated and passed on to the next module\n\n\n\n\nGoogLeNet, 2014"
  },
  {
    "objectID": "slides/08-object-detection.html#xception-networks",
    "href": "slides/08-object-detection.html#xception-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Xception networks",
    "text": "Xception networks\nThe Xception (“extreme inception”) architecture relies on depthwise separable convolution layers\n\n\n\n\n\nThese layers are available as keras.layers.SeparableConv2D and can be used just like the regular Conv2D, often with increased performance"
  },
  {
    "objectID": "slides/08-object-detection.html#beyond-cnns-vision-transformers",
    "href": "slides/08-object-detection.html#beyond-cnns-vision-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Beyond CNNs: Vision transformers",
    "text": "Beyond CNNs: Vision transformers\nCan also use methods from LLMs on image input * (get back to this in Ch. 15):\n\n\n\n\n\nHowever, pure ConvNets are still hard to beat **\n\n* Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929 ** Liu, Z., et al. (2022). A convnet for the 2020s. arxiv:2201.03545"
  },
  {
    "objectID": "slides/08-object-detection.html#keras.applications",
    "href": "slides/08-object-detection.html#keras.applications",
    "title": "DAT255: Deep learning engineering",
    "section": "keras.applications",
    "text": "keras.applications\nThe most popular computer vision architectures are available as pre-trained models in keras.applications.\nThese are excellent starting points for\n\nfeature extraction\nfine-tuning\ntransfer learning\n\n\nNote: The different models typically require specific preprocessing:\nIf you want to use\nkeras.applications.Xception()\nyou should process the input images with\nkeras.applications.xception.preprocess_input()"
  },
  {
    "objectID": "slides/08-object-detection.html#feature-extraction",
    "href": "slides/08-object-detection.html#feature-extraction",
    "title": "DAT255: Deep learning engineering",
    "section": "Feature extraction",
    "text": "Feature extraction"
  },
  {
    "objectID": "slides/08-object-detection.html#fine-tuning",
    "href": "slides/08-object-detection.html#fine-tuning",
    "title": "DAT255: Deep learning engineering",
    "section": "Fine-tuning",
    "text": "Fine-tuning\n\n\n\nKeep general patterns learned, but make them more specific to a new task:\n\n\n\n\n\n\nAdd custom network on top of a trained base network\nFreeze the base network\nTrain the custom part\nUnfreeze the base network\nJointly train the entire model"
  },
  {
    "objectID": "slides/08-object-detection.html#other-computer-vision-tasks-next-week",
    "href": "slides/08-object-detection.html#other-computer-vision-tasks-next-week",
    "title": "DAT255: Deep learning engineering",
    "section": "Other computer vision tasks (next week)",
    "text": "Other computer vision tasks (next week)\nConvolutional nets are great for other things than just classification:\n\n\n\n\nObject detection: Localise (several) objects in an image\n\n\n\n\nOriented bounding boxes: Localise and estimate orientation of objects\n\n\n\n\nSemantic segmentation: Classify each pixel onto an object\n\n\n\n\nInstance segmentation: Draw a detailed outline around abjects\n\n\n\n\nPose estimation: Localise distinctive features or parts of an object"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#improving-generalisation",
    "href": "slides/06-advanced-computer-vision.html#improving-generalisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Improving generalisation",
    "text": "Improving generalisation\nRealisation:\n\nWe don’t have infinite training data\nTraining data don’t cover the space of all realistic examples\n\n\nMitigation:\n\nAdd artificially modified duplicates of the training data (while preserving information)\n\n\n\n\n\n\n\n\nCat\n\n\n\n\n\n\n\nRotated cat is still cat\n\n\n\n\n\n\n\nFlipped cat is also cat???"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#augmentation",
    "href": "slides/06-advanced-computer-vision.html#augmentation",
    "title": "DAT255: Deep learning engineering",
    "section": "Augmentation",
    "text": "Augmentation\nThe benefit of augmentation greatly exceeds the effort involved\n Always recommended to use for computer vision.\n\n\n\n\n\nNo augmentation\n\n\n\n\n\n\n\nkeras.layers.RandomFlip\n\n\n\n\n\n\n\nkeras.layers.RandomRotation\n\n\n\n\n\n\n\nkeras.layers.RandomCrop\n\n\n\n\n\n\n\nkeras.layers.RandomBrightness\n\n\n\n\n\n\n\nkeras.layers.RandomHue\n\n\n\n\n\n\n\nkeras.layers.RandomTranslation\n\n\n\n\n\n\n\nkeras.layers.RandomShear\n\n\n\n\n\n\n\nkeras.layers.Equalization\n\n\n\n\n\n\n\nkeras.layers.RandAugment (“do it all”)"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#more-advanced-network-configurations",
    "href": "slides/06-advanced-computer-vision.html#more-advanced-network-configurations",
    "title": "DAT255: Deep learning engineering",
    "section": "More advanced network configurations",
    "text": "More advanced network configurations\nGoing beyond the Sequential model"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#the-keras-functional-api",
    "href": "slides/06-advanced-computer-vision.html#the-keras-functional-api",
    "title": "DAT255: Deep learning engineering",
    "section": "The Keras functional API",
    "text": "The Keras functional API\nConsider the equivalent ways of defining a network:\n\n\nThe Sequential API:\nfrom keras import layers\nmodel = keras.Sequential([\n  layers.Input(shape=input_shape),\n  layers.Conv2D(64, 3, activation=\"relu\"),\n  layers.MaxPooling2D(),\n  layers.Conv2D(128, 3, activation=\"relu\"),\n  layers.Flatten(),\n  layers.Dropout(0.5),\n  layers.Dense(\n    num_classes, activation=\"softmax\"\n  ),\n])\nLayers are stacked in a list\n\nThe Functional API\n\ninputs = layers.Input(shape=input_shape)\nx = layers.Conv2D(64, 3, activation=\"relu\")(inputs)\nx = layers.MaxPooling2D()(x)\nx = layers.Conv2D(128, 3, activation=\"relu\")(x)\nx = layers.Flatten()(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(\n  num_classes, activation=\"softmax\"\n)(x)\n\nmodel = keras.Model(\n  inputs=inputs,\n  outputs=outputs\n)\nLayers are used as functions taking the previous layer as input\nLastly we need to specify the input and output in a Model"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#example-bird-classifier",
    "href": "slides/06-advanced-computer-vision.html#example-bird-classifier",
    "title": "DAT255: Deep learning engineering",
    "section": "Example: Bird classifier",
    "text": "Example: Bird classifier\n\n\nSay that you for obvious reasons want to classify observations of birds.\nEach data point contains:\n\nPicture of bird\nSize of bird\n(other numerical values related to observation of bird) \n\n\n\n\n\n\n\n\nBird 1. Size: 50 cm\n\n\n\n\n\n\n\nBird 2. Size: 14 cm\n\n\n\n\n\n\n\nBird 3. Size: 63 cm\n\n\n\n\n\n\n\n\nHow to combine this information?"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#example-bird-classifier-1",
    "href": "slides/06-advanced-computer-vision.html#example-bird-classifier-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Example: Bird classifier",
    "text": "Example: Bird classifier\n\n\nIn the functional API we can easily have two input sources:\ninput1 = keras.layers.Input(shape=(128,128,3))\n\nx1 = keras.layers.Conv2D(64, 3, activation='relu')(input1)\nx1 = keras.layers.MaxPooling2D(2)(x1)\nx1 = keras.layers.Conv2D(64, 3, activation='relu')(x1)\nx1 = keras.layers.MaxPooling2D(2)(x1)\nx1 = keras.layers.Flatten()(x1)\n\ninput2 = keras.layers.Input(shape=(10,))\n\nx2 = keras.layers.Dense(32, activation='relu')(input2)\nx2 = keras.layers.Dense(32, activation='relu')(x2)\n\nconcat = keras.layers.Concatenate()([x1, x2])\n\nx = keras.layers.Dense(64, activation='relu')(concat)\nout = keras.layers.Dense(3, activation='softmax')(x)\n\nmodel = keras.Model(\n  inputs=[input1, input2],\n  outputs=out\n)"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#non-sequential-networks",
    "href": "slides/06-advanced-computer-vision.html#non-sequential-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Non-sequential networks",
    "text": "Non-sequential networks\n\n\n\n\n\nCan have networks with\n\nMultiple inputs\nMultiple outputs\nArbitrary layer connections\nNetworks-inside-the-network\nLoops (will come to this later)\n\n\n\n\n\n\nLet’s look at some noteworthy architectures for computer vision."
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#residual-connections",
    "href": "slides/06-advanced-computer-vision.html#residual-connections",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual connections",
    "text": "Residual connections\nIn deep learning we are computing composite functions: \\[\ny = f_3(f_2(f_1(x_1, x_2, \\dots, x_n | \\theta_1) | \\theta_2) | \\theta_3)\n\\]\nThese are prone to training problems like exploding or vanishing gradients.\n\nWe can stabilise training with good parameter initialisation, but going beyond ~16 Conv layers (the VGG models *) makes the model worse :/\nWith too many layers, the model gets dominated by noise.\n\n* Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556.\n\n\n\nA method for solving this is residual connections:"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#residual-connections-1",
    "href": "slides/06-advanced-computer-vision.html#residual-connections-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual connections",
    "text": "Residual connections\nImplementing this branching and recombination can be done with Keras’ functional API:\n\n\n\n\n\n\n\ninputs = keras.Input(shape=(32, 32, 3))\nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n# Set aside the residual\nresidual = x\n# Conv block\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nx = layers.MaxPooling2D(2, padding=\"same\")(x)\n# Conv layer with strides=2, so that the shapes match\nresidual = layers.Conv2D(64, 1, strides=2)(residual)\n# Add the block output with the residual \nx = layers.add([x, residual])\n\n\nIf we need to this multiple times, we can write it in a\n\nfor loop\na function\nor as a subclass of keras.layers.Layer"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#residual-networks",
    "href": "slides/06-advanced-computer-vision.html#residual-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual networks",
    "text": "Residual networks\nWith residual connections in place, we can continue learning even if the gradients in a layer vanishes:\n\n\n\n\n\n\nA. Geron: Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#residual-networks-1",
    "href": "slides/06-advanced-computer-vision.html#residual-networks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Residual networks",
    "text": "Residual networks\n\n\n…which enables us to train networks with 100+ layers\n\n\n\n\n\n\n\n\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition."
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#densely-connected-convolutional-networks",
    "href": "slides/06-advanced-computer-vision.html#densely-connected-convolutional-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Densely connected convolutional networks",
    "text": "Densely connected convolutional networks\nHow about adding skip connections (almost) everywhere?\nEnter the DenseNet:\n\n\n\n\n\n\nHuang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition."
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#inception-networks",
    "href": "slides/06-advanced-computer-vision.html#inception-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Inception networks",
    "text": "Inception networks\n\n\nWe can also add width in addition to depth:\nThe inception module features parallel convolutional layers with different kernel size\n\n\n\n\n\nOutput is concatenated and passed on to the next module\n\n\n\n\nGoogLeNet, 2014"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#xception-networks",
    "href": "slides/06-advanced-computer-vision.html#xception-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Xception networks",
    "text": "Xception networks\nThe Xception (“extreme inception”) architecture relies on depthwise separable convolution layers\n\n\n\n\n\nThese layers are available as keras.layers.SeparableConv2D and can be used just like the regular Conv2D, often with increased performance"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#beyond-cnns-vision-transformers",
    "href": "slides/06-advanced-computer-vision.html#beyond-cnns-vision-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Beyond CNNs: Vision transformers",
    "text": "Beyond CNNs: Vision transformers\nCan also use methods from LLMs on image input * (get back to this in Ch. 15):\n\n\n\n\n\nHowever, pure ConvNets are still hard to beat **\n\n* Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929 ** Liu, Z., et al. (2022). A convnet for the 2020s. arxiv:2201.03545"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#keras.applications",
    "href": "slides/06-advanced-computer-vision.html#keras.applications",
    "title": "DAT255: Deep learning engineering",
    "section": "keras.applications",
    "text": "keras.applications\nThe most popular computer vision architectures are available as pre-trained models in keras.applications.\nThese are excellent starting points for\n\nfeature extraction\nfine-tuning\ntransfer learning\n\n\nNote: The different models typically require specific preprocessing:\nIf you want to use\nkeras.applications.Xception()\nyou should process the input images with\nkeras.applications.xception.preprocess_input()"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#feature-extraction",
    "href": "slides/06-advanced-computer-vision.html#feature-extraction",
    "title": "DAT255: Deep learning engineering",
    "section": "Feature extraction",
    "text": "Feature extraction"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#fine-tuning",
    "href": "slides/06-advanced-computer-vision.html#fine-tuning",
    "title": "DAT255: Deep learning engineering",
    "section": "Fine-tuning",
    "text": "Fine-tuning\n\n\n\nKeep general patterns learned, but make them more specific to a new task:\n\n\n\n\n\n\nAdd custom network on top of a trained base network\nFreeze the base network\nTrain the custom part\nUnfreeze the base network\nJointly train the entire model"
  },
  {
    "objectID": "slides/06-advanced-computer-vision.html#other-computer-vision-tasks-next-week",
    "href": "slides/06-advanced-computer-vision.html#other-computer-vision-tasks-next-week",
    "title": "DAT255: Deep learning engineering",
    "section": "Other computer vision tasks (next week)",
    "text": "Other computer vision tasks (next week)\nConvolutional nets are great for other things than just classification:\n\n\n\n\nObject detection: Localise (several) objects in an image\n\n\n\n\nOriented bounding boxes: Localise and estimate orientation of objects\n\n\n\n\nSemantic segmentation: Classify each pixel onto an object\n\n\n\n\nInstance segmentation: Draw a detailed outline around abjects\n\n\n\n\nPose estimation: Localise distinctive features or parts of an object"
  },
  {
    "objectID": "slides/04-training.html#the-parameters-of-a-neural-network",
    "href": "slides/04-training.html#the-parameters-of-a-neural-network",
    "title": "DAT255: Deep learning engineering",
    "section": "The parameters of a neural network",
    "text": "The parameters of a neural network\nRecall the simple formulation of a machine learning model:\n\n\\[\n\\large \\hat{\\color{DarkBlue}{y}} = \\color{Purple}{f}(\\color{DarkOrange}{\\mathbf{x}}, \\boldsymbol{\\color{teal}{\\theta}})\n\\]\n\nwhere\n\n\\(\\hat{\\color{DarkBlue}{y}}\\) is the prediction\n\\(\\color{Purple}{f}\\) is a mathematical function, in our case a neural network\n\\(\\color{DarkOrange}{\\mathbf{x}}\\) is a data point\n\\(\\boldsymbol{\\color{teal}{\\theta}}\\) are parameters of the model"
  },
  {
    "objectID": "slides/04-training.html#the-parameters-of-a-neural-network-1",
    "href": "slides/04-training.html#the-parameters-of-a-neural-network-1",
    "title": "DAT255: Deep learning engineering",
    "section": "The parameters of a neural network",
    "text": "The parameters of a neural network\n\n\n\n\n\nThe parameters of the simple fully-connected (dense) network are\n\none weight per connection (line)\none bias term per node (circle)\n\n(for other types of layers, there are other types of parameters)"
  },
  {
    "objectID": "slides/04-training.html#the-parameters-of-a-neural-network-2",
    "href": "slides/04-training.html#the-parameters-of-a-neural-network-2",
    "title": "DAT255: Deep learning engineering",
    "section": "The parameters of a neural network",
    "text": "The parameters of a neural network"
  },
  {
    "objectID": "slides/04-training.html#the-activation-function",
    "href": "slides/04-training.html#the-activation-function",
    "title": "DAT255: Deep learning engineering",
    "section": "The activation function",
    "text": "The activation function\n\n\n\nWhat separates a neural network from standard linear regression is the non-linear activation function.\n\n(And not that we have many layers – stacking linear functions is still a linear function)\n\\[\n\\small\n\\begin{aligned}\n\\color{DarkBlue}{f}(x) &= 2x + 3  & (\\mathrm{linear}) \\\\\n\\color{Purple}{g}(x) &= 5x -1 & (\\mathrm{linear}) \\\\\n\\color{DarkBlue}{f}(\\color{Purple}{g}(x)) &= 2(5x-1) +3 &  \\\\\n&= 10x + 1 & (\\mathrm{also\\;linear})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-training.html#activation-function",
    "href": "slides/04-training.html#activation-function",
    "title": "DAT255: Deep learning engineering",
    "section": "Activation function",
    "text": "Activation function\n\n\n\n\n\nA good activation function is\n\nNonlinear\nDifferentiable*\nSwitches from off for negative inputs to on for positive inputs\n\n\n\n\n\n\nSigmoid\n\n\n\n\n\ntanh\n\n\n\n\n\nReLU\n\n\n\n\n\nGELU\n\n\n\n\n\nSwish\n\n\n\n\n\n\n\n\n*At least piecewise differentiable"
  },
  {
    "objectID": "slides/04-training.html#last-layer-activation-functions",
    "href": "slides/04-training.html#last-layer-activation-functions",
    "title": "DAT255: Deep learning engineering",
    "section": "Last layer activation functions",
    "text": "Last layer activation functions\nIn the final layer of the network, we need to choose an activation function that suits our task\n\nRegression:\nNo activation – just sum the weighted connections. Output range \\((-\\infty, \\infty)\\) Also called linear activation.  keras.layers.Dense(units, activation=None)\nClassification:\nNeed something with output range \\([0, 1]\\)\n\nBinary classification: Use sigmoid keras.layers.Dense(1, activation='sigmoid)\nMulticlass: Use softmax keras.layers.Dense(10, activation='softmax') (remember one-hot encoding)"
  },
  {
    "objectID": "slides/04-training.html#finding-optimal-parameters",
    "href": "slides/04-training.html#finding-optimal-parameters",
    "title": "DAT255: Deep learning engineering",
    "section": "Finding optimal parameters",
    "text": "Finding optimal parameters\n\n\n\n\n\nFor large networks we get a huge number of parameters\nNeed a clever way to optimise all at the same time.\n\nThe solution is backpropagation, which is relies on the network output being differentiable with respect to its parameters."
  },
  {
    "objectID": "slides/04-training.html#backpropagation",
    "href": "slides/04-training.html#backpropagation",
    "title": "DAT255: Deep learning engineering",
    "section": "Backpropagation",
    "text": "Backpropagation\nStep 1:\n\n\n\n\n\n\n\n\n\n\n\nRandomly initialise parameters\n\n\n\nRun a forward pass on a mini-batch, i.e. compute predictions, but keep track of the output for each node"
  },
  {
    "objectID": "slides/04-training.html#backpropagation-1",
    "href": "slides/04-training.html#backpropagation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Backpropagation",
    "text": "Backpropagation\nStep 2:\n\n\n\n\n\n\n\\[\n\\mathrm{loss}(\\hat{y}, y)\n\\]\n\n\nCompute the loss, which measures how big the error is"
  },
  {
    "objectID": "slides/04-training.html#backpropagation-2",
    "href": "slides/04-training.html#backpropagation-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Backpropagation",
    "text": "Backpropagation\nStep 3:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute how much each parameter contributed to the loss (i.e. compute the gradient for each parameter)\n\nStart at the last layer and move backwards (backwards pass)"
  },
  {
    "objectID": "slides/04-training.html#backpropagation-3",
    "href": "slides/04-training.html#backpropagation-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Backpropagation",
    "text": "Backpropagation\nStep 4:\n\n\n\n\n\n\nRun one step of gradient descent to find better values for all parameters"
  },
  {
    "objectID": "slides/04-training.html#gradient-descent",
    "href": "slides/04-training.html#gradient-descent",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradient descent",
    "text": "Gradient descent\n\n\n\n\n\n\n\nThe gradient points towards direction of maximum increase of the loss function.\nWe want to find the minimum, so need the negative gradient.\n\\[\n\\nabla \\color{MediumVioletRed}{L}(\\color{teal}{\\boldsymbol{\\theta}}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_0} \\\\\n  \\vdots \\\\\n  \\frac{\\partial \\color{MediumVioletRed}{L}}{\\partial \\color{teal}{\\theta}_n} \\\\\n\\end{bmatrix}\n\\]\n\n\n Vector in parameterspace\n\n\n Gradient"
  },
  {
    "objectID": "slides/04-training.html#gradient-descent-1",
    "href": "slides/04-training.html#gradient-descent-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradient descent",
    "text": "Gradient descent\n\n\n\n\n\n\n\nWith the gradient in place, we take steps downward (along the negative gradient), towards the optimal solution:\n\\[\n\\small\n\\boldsymbol{\\color{teal}{\\theta}}^{n+1} = \\boldsymbol{\\color{teal}{\\theta}}^n - \\eta \\nabla \\color{Purple}{L}\n\\]\nHere \\(\\eta\\) is the learning rate"
  },
  {
    "objectID": "slides/04-training.html#learning-rate",
    "href": "slides/04-training.html#learning-rate",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning rate",
    "text": "Learning rate\n\n\n\nLearning rate is a hyperparameter\n\n\n\n\n\n\n\n\n\nTo small (slow convergence)\n\n\n\n\n\n\n\nJust right\n\n\n\n\n\n\n\nTo high (no convergence)"
  },
  {
    "objectID": "slides/04-training.html#practical-problems",
    "href": "slides/04-training.html#practical-problems",
    "title": "DAT255: Deep learning engineering",
    "section": "Practical problems",
    "text": "Practical problems"
  },
  {
    "objectID": "slides/04-training.html#practical-problems-1",
    "href": "slides/04-training.html#practical-problems-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Practical problems",
    "text": "Practical problems\n\n\nLocalminimum -&gt; bad predictions"
  },
  {
    "objectID": "slides/04-training.html#practical-problems-2",
    "href": "slides/04-training.html#practical-problems-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Practical problems",
    "text": "Practical problems\n\n\nLocalminimum -&gt; bad predictions\n\n\nPlateau -&gt; slow convergence\n\n\nWill try to solve theseproblems next week"
  },
  {
    "objectID": "slides/04-training.html#vanishing-and-exploding-gradients",
    "href": "slides/04-training.html#vanishing-and-exploding-gradients",
    "title": "DAT255: Deep learning engineering",
    "section": "Vanishing and exploding gradients",
    "text": "Vanishing and exploding gradients\nWhen backpropagation steps through the network layers, we can get unfortunate amplification effects:\n\nVanishing gradients:    Gradients go towards zero  no learning\nExploding gradients:    Gradients go towards infinity  no learning\n\n\nGet improved training stability if we can make the variance of the output of a layer to be similar to the variance or the input:"
  },
  {
    "objectID": "slides/04-training.html#some-tricks",
    "href": "slides/04-training.html#some-tricks",
    "title": "DAT255: Deep learning engineering",
    "section": "Some tricks",
    "text": "Some tricks\n\n\nCommon apporaches to avoid vanishing/exploding gradients:\n\nChoose a non-saturating activation function (like ReLU)\n\n\n\nInitialise each layer’s parameters according to number of input and output connections\n\n\n\n\n\nInitialisation method\nActivation function\n\n\n\n\nGlorot\nNone, tanh, sigmoid, softmax\n\n\nHe (Kaiming)\nReLU, GELU, Swish, …\n\n\n\n\n\n\n\n\n\n\n\n\nSigmoid\n\n\n\n\n\n\n\n\n\nReLU"
  },
  {
    "objectID": "slides/04-training.html#some-tricks-1",
    "href": "slides/04-training.html#some-tricks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Some tricks",
    "text": "Some tricks\n\n\nCommon apporaches to avoid vanishing/exploding gradients:\n\nChoose a non-saturating activation function (like ReLU)\n\n\n\n\n\nInitialise each layer’s parameters according to number of input and output connections\n\n\n\n\n\nInitialisation method\nActivation function\n\n\n\n\nGlorot\nNone, tanh, sigmoid, softmax\n\n\nHe (Kaiming)\nReLU, GELU, Swish, …\n\n\n\n\n\nkeras.layers.Dense(\n    units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    lora_rank=None,\n    **kwargs\n)"
  },
  {
    "objectID": "slides/04-training.html#some-tricks-2",
    "href": "slides/04-training.html#some-tricks-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Some tricks",
    "text": "Some tricks\n\n\nCommon apporaches to avoid vanishing/exploding gradients:\n\nChoose a non-saturating activation function (like ReLU)\n\n\n\n\n\nInitialise each layer’s parameters according to number of input and output connections\n\n\n\n\n\nAdd normalisation layers to the model (this is in Ch. 9 (next week) but metioning it now already)"
  },
  {
    "objectID": "slides/04-training.html#normalisation-layers",
    "href": "slides/04-training.html#normalisation-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Normalisation layers",
    "text": "Normalisation layers\n\n\nMost common: keras.layers.BatchNormalisation\n\nFrom the documentation: (read the textbook for further details)\n\nBatch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\nImportantly, batch normalization works differently during training and during inference.\n\n\n\n\n\n\nkeras.layers.BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=\"zeros\",\n    gamma_initializer=\"ones\",\n    moving_mean_initializer=\"zeros\",\n    moving_variance_initializer=\"ones\",\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    synchronized=False,\n    **kwargs\n)\n\n Comes with sensible defaults"
  },
  {
    "objectID": "slides/04-training.html#regularisation",
    "href": "slides/04-training.html#regularisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Regularisation",
    "text": "Regularisation\nThe usual L1 and L2 regularisation can be applied to neural network nodes\nTechnically three different options for where to add it (see docs):\nlayer = keras.layers.Dense(\n    units=64, \n    kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4),\n    bias_regularizer=keras.regularizers.L2(1e-4),\n    activity_regularizer=keras.regularizers.L1(1e-5)\n)"
  },
  {
    "objectID": "slides/04-training.html#dropout-regularisation",
    "href": "slides/04-training.html#dropout-regularisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Dropout regularisation",
    "text": "Dropout regularisation\nOne of the most common regularisation techniques is very simple:\nAt each training step, randomly remove a fraction of the neurons\n\n\n\n\n\nPrevents neuron co-adaptation\n(should be enabled during training time only)"
  },
  {
    "objectID": "slides/04-training.html#dropout-regularisation-1",
    "href": "slides/04-training.html#dropout-regularisation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Dropout regularisation",
    "text": "Dropout regularisation\nThe rate adjusts the percentage of nodes dropped\n\n\n\nkeras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n\n\n\nAfter convolutional layers it is recommended to rather use spatial dropout, which drops entire feature maps\n\n\n\nkeras.layers.SpatialDropout2D(\n    rate, data_format=None, seed=None, name=None, dtype=None\n)"
  },
  {
    "objectID": "slides/04-training.html#putting-together-an-improved-network",
    "href": "slides/04-training.html#putting-together-an-improved-network",
    "title": "DAT255: Deep learning engineering",
    "section": "Putting together an improved network",
    "text": "Putting together an improved network\nfrom keras.layers import (\n  Input, Rescaling, Conv2D, BatchNormalization,\n  MaxPooling2D, Activation, Dropout, Dense\n)\n\nmodel = keras.Sequential([\n  keras.Input(shape=(128, 128, 3)),\n  Rescaling(1.0 / 255),\n  Conv2D(128, kernel_size=3, kernel_initializer=\"he_uniform\", padding=\"same\"),\n  BatchNormalization(),\n  Activation(\"relu\"),\n  MaxPooling2D(3, padding=\"same\"),\n  # ...\n  # (more layers)\n  # ...\n  Conv2D(128, kernel_size=3, kernel_initializer=\"he_uniform\", padding=\"same\"),\n  BatchNormalization(),\n  layers.Activation(\"relu\"),\n  MaxPooling2D(3, padding=\"same\"),\n  Flatten(),\n  Dropout(0.3),\n  Dense(num_classes, activation=\"softmax\"),\n])"
  },
  {
    "objectID": "slides/04-training.html#best-practices",
    "href": "slides/04-training.html#best-practices",
    "title": "DAT255: Deep learning engineering",
    "section": "Best practices",
    "text": "Best practices\nWith the choice of\n\nArchitecture (layers and nodes)\nParameter initialisers\nActivation functions\nRegularisation\nOptimisation algorithm\nLearning rate and scheduling\n\nthe search space for finding the optimal solution is huge\nGuidelines from this and next week are meant to save time, but are not absolute.\n(Better guidelines will come along the next few years anyway)"
  },
  {
    "objectID": "slides/02-tools.html#machine-learning",
    "href": "slides/02-tools.html#machine-learning",
    "title": "DAT255: Deep learning engineering",
    "section": "Machine learning",
    "text": "Machine learning\n\n\n\n\n\n\nChollet Fig 1.2"
  },
  {
    "objectID": "slides/02-tools.html#classification",
    "href": "slides/02-tools.html#classification",
    "title": "DAT255: Deep learning engineering",
    "section": "Classification",
    "text": "Classification\n\n\n\n\n\n\nChollet Fig 1.4\n\n\nClassification threshold:\nx &lt; 0.5  \nx &gt; 0.5"
  },
  {
    "objectID": "slides/02-tools.html#section",
    "href": "slides/02-tools.html#section",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "TensorFlow playground"
  },
  {
    "objectID": "slides/02-tools.html#what-do-we-need-from-our-tools",
    "href": "slides/02-tools.html#what-do-we-need-from-our-tools",
    "title": "DAT255: Deep learning engineering",
    "section": "What do we need from our tools",
    "text": "What do we need from our tools\n\n\n\nA good deep learning framework should provide\n\nA data structure for representing \\(\\small{}N\\)-dimensional arrays (\\(\\small N \\in \\{0, 1, 2, \\dots\\}\\)), aka tensors\nA way to compute derivatives of arbitrary functions\nA way to run computations on hardware accelerators, such as GPUs\n\n\n\n\nThere are many of these; luckily most have converged to a “NumPy-like” API.\nRead about similarities and differences in Ch. 3 (optional)"
  },
  {
    "objectID": "slides/02-tools.html#frameworks",
    "href": "slides/02-tools.html#frameworks",
    "title": "DAT255: Deep learning engineering",
    "section": "Frameworks",
    "text": "Frameworks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-level\n\n\nCompute\n\n\nSupporting"
  },
  {
    "objectID": "slides/02-tools.html#frameworks-1",
    "href": "slides/02-tools.html#frameworks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Frameworks",
    "text": "Frameworks\n\n\n\npython is the de-facto language for deep learning\nIn case you need a refresher, look at e.g.\n\nKaggle Learn: https://www.kaggle.com/learn/python\nGoogle Edu: https://developers.google.com/edu/python\n\n\nTechnically, we use python as a configuration language while the framework backends are C++/CUDA (but we won’t touch this)\nFor deployment, there are hooks to Haskell / C# / Julia / Java / R / Ruby / Rust / Scala / Perl and others\nWe assume you have been exposed to NumPy – although it’s no requirement"
  },
  {
    "objectID": "slides/02-tools.html#this-week",
    "href": "slides/02-tools.html#this-week",
    "title": "DAT255: Deep learning engineering",
    "section": "This week",
    "text": "This week\n\n\n\n\n\n\n\nEnvironment setup: Check our GitHub\nIntro to TensorFlow:We look at this today"
  },
  {
    "objectID": "slides/02-tools.html#computing-resources",
    "href": "slides/02-tools.html#computing-resources",
    "title": "DAT255: Deep learning engineering",
    "section": "Computing resources",
    "text": "Computing resources\nMany exercises in this course are compute intensive, and will benefit from a hardware accelerator (i.e. a GPU)\nSome options:\n\nYour own computer (NVIDIA GPU or M-series Mac)\nCloud services\n\nGoogle Colab: T4 for free\nKaggle Notebooks: P100 for free\n(and others)\n\nResearch group hardware If you are connected to some research group at HVL/UiB"
  },
  {
    "objectID": "slides/02-tools.html#low-level-tensorflow",
    "href": "slides/02-tools.html#low-level-tensorflow",
    "title": "DAT255: Deep learning engineering",
    "section": "Low-level TensorFlow",
    "text": "Low-level TensorFlow\nMost things work like NumPy, but with the benefit of GPU support and JIT compilation.\nThe core object is the Tensor, which is basically a multidimensional array.\n\nNumPy\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.array([[1,2,3], [4,5,6]], dtype=np.float32)\n&gt;&gt;&gt; print(x)\n[[1. 2. 3.]\n [4. 5. 6.]]\n\n\nTensorFlow\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; x = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\n&gt;&gt;&gt; print(x)\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)"
  },
  {
    "objectID": "slides/02-tools.html#low-level-tensorflow-the-tensor",
    "href": "slides/02-tools.html#low-level-tensorflow-the-tensor",
    "title": "DAT255: Deep learning engineering",
    "section": "Low-level TensorFlow: The Tensor",
    "text": "Low-level TensorFlow: The Tensor\nTensors are immutable, and are useful for storing constant values such as input data\nFor values that will be updated (such as model weights), use a Variable:\n&gt;&gt;&gt; y = tf.Variable([[1,2,3], [4,5,6]], dtype=tf.float32, name=\"My first variable\")\n&gt;&gt;&gt; y[0,1].assign(50)\n&lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 1., 50.,  3.],\n       [ 4.,  5.,  6.]], dtype=float32)&gt;"
  },
  {
    "objectID": "slides/02-tools.html#low-level-tensorflow-simple-operations",
    "href": "slides/02-tools.html#low-level-tensorflow-simple-operations",
    "title": "DAT255: Deep learning engineering",
    "section": "Low-level TensorFlow: Simple operations",
    "text": "Low-level TensorFlow: Simple operations\nBasic math is accessed through operators\n&gt;&gt;&gt; a = b + c       # Element-wise addition\n&gt;&gt;&gt; a = b * c       # Element-wise multiplication (Hadamard product)\n&gt;&gt;&gt; a = b @ c       # Matrix multiplication\nwhile more complicated stuff is available as functions in tf.math\n\nNotice in particular the reduce_ functions, which look different from NumPy:\n&gt;&gt;&gt; x = tf.constant([[1,2,3], [4,5,6]]); print(x)\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n&gt;&gt;&gt; tf.math.reduce_sum(x, axis=0)\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)&gt;\n\n&gt;&gt;&gt; tf.math.reduce_sum(x, axis=1)\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 6, 15], dtype=int32)&gt;\n\n&gt;&gt;&gt; tf.math.reduce_sum(x, axis=None)\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=21&gt;"
  },
  {
    "objectID": "slides/02-tools.html#low-level-tensorflow-shapes",
    "href": "slides/02-tools.html#low-level-tensorflow-shapes",
    "title": "DAT255: Deep learning engineering",
    "section": "Low-level TensorFlow: Shapes",
    "text": "Low-level TensorFlow: Shapes\nBroadcasting works like in NumPy:\n&gt;&gt;&gt; x = tf.constant([1,2,3], dtype=tf.float32)\n&gt;&gt;&gt; x + 1\n&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 3., 4.], dtype=float32)&gt;\nSame for shapes:"
  },
  {
    "objectID": "slides/02-tools.html#a-typical-shape-128-299-299-3",
    "href": "slides/02-tools.html#a-typical-shape-128-299-299-3",
    "title": "DAT255: Deep learning engineering",
    "section": "A typical shape: [128, 299, 299, 3]",
    "text": "A typical shape: [128, 299, 299, 3]\nImages are typically represented as [height, width, channel]\n\n\n\n\n\n\nDuring model training we want to do minibatch gradient descent, and load a subset of the data at a time\n This adds a fourth batch dimension: [batch, height, width, channel]\nWhen processing single data points, we often need to add or remove it:\nimg = tf.expand_dims(img, 0)    # [24, 24, 3]    -&gt; [1, 24, 24, 3]\nimg = tf.squeeze(img)           # [1, 24, 24, 3] -&gt; [24, 24, 3]"
  },
  {
    "objectID": "slides/02-tools.html#run-computations-on-a-gpu",
    "href": "slides/02-tools.html#run-computations-on-a-gpu",
    "title": "DAT255: Deep learning engineering",
    "section": "Run computations on a GPU",
    "text": "Run computations on a GPU\nTensorFlow will automatically try to use the fastest compute device available\nTensor operations are typically faster when parallelised on a GPU\n\n\n\nWhile this is automatic, it can also be forced:\nwith tf.device('CPU:0'):\n  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n  b = tf.Variable([[1.0, 2.0, 3.0]])\n\nwith tf.device('GPU:0'):\n  k = a * b\n\nprint(k)"
  },
  {
    "objectID": "slides/02-tools.html#automatic-differentiation",
    "href": "slides/02-tools.html#automatic-differentiation",
    "title": "DAT255: Deep learning engineering",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nLet’s try to compute this derivative:\n\\[\n\\small\n\\frac{\\mathrm{d}}{\\mathrm{d}x} \\Big|_{x=1} \\; x^2 + 2x - 5\n\\]\n\n\\[\n\\small\n= 2x + 2 \\big|_{x=1} = 2 \\cdot 1 + 2 = 4\n\\]\n\n\nTurns out TensorFlow can do it for us:\ndef f(x):\n  return x**2 + 2*x - 5\n\nx = tf.Variable(1.0)\n\nwith tf.GradientTape() as tape:\n  y = f(x)\n\nd_dx = tape.gradient(y, x)\nprint(d_dx)\n\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;"
  },
  {
    "objectID": "slides/02-tools.html#solving-linear-regression-with-autodiff",
    "href": "slides/02-tools.html#solving-linear-regression-with-autodiff",
    "title": "DAT255: Deep learning engineering",
    "section": "Solving linear regression with autodiff",
    "text": "Solving linear regression with autodiff\n\n\nLet’s say we have some data points \\(\\small\\mathbf(X)\\) and want to fit a straight line to them. This linear model would look like\n\\[\n\\small\n\\begin{align}\n\\hat{\\color{DarkBlue}{y}} &= \\color{teal}{\\theta}_0\n+ \\color{teal}{\\theta}_1 \\color{DarkOrange}{x}_1\n+ \\cdots\n+ \\color{teal}{\\theta}_n \\color{DarkOrange}{x}_n \\\\\n&= \\boldsymbol{\\color{teal}{\\theta}}^{\\intercal} \\cdot \\color{DarkOrange}{\\mathbf{x}}\n\\end{align}\n\\]\nfor \\(n\\) features, with \\(\\small\\boldsymbol{\\color{teal}{\\theta}}\\) being the parameters of the model.\n\n\n\n\n\nOr in code: y_pred = theta.T.dot(X)      (or even y_pred = theta.dot(X))\n\n\n\nNow need a loss function (error function) to tell how much we missed by, for instance mean squared error (MSE):\n\\[\n\\small\n\\begin{align}\n\\color{MediumVioletRed}{L}_{\\mathrm{MSE}}\n&= \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\boldsymbol{\\color{teal}{\\theta}}^{\\intercal} \\color{DarkOrange}{\\mathbf{x}}^{(i)} - \\color{DarkBlue}{y}^{(i)}\\right)^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/02-tools.html#solving-linear-regression-with-autodiff-1",
    "href": "slides/02-tools.html#solving-linear-regression-with-autodiff-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Solving linear regression with autodiff",
    "text": "Solving linear regression with autodiff\nLet’s write the loss function in TensorFlow:\n\n\n\n\n\n\\[\n\\small\n\\begin{align}\n\\color{MediumVioletRed}{L}_{\\mathrm{MSE}}\n&= \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\boldsymbol{\\color{teal}{\\theta}}^{\\intercal} \\color{DarkOrange}{\\mathbf{x}}^{(i)} - \\color{DarkBlue}{y}^{(i)}\\right)^2\n\\end{align}\n\\]\n\n\n\n\n\n\n\ndef mse(y_pred, y_true):\n  return tf.reduce_mean(\n    tf.math.square(y_pred - y_true)\n  )\n\n\n\n\n\n\n\n\n\nNotice this is a differentiable function.\n\n\n\n\n\nSo we have\n\nA differentiable function (mean squared error)\nA tool that can compute derivatives (TensorFlow)\n\nwhich means we are all set to do gradient descent."
  },
  {
    "objectID": "slides/02-tools.html#gradient-descent",
    "href": "slides/02-tools.html#gradient-descent",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradient descent",
    "text": "Gradient descent\nRecalling that minimum loss (error) is where the derivative of the function is zero, let’s find the optimal parameters theta numerically:\n\n\n\ntheta = tf.Variable([0.21, 1.43]) # random values\n\nfor epoch in range(10):\n\n  with tf.GradientTape() as tape:\n    y_pred = X.dot(theta)\n    loss = mse(y_pred, y_true)\n\n  better_theta = theta - learning_rate * tape.gradient(loss, theta)\n\n  theta.assign(better_theta)\n\n\n\n\nThis loop is at the core of all neural network training\n(but with Keras we don’t write it out explicitly)"
  },
  {
    "objectID": "slides/02-tools.html#artificial-neural-networks",
    "href": "slides/02-tools.html#artificial-neural-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\n\n\n\n\n\n\n\n\n\nThe base element of a neural network is the neuron, which looks a lot like multiple linear regression: \\[\n\\small\na = b + \\sum_{i=1}^{M} w_ix_i\n\\]\nBut we can solve nonlinear problems if we add a nonlinear transformation \\(\\small{}f\\),\n\\[\n\\small\ny = f(a)\n\\]\nwhich we call an activation function.\n\n\nAgain we usually don’t bother writing this computation out explicitly, but rather interact with Keras layers.\nThis one is called keras.layers.Dense, but we’ll look at several types next week"
  },
  {
    "objectID": "slides/02-tools.html#keras",
    "href": "slides/02-tools.html#keras",
    "title": "DAT255: Deep learning engineering",
    "section": "Keras",
    "text": "Keras\nThe Keras framework contains all the high-level components we need to construct and train a neural network:\n\nkeras.layers: Different types of layers and activation functions\nkeras.callbacks: Monitor, modify or stop the training process\nkeras.optimizers: Optimisation algorithms\nkeras.metrics: Performance metrics\nkeras.losses: Loss functions\nkeras.datasets: Small datasets for testing\nkeras.applications: Pre-trained networks for different tasks"
  },
  {
    "objectID": "slides/02-tools.html#referansegruppe",
    "href": "slides/02-tools.html#referansegruppe",
    "title": "DAT255: Deep learning engineering",
    "section": "Referansegruppe",
    "text": "Referansegruppe"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DAT255 lecture notes",
    "section": "",
    "text": "Published after each lecture week – see Canvas for additional material.\n\nWeek 3\n1: Monday: Welcome and introduction\n2: Tuesday: The tools of deep learning\n\n\nWeek 4\n3: Monday: Computer vision and the concepts of layers\n4: Tuesday: Training deep neural networks\n\n\nWeek 5\n5: Monday: Optimisation algorithms and learning rates\n6: Tuesday: Augmentation and advanced computer vision\n\n\nWeek 6\n7: Monday: Image segmentation\n\n8: Tuesday: Object detection\n\n\nWeek 7\n9: Monday: Sequences, time series, recurrent networks"
  },
  {
    "objectID": "slides/01-welcome.html#what-can-you-do-with-deep-learning",
    "href": "slides/01-welcome.html#what-can-you-do-with-deep-learning",
    "title": "DAT255: Deep learning engineering",
    "section": "What can you do with deep learning?",
    "text": "What can you do with deep learning?"
  },
  {
    "objectID": "slides/01-welcome.html#stable-diffusion-dall-e",
    "href": "slides/01-welcome.html#stable-diffusion-dall-e",
    "title": "DAT255: Deep learning engineering",
    "section": "Stable Diffusion / DALL-E",
    "text": "Stable Diffusion / DALL-E\n\n\nVideo"
  },
  {
    "objectID": "slides/01-welcome.html#sora-openai",
    "href": "slides/01-welcome.html#sora-openai",
    "title": "DAT255: Deep learning engineering",
    "section": "Sora (OpenAI)",
    "text": "Sora (OpenAI)\n\n\nVideo\n\n\nVideo\n\n\nVideo\n\n\nVideo\n\n\nVideo"
  },
  {
    "objectID": "slides/01-welcome.html#segment-anything",
    "href": "slides/01-welcome.html#segment-anything",
    "title": "DAT255: Deep learning engineering",
    "section": "Segment anything",
    "text": "Segment anything\n\nhttps://ai.meta.com/sam2/"
  },
  {
    "objectID": "slides/01-welcome.html#notebooklm",
    "href": "slides/01-welcome.html#notebooklm",
    "title": "DAT255: Deep learning engineering",
    "section": "NotebookLM",
    "text": "NotebookLM\n\nVideo"
  },
  {
    "objectID": "slides/01-welcome.html#section-3",
    "href": "slides/01-welcome.html#section-3",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "suno.com"
  },
  {
    "objectID": "slides/01-welcome.html#cursor",
    "href": "slides/01-welcome.html#cursor",
    "title": "DAT255: Deep learning engineering",
    "section": "Cursor",
    "text": "Cursor\n\nVideo"
  },
  {
    "objectID": "slides/01-welcome.html#section-4",
    "href": "slides/01-welcome.html#section-4",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Video\n\n\nVideo"
  },
  {
    "objectID": "slides/01-welcome.html#practical-things-in-dat255",
    "href": "slides/01-welcome.html#practical-things-in-dat255",
    "title": "DAT255: Deep learning engineering",
    "section": "Practical things in DAT255",
    "text": "Practical things in DAT255"
  },
  {
    "objectID": "slides/01-welcome.html#textbook",
    "href": "slides/01-welcome.html#textbook",
    "title": "DAT255: Deep learning engineering",
    "section": "Textbook",
    "text": "Textbook\nF. Chollet: Deep Learning with Python, 3rd Edition\n\nRead it for free at deeplearningwithpython.io\n\nChapters:\n\n\n\n\nWhat is deep learning?\nNeural network basics\nIntro to DL frameworks\nClassification and regression\nFundamentals of ML\nML workflow\nKeras deep dive\nImage classifiction\nConvNet architectures\nInterpreting models\n\n\n\n\n\nImage segmentation\nObject detection\nTimeseries forecasting\nText classification\nLanguage models\nText generation\nImage generation\nBest practices\nFuture of AI\nSummary"
  },
  {
    "objectID": "slides/01-welcome.html#extra-resources",
    "href": "slides/01-welcome.html#extra-resources",
    "title": "DAT255: Deep learning engineering",
    "section": "Extra resources",
    "text": "Extra resources\n\nFramework documentation:\n\nThe Keras documentation and examples\nThe TensorFlow documentation and examples\n\nVarious research blogs\nOther reputable internet material\n\nYou are expected to research certain topics and find supporting material yourself (especially for the project work)"
  },
  {
    "objectID": "slides/01-welcome.html#exercises",
    "href": "slides/01-welcome.html#exercises",
    "title": "DAT255: Deep learning engineering",
    "section": "Exercises",
    "text": "Exercises\nExercises for the weekly lab are published as notebooks on GitHub (all links are also on Canvas)\nThey will be structured as\n\nTutorials showing how to solve a task\nExercises where you have to solve it yourself"
  },
  {
    "objectID": "slides/01-welcome.html#lectures-vs-exercises-vs-the-textbook",
    "href": "slides/01-welcome.html#lectures-vs-exercises-vs-the-textbook",
    "title": "DAT255: Deep learning engineering",
    "section": "Lectures vs exercises vs the textbook",
    "text": "Lectures vs exercises vs the textbook\n\n\n\nLectures are intended to introduce and explain topics, not to give the complete overview. (also we don’t have time to cover everything)\n\n\n\nThis means you have to read the book to follow the course.\n\n\n\nTutorial notebooks are also part of the curriculum,and this is where you learn how to really do deep learning."
  },
  {
    "objectID": "slides/01-welcome.html#assessment",
    "href": "slides/01-welcome.html#assessment",
    "title": "DAT255: Deep learning engineering",
    "section": "Assessment",
    "text": "Assessment\n\n\n\nThe exam has two parts:\n\nGroup project report, counts 50%\nShort written exam, counts 50%\n\nExam date will be posted on StudentWeb\nLast chance for registering for the exam is Feb. 2"
  },
  {
    "objectID": "slides/01-welcome.html#project-work",
    "href": "slides/01-welcome.html#project-work",
    "title": "DAT255: Deep learning engineering",
    "section": "Project work",
    "text": "Project work\nThe project work puts the engineering into Deep learning engineering\n\n\n\n\nPrimary goal:  Build and deploy a deep learning-based application, based on the theory, techniques and tools you learn in this course\n\n\n\n\n\n\nSecondary goals:   Make something that is\n\nUseful\nFun\nUseful and fun"
  },
  {
    "objectID": "slides/01-welcome.html#project-work-1",
    "href": "slides/01-welcome.html#project-work-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Project work",
    "text": "Project work\nThe project work puts the engineering into Deep learning engineering\n\n\n\n\nPrimary goal:  Build and deploy a deep learning-based application, based on the theory, techniques and tools you learn in this course\n\n\n\nTopic options:\n\nChoose yourself\nChoose from a project catalog (published on canvas)\n\nAll projects, along with a plan and description, must be approved beforehand\n📆 Submission deadline to be determined based on your input"
  },
  {
    "objectID": "slides/01-welcome.html#canvas",
    "href": "slides/01-welcome.html#canvas",
    "title": "DAT255: Deep learning engineering",
    "section": "Canvas",
    "text": "Canvas\nThe canonical source of information is Canvas"
  },
  {
    "objectID": "slides/01-welcome.html#calendar",
    "href": "slides/01-welcome.html#calendar",
    "title": "DAT255: Deep learning engineering",
    "section": "Calendar",
    "text": "Calendar\n\n\n(suggest we start at 9.15 most Mondays)"
  },
  {
    "objectID": "slides/01-welcome.html#deep-learning",
    "href": "slides/01-welcome.html#deep-learning",
    "title": "DAT255: Deep learning engineering",
    "section": "Deep learning",
    "text": "Deep learning"
  },
  {
    "objectID": "slides/01-welcome.html#ai-vs-ml-vs-dl",
    "href": "slides/01-welcome.html#ai-vs-ml-vs-dl",
    "title": "DAT255: Deep learning engineering",
    "section": "AI vs ML vs DL",
    "text": "AI vs ML vs DL\n\n\n\nWikiMedia Commons"
  },
  {
    "objectID": "slides/01-welcome.html#quick-history-of-deep-learning",
    "href": "slides/01-welcome.html#quick-history-of-deep-learning",
    "title": "DAT255: Deep learning engineering",
    "section": "Quick history of deep learning",
    "text": "Quick history of deep learning\n\n\n\n\nArtificial neural network McCulloch & Pitts, 1943\n\n\n\n\\[\n\\small\na = \\sum_{i=1}^{M} w_ix_i\n\\] \\[\n\\small\ny = f(a)\n\\]"
  },
  {
    "objectID": "slides/01-welcome.html#quick-history-of-deep-learning-1",
    "href": "slides/01-welcome.html#quick-history-of-deep-learning-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Quick history of deep learning",
    "text": "Quick history of deep learning\nThe perceptron Rosenblatt 1960\n\n\n\n\n\n\n\n\\[\n\\small\nf(a) =\n\\begin{cases}\n0, & a \\leq 0 \\\\\n1, & a &gt; 0\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/01-welcome.html#quick-history-of-deep-learning-2",
    "href": "slides/01-welcome.html#quick-history-of-deep-learning-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Quick history of deep learning",
    "text": "Quick history of deep learning\nBackpropagation Rumelhart, Hinton & Williams, 1986\n\n\nALVINN, 1989\n\n\n\n\n\n\n\nLeNet, 1989"
  },
  {
    "objectID": "slides/01-welcome.html#quick-history-of-deep-learning-3",
    "href": "slides/01-welcome.html#quick-history-of-deep-learning-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Quick history of deep learning",
    "text": "Quick history of deep learning\n\n\nAlexNet, 2012"
  },
  {
    "objectID": "slides/01-welcome.html#quick-history-of-deep-learning-4",
    "href": "slides/01-welcome.html#quick-history-of-deep-learning-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Quick history of deep learning",
    "text": "Quick history of deep learning\n\n\n\n\n\nLarge language models (LLMs), 2017\nGhatGPT 2022"
  },
  {
    "objectID": "slides/01-welcome.html#quick-history-of-deep-learning-5",
    "href": "slides/01-welcome.html#quick-history-of-deep-learning-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Quick history of deep learning",
    "text": "Quick history of deep learning\n\n\n\n\n\n\n\n\nDiffusion models, 2020"
  },
  {
    "objectID": "slides/01-welcome.html#section-12",
    "href": "slides/01-welcome.html#section-12",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Maslej et al. (2025) Artificial intelligence index report 2025. https://arxiv.org/abs/2504.07139"
  },
  {
    "objectID": "slides/01-welcome.html#section-13",
    "href": "slides/01-welcome.html#section-13",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "TensorFlowplayground"
  },
  {
    "objectID": "slides/03-convolutions.html#section-1",
    "href": "slides/03-convolutions.html#section-1",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "TensorFlowplayground"
  },
  {
    "objectID": "slides/03-convolutions.html#shallow-learning",
    "href": "slides/03-convolutions.html#shallow-learning",
    "title": "DAT255: Deep learning engineering",
    "section": "Shallow learning",
    "text": "Shallow learning\n\n\n\n\n\n\n%%{ init: {'flowchart': { 'curve': 'bumpX', 'nodeSpacing': 50, 'rankSpacing': 80 } } }%%\nflowchart LR\n    X1(\"x1\") --&gt; M{\"Model _f_\"}\n    X2(\"x2\") --&gt; M\n    X1 --&gt; X3(\"x3\")\n    X2 --&gt; X3\n    X1 --&gt; X4(\"x4\")\n    X2 --&gt; X4\n    X3 --&gt; M\n    X4 --&gt; M\n    subgraph F[Original features]\n    X1\n    X2\n    end\n    subgraph FE[Engineered features]\n    X3\n    X4\n    end \n    M --&gt; P(\"Prediction _y_\")\n\nstyle F fill:#ededed,stroke:#606060\nstyle FE fill:#FFF8E1,stroke:#606060\n\n\n\n\n\n\n\n\\[\ny = f(x_1, x_2, x_3, x_4, \\dots, x_n | \\theta)\n\\]"
  },
  {
    "objectID": "slides/03-convolutions.html#deep-learning",
    "href": "slides/03-convolutions.html#deep-learning",
    "title": "DAT255: Deep learning engineering",
    "section": "Deep learning",
    "text": "Deep learning\n\n\n\n\n\n\n%%{ init: {'flowchart': { 'curve': 'bumpX', 'nodeSpacing': 50, 'rankSpacing': 80 } } }%%\nflowchart LR\n    X1(\"x1\") --&gt; L1(\"Layer 1 (_f1_)\")\n    X2(\"x2\") --&gt; L1\n    subgraph ML[Neural network]\n    L1 --&gt; L2(\"Layer 2 (_f2_)\")\n    L2 --&gt; L3(\"Layer 3 (_f3_)\")\n    end\n    subgraph F[Original features]\n    X1\n    X2\n    end\n    L3 --&gt; P(\"Prediction _y_\")\n\nstyle ML fill:#ededed,stroke:#606060\nstyle F fill:#ededed,stroke:#606060\n\n\n\n\n\n\n\n\\[\ny = f_3(f_2(f_1(x_1, x_2 | \\theta_1) | \\theta_2) | \\theta_3)\n\\]"
  },
  {
    "objectID": "slides/03-convolutions.html#section-2",
    "href": "slides/03-convolutions.html#section-2",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "TensorFlowplayground"
  },
  {
    "objectID": "slides/03-convolutions.html#deep-learning-1",
    "href": "slides/03-convolutions.html#deep-learning-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Deep learning",
    "text": "Deep learning\n\n\n\nThe point of deep learning is to sequentially learn better feature representations, and use these to solve a task.\n\n\n\n\n\ninsufficient:    good:    better:\n\n\n\ndata    data    data\n\n\\(\\rightarrow\\)    \\(\\rightarrow\\)    \\(\\rightarrow\\)\n\nprediction    representation    representation\n\n   \\(\\rightarrow\\)    \\(\\rightarrow\\)\n\n   prediction    representation\n\n        \\(\\rightarrow\\)\n\n        prediction\n\n\n\n\nSince neural networks are universal function approximators, they can model arbitrarily complex relationships. The cost of doing so, is that we need a lot of data."
  },
  {
    "objectID": "slides/03-convolutions.html#the-feed-forward-neural-network",
    "href": "slides/03-convolutions.html#the-feed-forward-neural-network",
    "title": "DAT255: Deep learning engineering",
    "section": "The feed-forward neural network",
    "text": "The feed-forward neural network\nFor the demo we used the good ol’ fully-connected feed-forward network:\n\n\n\n\nEach node computes an output by\n\\[\n\\small\ny = f\\left(b + \\sum_{i=1}^{n} w_ix_i\\right)\n\\]\nwhere\n\n\\(w_i\\) is the weight of each incoming connection\n\\(b\\) is the bias term\n\\(f\\) is the activation function (more tomorrow)\n\n\n\nNow we want to motivate new types of layers."
  },
  {
    "objectID": "slides/03-convolutions.html#image-classification",
    "href": "slides/03-convolutions.html#image-classification",
    "title": "DAT255: Deep learning engineering",
    "section": "Image classification",
    "text": "Image classification\nLet’s classify this image: (see notebook 1)"
  },
  {
    "objectID": "slides/03-convolutions.html#image-classification-1",
    "href": "slides/03-convolutions.html#image-classification-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Image classification",
    "text": "Image classification\nLet’s classify this image: (see notebook 1)\nTry to treat every pixel as feature:\n\n\n2"
  },
  {
    "objectID": "slides/03-convolutions.html#image-classification-2",
    "href": "slides/03-convolutions.html#image-classification-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Image classification",
    "text": "Image classification\nLet’s classify this image: (see notebook 1)\nTry to treat every pixel as feature:\n\n\nnot 2\n\n\nTwo obvious problems:\n\nNot invariant under translation(move the image  different result)\nNot invariant under dilation(resize the image  different result)"
  },
  {
    "objectID": "slides/03-convolutions.html#enter-the-convolution-operation",
    "href": "slides/03-convolutions.html#enter-the-convolution-operation",
    "title": "DAT255: Deep learning engineering",
    "section": "Enter the convolution operation",
    "text": "Enter the convolution operation\nThe foundation for modern computer vision (plus lots of other things!) is convolution:\nan operation that takes in two functions and returns a new function\n\n\n\n\n\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\n\\]"
  },
  {
    "objectID": "slides/03-convolutions.html#enter-the-convolution-operation-1",
    "href": "slides/03-convolutions.html#enter-the-convolution-operation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Enter the convolution operation",
    "text": "Enter the convolution operation\nThe foundation for modern computer vision (plus lots of other things!) is convolution:\nan operation that takes in two functions and returns a new function\n\n\n\n\n\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\n\\]\n\n\n\nIn practice, convolution is a way to recognise and localise patterns in data\n\n\n\n\nTechnical note:\nIn signal processing we would call \\(\\small\\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\\) cross-correlation and the time-reversed version \\(\\small\\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d\\tau\\) would be convolution. In ML these got mixed up and we just talk about convolution."
  },
  {
    "objectID": "slides/03-convolutions.html#discrete-convolution",
    "href": "slides/03-convolutions.html#discrete-convolution",
    "title": "DAT255: Deep learning engineering",
    "section": "Discrete convolution",
    "text": "Discrete convolution\nConvolution is a lot easier with discrete data such as images, because:\n\nthe integral becomes a sum\nthe first function is our image\nthe second function is our kernel or filter, which tries to find patterns in the image."
  },
  {
    "objectID": "slides/03-convolutions.html#convolution-over-images",
    "href": "slides/03-convolutions.html#convolution-over-images",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolution over images",
    "text": "Convolution over images\nSimple case – one input channel"
  },
  {
    "objectID": "slides/03-convolutions.html#convolution-over-images-1",
    "href": "slides/03-convolutions.html#convolution-over-images-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolution over images",
    "text": "Convolution over images\nFor RGB color images: Process each color channel, then sum"
  },
  {
    "objectID": "slides/03-convolutions.html#the-convolution-kernel-or-filter",
    "href": "slides/03-convolutions.html#the-convolution-kernel-or-filter",
    "title": "DAT255: Deep learning engineering",
    "section": "The convolution kernel (or filter)",
    "text": "The convolution kernel (or filter)\nConvolution with predefined kernels is the core to digital image processing (but then we call it filters)\n\n\n\n\n\n\n\n\n\nIdentify vertical lines\n\n\n\n\n\n\n\nIdentify diagonal lines\n\n\n\n\n\n\n\nIdentifiy horizontal lines"
  },
  {
    "objectID": "slides/03-convolutions.html#more-filters",
    "href": "slides/03-convolutions.html#more-filters",
    "title": "DAT255: Deep learning engineering",
    "section": "More filters",
    "text": "More filters\n\n\nAverage: Blurring effect\n\n\n\n\n\n\n\n\nSobel filter: Edge detector"
  },
  {
    "objectID": "slides/03-convolutions.html#kernels-for-image-recognition",
    "href": "slides/03-convolutions.html#kernels-for-image-recognition",
    "title": "DAT255: Deep learning engineering",
    "section": "Kernels for image recognition",
    "text": "Kernels for image recognition\nLet’s try handcrafting some filters/kernels:\n\n\n\n\n\n\n\n\n\n\n\nNeed to refinethe approach :/"
  },
  {
    "objectID": "slides/03-convolutions.html#decomposition-into-simple-patters",
    "href": "slides/03-convolutions.html#decomposition-into-simple-patters",
    "title": "DAT255: Deep learning engineering",
    "section": "Decomposition into simple patters",
    "text": "Decomposition into simple patters\nA better approach: Multiple small filters\n\n\n\n\n\n\nOriginal image\n\n\nNew image, in lower resolution\n\n\nRepeat"
  },
  {
    "objectID": "slides/03-convolutions.html#decomposition-into-simple-patters-1",
    "href": "slides/03-convolutions.html#decomposition-into-simple-patters-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Decomposition into simple patters",
    "text": "Decomposition into simple patters"
  },
  {
    "objectID": "slides/03-convolutions.html#section-3",
    "href": "slides/03-convolutions.html#section-3",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Talk is cheap.Show me the code"
  },
  {
    "objectID": "slides/03-convolutions.html#keras-layers",
    "href": "slides/03-convolutions.html#keras-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Keras layers",
    "text": "Keras layers\nFor our proposed solution we need three layer types:\n\n\n\n\nConvolution layers to extract image features\nkeras.layers.Conv2D\n\n\n\n\n\nPooling layers to downsample and aggregate the features\nkeras.layers.MaxPooling2D\n\n\n\n\n\nFully-connected (dense) layers to compute the final prediction\nkeras.layers.Dense"
  },
  {
    "objectID": "slides/03-convolutions.html#the-conv2d-layer",
    "href": "slides/03-convolutions.html#the-conv2d-layer",
    "title": "DAT255: Deep learning engineering",
    "section": "The Conv2D layer",
    "text": "The Conv2D layer\n\n\nkeras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding=\"valid\",\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)"
  },
  {
    "objectID": "slides/03-convolutions.html#the-conv2d-layer-1",
    "href": "slides/03-convolutions.html#the-conv2d-layer-1",
    "title": "DAT255: Deep learning engineering",
    "section": "The Conv2D layer",
    "text": "The Conv2D layer\n\n\nkeras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding=\"valid\",\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n\nNumber of filters to use (typical: 32 to 512)"
  },
  {
    "objectID": "slides/03-convolutions.html#the-conv2d-layer-2",
    "href": "slides/03-convolutions.html#the-conv2d-layer-2",
    "title": "DAT255: Deep learning engineering",
    "section": "The Conv2D layer",
    "text": "The Conv2D layer\n\n\nkeras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding=\"valid\",\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n\nSize of the filter/kernel (typical: 3x3, sometimes bigger in the first layer)"
  },
  {
    "objectID": "slides/03-convolutions.html#the-conv2d-layer-3",
    "href": "slides/03-convolutions.html#the-conv2d-layer-3",
    "title": "DAT255: Deep learning engineering",
    "section": "The Conv2D layer",
    "text": "The Conv2D layer\n\n\nkeras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding=\"valid\",\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n\nStepsize for the convolution operation (typical: 1 or 2)\n\n\n\n\n\n\nstride=(1,1)\n\n\n\n\n\n\n\n\n\nstride=(2,2)"
  },
  {
    "objectID": "slides/03-convolutions.html#the-conv2d-layer-4",
    "href": "slides/03-convolutions.html#the-conv2d-layer-4",
    "title": "DAT255: Deep learning engineering",
    "section": "The Conv2D layer",
    "text": "The Conv2D layer\n\n\nkeras.layers.Conv2D(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding=\"valid\",\n    data_format=None,\n    dilation_rate=(1, 1),\n    groups=1,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n\nPadding around the image edges (typical: 1 or 2)\n\n\n\n\n\n\npadding=\"valid\"\n\n\n\n\n\n\n\n\n\npadding=\"same\""
  },
  {
    "objectID": "slides/03-convolutions.html#pooling-layers",
    "href": "slides/03-convolutions.html#pooling-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Pooling layers",
    "text": "Pooling layers\nTwo reasons for downsampling the image features:\n\nLearn a spatial hierarchy by widening the receptive field\nReduce the total parameter count\n\nMost common approach: Choose the maximum value from a window of pixels"
  },
  {
    "objectID": "slides/03-convolutions.html#the-maxpooling2d-layer",
    "href": "slides/03-convolutions.html#the-maxpooling2d-layer",
    "title": "DAT255: Deep learning engineering",
    "section": "The MaxPooling2D layer",
    "text": "The MaxPooling2D layer\n\n\nkeras.layers.MaxPooling2D(\n    pool_size=(2, 2),\n    strides=None,\n    padding=\"valid\",\n    data_format=None, \n    name=None, \n    **kwargs\n)\n\npool_size: Size of pooling window (typical: 2x2 or 3x3)\nstrides: Step size (typical: same as pool_size)\npadding: Padding around edges, valid or same"
  },
  {
    "objectID": "slides/03-convolutions.html#the-dense-layer",
    "href": "slides/03-convolutions.html#the-dense-layer",
    "title": "DAT255: Deep learning engineering",
    "section": "The Dense layer",
    "text": "The Dense layer\n\n\nkeras.layers.Dense(\n    units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    lora_rank=None,\n    **kwargs\n)\n\nunits: number of nodes in this layer\n\n\n\n\n\n\nIn the last dense layer, units must be equal to the number of classes (or otherwise number of desired outputs)."
  },
  {
    "objectID": "slides/03-convolutions.html#my-first-convolutional-network",
    "href": "slides/03-convolutions.html#my-first-convolutional-network",
    "title": "DAT255: Deep learning engineering",
    "section": "My first convolutional network ✨",
    "text": "My first convolutional network ✨\nLet’s piece together a convnet using Keras’ sequential model API:\n\n\n\nconvnet = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Flatten(),\n        keras.layers.Dense(10, activation=\"softmax\"),\n    ]\n)\n\n\n\n(More details about activations and training tomorrow)"
  },
  {
    "objectID": "slides/03-convolutions.html#my-first-convolutional-network-1",
    "href": "slides/03-convolutions.html#my-first-convolutional-network-1",
    "title": "DAT255: Deep learning engineering",
    "section": "My first convolutional network ✨",
    "text": "My first convolutional network ✨\nconvnet.summary()\nModel: \"sequential_1\"\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 26, 26, 32)          │             320 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 13, 13, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 11, 11, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 5, 5, 32)            │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (Flatten)                  │ (None, 800)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 800)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 10)                  │           8,010 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n Total params: 17,578 (68.66 KB)\n\n Trainable params: 17,578 (68.66 KB)\n\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "slides/03-convolutions.html#my-first-convolutional-network-2",
    "href": "slides/03-convolutions.html#my-first-convolutional-network-2",
    "title": "DAT255: Deep learning engineering",
    "section": "My first convolutional network ✨",
    "text": "My first convolutional network ✨\n\n\n\nConfigure the training objective and strategy:\nconvnet.compile(\n  loss=\"categorical_crossentropy\",\n  optimizer=\"adam\",\n  metrics=[\"accuracy\"]\n)\n(again, more details later + in the textbook)\n\n\n\nStart training!\nconvnet.fit(\n  X_train,\n  y_train,\n  batch_size=128,\n  epochs=15,\n  validation_split=0.1\n)"
  },
  {
    "objectID": "slides/03-convolutions.html#decomposition-into-simple-patters-theory-vs-practice",
    "href": "slides/03-convolutions.html#decomposition-into-simple-patters-theory-vs-practice",
    "title": "DAT255: Deep learning engineering",
    "section": "Decomposition into simple patters: Theory vs practice",
    "text": "Decomposition into simple patters: Theory vs practice\nRemember the cat:\n\n\n\n\n\n(We’ll try to classify pictures of cats in exercise 3, but let’s test out a cat detector convnet already now)"
  },
  {
    "objectID": "slides/03-convolutions.html#decomposition-into-simple-patters-theory-vs-practice-1",
    "href": "slides/03-convolutions.html#decomposition-into-simple-patters-theory-vs-practice-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Decomposition into simple patters: Theory vs practice",
    "text": "Decomposition into simple patters: Theory vs practice\n\n\n\nOur test image:\n\n\n\n\n\n\nFrom chapter 10, Interpreting what ConvNets learn"
  },
  {
    "objectID": "slides/03-convolutions.html#layer-activations",
    "href": "slides/03-convolutions.html#layer-activations",
    "title": "DAT255: Deep learning engineering",
    "section": "Layer activations",
    "text": "Layer activations\nWe can visualise what each filter does by looking at its activation on the test image: The output after the convolution and applying the activation function.\nExamples:\n\n\n\n\nLayer 1, filter 4"
  },
  {
    "objectID": "slides/03-convolutions.html#layer-activations-1",
    "href": "slides/03-convolutions.html#layer-activations-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 1 and 2:"
  },
  {
    "objectID": "slides/03-convolutions.html#layer-activations-2",
    "href": "slides/03-convolutions.html#layer-activations-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 3:"
  },
  {
    "objectID": "slides/03-convolutions.html#layer-activations-3",
    "href": "slides/03-convolutions.html#layer-activations-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 4:"
  },
  {
    "objectID": "slides/03-convolutions.html#layer-activations-4",
    "href": "slides/03-convolutions.html#layer-activations-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Layer activations",
    "text": "Layer activations\nRepeat for all filters in all layers:\nLayer 5 (last):"
  },
  {
    "objectID": "slides/05-optimisers.html#this-week",
    "href": "slides/05-optimisers.html#this-week",
    "title": "DAT255: Deep learning engineering",
    "section": "This week",
    "text": "This week\n\n\n\nToday:\n\nA little more on gradient descent\n\nImproved optimisation methods\n\nLoss curves and learning rates\n\nTomorrow:\n\nData augmentation\nNon-sequential layer connections\nTraining very deep convolutional networks\nModern ConvNet architectures"
  },
  {
    "objectID": "slides/05-optimisers.html#loss-functions",
    "href": "slides/05-optimisers.html#loss-functions",
    "title": "DAT255: Deep learning engineering",
    "section": "Loss functions",
    "text": "Loss functions\n\n\nReminder: For all statistical modelling we need a measure of prediction quality\nThe loss function \\(\\small{}L\\) must satisfy two conditions:\n\nDifferentiable*\nBounded below (\\(\\small L \\geq 0\\))\n\n\nIn this course we stick mostly to\n\nMean squared error (MSE) loss, for regression tasks\n\n\n\n\n\n\\[\n\\small\nL_{\\mathrm{MSE}}(\\boldsymbol{\\hat{y}, y}) = \\frac{1}{N} \\sum_{i}^{N} (\\hat{y}_i - y_i)^2\n\\] where \\(\\small{}N\\) is the number of data points\n\n\n\n\n\n\n\n\n\n*Again, at least piecewise differentiable"
  },
  {
    "objectID": "slides/05-optimisers.html#loss-functions-1",
    "href": "slides/05-optimisers.html#loss-functions-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Loss functions",
    "text": "Loss functions\n\n\nReminder: For all statistical modelling we need a measure of prediction quality\nThe loss function \\(\\small{}L\\) must satisfy two conditions:\n\nDifferentiable*\nBounded below (\\(\\small{}L \\geq 0\\))\n\nIn this course we stick mostly to\n\nMean squared error (MSE) loss, for regression tasks\nCross-entropy loss (or log loss), for classification tasks\n\n\n\n\\[\n\\scriptsize\nL_{\\mathrm{log}}(\\boldsymbol{\\hat{y}, y}) = -\\frac{1}{N} \\sum_i^N \\sum_k^K y_{i,k} \\log\\hat{y}_{i,k}\n\\] where \\(K\\) is the number of classes\n\n\n\n\n\n\n\n\n*Again, at least piecewise differentiable"
  },
  {
    "objectID": "slides/05-optimisers.html#gradient-descent",
    "href": "slides/05-optimisers.html#gradient-descent",
    "title": "DAT255: Deep learning engineering",
    "section": "Gradient descent",
    "text": "Gradient descent\n\n\n\n\n\n\n\n\n\n\nWith the gradient in place, we take steps downward (along the negative gradient), towards the optimal solution:\n\\[\n\\small\n\\boldsymbol{\\color{teal}{\\theta}}^{n+1} = \\boldsymbol{\\color{teal}{\\theta}}^n - \\eta \\nabla \\color{Purple}{L}\n\\]\nHere \\(\\eta\\) is the learning rate"
  },
  {
    "objectID": "slides/05-optimisers.html#local-minima",
    "href": "slides/05-optimisers.html#local-minima",
    "title": "DAT255: Deep learning engineering",
    "section": "Local minima",
    "text": "Local minima\n\n\nLocalminimum -&gt; bad predictions"
  },
  {
    "objectID": "slides/05-optimisers.html#comparing-gradient-descent-algorithms",
    "href": "slides/05-optimisers.html#comparing-gradient-descent-algorithms",
    "title": "DAT255: Deep learning engineering",
    "section": "Comparing gradient descent algorithms",
    "text": "Comparing gradient descent algorithms"
  },
  {
    "objectID": "slides/05-optimisers.html#momentum-optimisation",
    "href": "slides/05-optimisers.html#momentum-optimisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Momentum optimisation",
    "text": "Momentum optimisation\n\n\nImprove on regular gradient descent by keeping track of past gradients in a new term \\(\\small\\boldsymbol{m}\\):\n\\[\n\\small\n\\begin{align}\n    \\boldsymbol{m}_n &= \\beta \\boldsymbol{m}_{n-1} - \\eta \\nabla \\color{Purple}{L}(\\color{teal}{\\theta}_n) \\\\\n    \\boldsymbol{\\color{teal}{\\theta}}_{n+1} &= \\boldsymbol{\\color{teal}{\\theta}}_n + \\boldsymbol{m}_n\n\\end{align}\n\\]\n(behaves like a heavy ball rolling down a hill, where \\(\\small\\boldsymbol{m}\\) is the velocity)\n\nIntroduces a new hyperparameter \\(\\small\\beta \\in [0,1]\\), which is called the momentum\n\n\\(\\small\\beta = 0\\): “max friction”\n\\(\\small\\beta = 1\\): “no friction”"
  },
  {
    "objectID": "slides/05-optimisers.html#momentum-optimisation-1",
    "href": "slides/05-optimisers.html#momentum-optimisation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Momentum optimisation",
    "text": "Momentum optimisation\n\n\nImprove on regular gradient descent by keeping track of past gradients in a new term \\(\\small\\boldsymbol{m}\\):\n\\[\n\\small\n\\begin{align}\n    \\boldsymbol{m}_n &= \\beta \\boldsymbol{m}_{n-1} - \\eta \\nabla \\color{Purple}{L}(\\color{teal}{\\theta}_n) \\\\\n    \\boldsymbol{\\color{teal}{\\theta}}_{n+1} &= \\boldsymbol{\\color{teal}{\\theta}}_n + \\boldsymbol{m}_n\n\\end{align}\n\\]\n(behaves like a heavy ball rolling down a hill, where \\(\\small\\boldsymbol{m}\\) is the velocity)\nIntroduces a new hyperparameter \\(\\small\\beta \\in [0,1]\\), which is called the momentum\n\n\n\n\n\n\nkeras.optimizers.SGD(learning_rate=0.01, momentum=0.9)"
  },
  {
    "objectID": "slides/05-optimisers.html#nesterov-accelerated-gradient",
    "href": "slides/05-optimisers.html#nesterov-accelerated-gradient",
    "title": "DAT255: Deep learning engineering",
    "section": "Nesterov accelerated gradient",
    "text": "Nesterov accelerated gradient\nKeep the concept of momentum, but compute the gradient slightly ahead:\n\\[\n\\small\n\\begin{align}\n    \\boldsymbol{m}_n &=\n        \\beta \\boldsymbol{m}_{n-1}\n        - \\eta \\nabla \\color{Purple}{L}(\\color{teal}{\\theta}_n + \\beta \\boldsymbol{m}_{n-1}) \\\\\n    \\boldsymbol{\\color{teal}{\\theta}}_{n+1} &= \\boldsymbol{\\color{teal}{\\theta}}_n + \\boldsymbol{m}_n\n\\end{align}\n\\]\nkeras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)"
  },
  {
    "objectID": "slides/05-optimisers.html#adagrad-adaptive-gradient",
    "href": "slides/05-optimisers.html#adagrad-adaptive-gradient",
    "title": "DAT255: Deep learning engineering",
    "section": "AdaGrad (Adaptive Gradient)",
    "text": "AdaGrad (Adaptive Gradient)\nAdaGrad introduces an adaptive learning rate, which is adjusted independently for the different parameters\n\n\n\nFor parameters with steep gradient, learning rate is reduced quickly\nFor parameters with shallow gradient, learning rate is reduced slowly\n\n\n\n\n\n\n\n\n\n\n\nAdaGrad in practice:\n\nLess sensitive to choice of learning rate (since it’s adaptive) (good)\nFast convergence for simple problems (good)\nSlow (or no) convergence for difficult problems (bad)\n\n Add intelligent scaling to get RMSProp"
  },
  {
    "objectID": "slides/05-optimisers.html#adam-and-friends",
    "href": "slides/05-optimisers.html#adam-and-friends",
    "title": "DAT255: Deep learning engineering",
    "section": "Adam (and friends)",
    "text": "Adam (and friends)\n\n\n\nMomentum\n + RMSProp\n + some technical details\n = adaptive moment estimation (Adam)\nkeras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n\n\n\nMore tricks:\n\nAdaMax: Scale the past gradients differently\nNadam: Add Nesterov momentum\nAdamW Add L2 regularisation (aka weight decay)"
  },
  {
    "objectID": "slides/05-optimisers.html#comparison-and-guidelines",
    "href": "slides/05-optimisers.html#comparison-and-guidelines",
    "title": "DAT255: Deep learning engineering",
    "section": "Comparison and guidelines",
    "text": "Comparison and guidelines\n\n\n\n\n\n\n\n\n\n\nMethod\nConvergence\n\n\nSpeed\nQuality\n\n\n\n\nSGD\n⚡️\n⭐️⭐️⭐️\n\n\nSGD w/ momentum\n⚡️⚡️\n⭐️⭐️⭐️\n\n\nSGD w/ Nesterov\n⚡️⚡️\n⭐️⭐️⭐️\n\n\nAdaGrad\n⚡️⚡️⚡️\n⭐️\n\n\nRMSProp\n⚡️⚡️⚡️\n⭐️⭐️(⭐️)\n\n\nAdam\n⚡️⚡️⚡️\n⭐️⭐️(⭐️)\n\n\nAdaMax\n⚡️⚡️⚡️\n⭐️⭐️(⭐️)\n\n\nNadam\n⚡️⚡️⚡️\n⭐️⭐️(⭐️)\n\n\nAdamW\n⚡️⚡️⚡️\n⭐️⭐️(⭐️)\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nStart\n\nStart\n\n\n\nAdam\n\nAdam\n\n\n\nStart-&gt;Adam\n\n\n\n\n\nwork\n\nDid it\nwork?\n\n\n\nwhy\n\nWhy\nnot?\n\n\n\nwork-&gt;why\n\n\nNo\n\n\n\nDone\n\nDone!\n\n\n\nwork-&gt;Done\n\n\nYes\n\n\n\nAdaMax\n\nAdaMax\n\n\n\nwhy-&gt;AdaMax\n\n\nNumerically\nunstable\n\n\n\nAdamW\n\nAdamW\n\n\n\nwhy-&gt;AdamW\n\n\nNeeds\nregularisation\n\n\n\nNadam\n\nNadam\n\n\n\nwhy-&gt;Nadam\n\n\nSlow\n\n\n\nsgdn\n\nSGD w/\nNesterov\n\n\n\nwhy-&gt;sgdn\n\n\nUnderfitting\n\n\n\nAdam-&gt;work\n\n\n\n\n\nAdaMax-&gt;work\n\n\n\n\n\nAdamW-&gt;work\n\n\n\n\n\nNadam-&gt;work\n\n\n\n\n\nsgdn-&gt;work\n\n\n\n\n\n\n\n\n\n\n\n\n\nA. Geron: Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow"
  },
  {
    "objectID": "slides/05-optimisers.html#learning-rate-and-loss-curves",
    "href": "slides/05-optimisers.html#learning-rate-and-loss-curves",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning rate and loss curves",
    "text": "Learning rate and loss curves\nCommon to all optimisation methods is that we must choose a learning rate \\(\\eta\\).\nThis affects the training progress:"
  },
  {
    "objectID": "slides/05-optimisers.html#learning-rate-scheduling",
    "href": "slides/05-optimisers.html#learning-rate-scheduling",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning rate scheduling",
    "text": "Learning rate scheduling\nSome options for the most efficient learning: (see notebooks)\n\nReduce \\(\\eta\\) when learning stops:\nkeras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\nGradually reduce \\(\\eta\\) for each step:\nkeras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\nkeras.optimizers.schedules.PolynomialDecay(initial_learning_rate, ...)\nChange \\(\\eta\\) by some other rule:\nclass MyLRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n\n    def __init__(self, initial_learning_rate):\n        self.initial_learning_rate = initial_learning_rate\n\n    def __call__(self, step):\n        return self.initial_learning_rate / (step + 1)\n\noptimizer = keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))"
  },
  {
    "objectID": "slides/05-optimisers.html#learning-rate-schedulers",
    "href": "slides/05-optimisers.html#learning-rate-schedulers",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning rate schedulers",
    "text": "Learning rate schedulers"
  },
  {
    "objectID": "slides/07-image-segmentation.html#computer-vision-tasks",
    "href": "slides/07-image-segmentation.html#computer-vision-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "Computer vision tasks",
    "text": "Computer vision tasks"
  },
  {
    "objectID": "slides/07-image-segmentation.html#computer-vision-tasks-1",
    "href": "slides/07-image-segmentation.html#computer-vision-tasks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Computer vision tasks",
    "text": "Computer vision tasks"
  },
  {
    "objectID": "slides/07-image-segmentation.html#image-segmentation",
    "href": "slides/07-image-segmentation.html#image-segmentation",
    "title": "DAT255: Deep learning engineering",
    "section": "Image segmentation",
    "text": "Image segmentation\n\nSemantic segmentation: Classify each pixel onto a category\nInstance segmentation: Classify onto individial instances (possibly of same category)\nPanoptic segmentation: Classify both category and instance"
  },
  {
    "objectID": "slides/07-image-segmentation.html#building-a-semantic-segmentation-model",
    "href": "slides/07-image-segmentation.html#building-a-semantic-segmentation-model",
    "title": "DAT255: Deep learning engineering",
    "section": "Building a semantic segmentation model",
    "text": "Building a semantic segmentation model\nStep 1: Training data\nNeed per-pixel annotations of each category (“masks”), such as\n\n1: foreground, 2: background, 3: contour"
  },
  {
    "objectID": "slides/07-image-segmentation.html#building-a-semantic-segmentation-model-1",
    "href": "slides/07-image-segmentation.html#building-a-semantic-segmentation-model-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Building a semantic segmentation model",
    "text": "Building a semantic segmentation model\nStep 2: Constructing the model\nOur targets have same width and height dimensions as the input image, so the model output should have that too.\n\nNeed an architecture that can do\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(H, W, C)\nImage\n\n\n\n\n\n\n\n(?, ?, ?)\nInternal representation\n\n\n\n\n\n\n\n(H, W, 1)\nTargets\n\n\n\n\n\n\n\nEncoding\n\n\nDecoding"
  },
  {
    "objectID": "slides/07-image-segmentation.html#defining-the-model",
    "href": "slides/07-image-segmentation.html#defining-the-model",
    "title": "DAT255: Deep learning engineering",
    "section": "Defining the model",
    "text": "Defining the model\ninputs = keras.Input(shape=img_size + (3,))\nx = Rescaling(1.0 / 255)(inputs)\n\nx = Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\nx = Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n\noutputs = ...\nmodel = keras.Model(inputs, outputs)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 200, 200, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ rescaling (Rescaling)           │ (None, 200, 200, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (Conv2D)                 │ (None, 100, 100, 64)   │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)               │ (None, 100, 100, 64)   │        36,928 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)               │ (None, 50, 50, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (Conv2D)               │ (None, 50, 50, 128)    │       147,584 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (Conv2D)               │ (None, 25, 25, 256)    │       295,168 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (Conv2D)               │ (None, 25, 25, 256)    │       590,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤"
  },
  {
    "objectID": "slides/07-image-segmentation.html#transposed-convolution",
    "href": "slides/07-image-segmentation.html#transposed-convolution",
    "title": "DAT255: Deep learning engineering",
    "section": "Transposed convolution",
    "text": "Transposed convolution\nUse transposed convolution layers to upsample dimensions:"
  },
  {
    "objectID": "slides/07-image-segmentation.html#defining-the-model-1",
    "href": "slides/07-image-segmentation.html#defining-the-model-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Defining the model",
    "text": "Defining the model\ninputs = keras.Input(shape=img_size + (3,))\nx = Rescaling(1.0 / 255)(inputs)\n\nx = Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\nx = Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n\nx = Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2DTranspose(256, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\nx = Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2DTranspose(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\nx = Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\nx = Conv2DTranspose(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n\noutputs = Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/07-image-segmentation.html#defining-the-model-2",
    "href": "slides/07-image-segmentation.html#defining-the-model-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Defining the model",
    "text": "Defining the model\nmodel.summary()\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 200, 200, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ rescaling (Rescaling)           │ (None, 200, 200, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (Conv2D)                 │ (None, 100, 100, 64)   │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n ...\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (Conv2D)               │ (None, 25, 25, 256)    │       590,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_transpose                │ (None, 25, 25, 256)    │       590,080 │\n│ (Conv2DTranspose)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_transpose_1              │ (None, 50, 50, 256)    │       590,080 │\n│ (Conv2DTranspose)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_transpose_2              │ (None, 50, 50, 128)    │       295,040 │\n│ (Conv2DTranspose)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_transpose_3              │ (None, 100, 100, 128)  │       147,584 │\n│ (Conv2DTranspose)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_transpose_4              │ (None, 100, 100, 64)   │        73,792 │\n│ (Conv2DTranspose)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_transpose_5              │ (None, 200, 200, 64)   │        36,928 │\n│ (Conv2DTranspose)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_6 (Conv2D)               │ (None, 200, 200, 3)    │         1,731 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘"
  },
  {
    "objectID": "slides/07-image-segmentation.html#improved-segmentation-models-the-u-net",
    "href": "slides/07-image-segmentation.html#improved-segmentation-models-the-u-net",
    "title": "DAT255: Deep learning engineering",
    "section": "Improved segmentation models: the U-Net",
    "text": "Improved segmentation models: the U-Net\nWe learned last time that deep networks can be improved by adding residual connections.\n\nHow about we add them to the encoder-decoder segmentation model?Enter the U-Net:"
  },
  {
    "objectID": "slides/07-image-segmentation.html#u-net-use-cases",
    "href": "slides/07-image-segmentation.html#u-net-use-cases",
    "title": "DAT255: Deep learning engineering",
    "section": "U-Net use cases",
    "text": "U-Net use cases\n\n\n\n\n\nCell tracking\n\n\n\n\n\n\n\nCT scans\n\n\n\n\n\n\n\nHematoma detection\n\n\n\n\n\n\n\nCrop segmentation\n\n\n\n\n\n\n\nAutonomous driving"
  },
  {
    "objectID": "slides/07-image-segmentation.html#state-of-the-art-image-segmentation",
    "href": "slides/07-image-segmentation.html#state-of-the-art-image-segmentation",
    "title": "DAT255: Deep learning engineering",
    "section": "State-of-the-art image segmentation",
    "text": "State-of-the-art image segmentation\nSegment Anything*: Using images with up to 500 masks\n\n\n\n\n\n\n* https://github.com/facebookresearch/segment-anything\n\nAdds user-selected objects (by prompts), consistent tracking in videos, ++\n\nDemo"
  },
  {
    "objectID": "slides/10-timeseries.html#sequences",
    "href": "slides/10-timeseries.html#sequences",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequences",
    "text": "Sequences\nSequences are ordered series. For instance\n\n\n\nNatural language\nI only drink coffee (\\(\\neq\\) only I drink coffee)\n\n\n\nTime series\n\n\n\nTime\n11:00\n12:00\n13:00\n14:00\n15:00\n\n\nTemp\n7°C\n8°C\n10°C\n12 °C\n12 °C\n\n\n\n\n\n\n\nAudio\n\n\n\n\n\n\nVideo\nVideo\n\n\n\n\nDNA"
  },
  {
    "objectID": "slides/10-timeseries.html#sequence-prediction-tasks",
    "href": "slides/10-timeseries.html#sequence-prediction-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence prediction tasks",
    "text": "Sequence prediction tasks\n\n\n\n\nClassification:\n\nSpeech recognition\nFraud detection, network intrusion detection\nFault detection and predictive maintenance\nMedical diagnostics\nSentiment analysis\nTopic classification\n\n\n We already know (most of) the tools needed\n\n\n\n\nForecasting (regression of future values)\n\nPredicting weather, energy prices, stock prices\nText generation\n\n\n Need a model that can remember the past\n\n\n\nSequence-to-sequence learning\n\nLanguage translation\nImage captioning\nText summarisation\n\n\n Need a model that can remember the context"
  },
  {
    "objectID": "slides/10-timeseries.html#sequence-classification",
    "href": "slides/10-timeseries.html#sequence-classification",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence classification",
    "text": "Sequence classification\n\nFor images we looked for patterns between neighbouring pixels, in 2D\nFor sequences we equivalently look for patterns between neighbouring elements (e.g. points in time), in 1D\n\nWhile 1D, we can still have multiple channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2D (images)\n\n1D (sequences)\n\n\n\n\nkeras.layers.Conv2D\n\nkeras.layers.Conv1D\n\n\nkeras.layers.Conv2DTranspose\n\nkeras.layers.Conv1DTranspose\n\n\nkeras.layers.MaxPooling2D\n\nkeras.layers.MaxPooling1D"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-1",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-2",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-3",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-4",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-5",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-6",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-6",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolutions-recap-but-1d-7",
    "href": "slides/10-timeseries.html#convolutions-recap-but-1d-7",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolutions recap (but 1D)",
    "text": "Convolutions recap (but 1D)"
  },
  {
    "objectID": "slides/10-timeseries.html#convolution-appreciation-slide",
    "href": "slides/10-timeseries.html#convolution-appreciation-slide",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolution appreciation slide",
    "text": "Convolution appreciation slide\nRecall we use the convolution* operation as a translationally invariant pattern detector\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\n\\]\n\n\n\n\n* Again, in signal processing we would call it cross-correlation"
  },
  {
    "objectID": "slides/10-timeseries.html#convolution-appreciation-slide-1",
    "href": "slides/10-timeseries.html#convolution-appreciation-slide-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Convolution appreciation slide",
    "text": "Convolution appreciation slide\nRecall we use the convolution* operation as a translationally invariant pattern detector\n\\[\n\\small\nf \\ast g \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d\\tau\n\\]\n\n\n\n\n* Again, in signal processing we would call it cross-correlation"
  },
  {
    "objectID": "slides/10-timeseries.html#sequence-forecasting-predicting-the-future",
    "href": "slides/10-timeseries.html#sequence-forecasting-predicting-the-future",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence forecasting: Predicting the future",
    "text": "Sequence forecasting: Predicting the future\nCNNs are great for classification because of translation invariance\nFor forecasting, we often don’t want this.\n\nNew assumption:\n\nRecent data is more informative than old data"
  },
  {
    "objectID": "slides/10-timeseries.html#seasonality",
    "href": "slides/10-timeseries.html#seasonality",
    "title": "DAT255: Deep learning engineering",
    "section": "Seasonality",
    "text": "Seasonality\nMany timeseries have recurring patters, caused by some physical phenomenon  Collectively talk about this as seasonality\nExample from the textbook: Temperature\n\n\n\n\n\n\nEight years (Fig 13.1)\n\n\n\n\n\n\n\nTen days (Fig 13.2)"
  },
  {
    "objectID": "slides/10-timeseries.html#recurrent-neural-networks-rnns",
    "href": "slides/10-timeseries.html#recurrent-neural-networks-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way: Let each node store its previous output"
  },
  {
    "objectID": "slides/10-timeseries.html#recurrent-neural-networks-rnns-1",
    "href": "slides/10-timeseries.html#recurrent-neural-networks-rnns-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way: Let each node store its previous output"
  },
  {
    "objectID": "slides/10-timeseries.html#recurrent-neural-networks-rnns-2",
    "href": "slides/10-timeseries.html#recurrent-neural-networks-rnns-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nRecall that a regular Dense layer computes its output by\noutputs = activation(tf.dot(W, inputs) + b)\n(where W is the weight matrix, inputs is the vector of features and b is the bias vector)\n\nThe recurrent node has two sets of weights:\n\nThe usual ones, call them W\nThose to be applied to the previous output, call them U\n\n\n\nThe outputs then become\nstate_t = tf.zeros(shape=(num_output_features))\noutputs = []\nfor input_t in input_sequence:  # loop over inputs at time t\n  output_t = activation(tf.dot(W, inputs) + tf.dot(U, state_t) + b)\n  outputs.append(output_t)\n  state_t = output_t\noutput_sequence = tf.stack(outputs, axis=0)\nImplemented in Keras as keras.layers.SimpleRNN"
  },
  {
    "objectID": "slides/10-timeseries.html#input-and-output-sequences",
    "href": "slides/10-timeseries.html#input-and-output-sequences",
    "title": "DAT255: Deep learning engineering",
    "section": "Input and output sequences",
    "text": "Input and output sequences\n\n\n\n\n\n\nA. Geron: Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow"
  },
  {
    "objectID": "slides/10-timeseries.html#intermezzo-autoregressive-models",
    "href": "slides/10-timeseries.html#intermezzo-autoregressive-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Intermezzo: Autoregressive models",
    "text": "Intermezzo: Autoregressive models\nSimplest possible forecast:\nThe value tomorrow is the same as the value today. \\[\n\\small\ny_i = y_{t-1}\n\\]\n\n\n\n\n\nNumber of rail and bus passengers in Chicago 2019\n\n\n\n\n\n\n\nPartial autocorrelation"
  },
  {
    "objectID": "slides/10-timeseries.html#intermezzo-autoregressive-models-1",
    "href": "slides/10-timeseries.html#intermezzo-autoregressive-models-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Intermezzo: Autoregressive models",
    "text": "Intermezzo: Autoregressive models\n\n\nMore advanced forecast:\nThe value tomorrow is given by a weighted sum of the \\(p\\) previous time steps, plus a noise term\n\\[\n\\small\ny_i = \\sum^p \\varphi_i y_{t-i} + \\epsilon_t\n\\]\n(\\(\\varphi_i\\) are the parameters of the model)\n\n\n\n\nCan add moving average to get an ARMA model, look at differences to get ARIMA, add seasonality to get SARIMA, …\nLots of work on (traditional) statistical time series modelling - usually worth trying out before going to deep learning."
  },
  {
    "objectID": "slides/10-timeseries.html#improved-memory-cells",
    "href": "slides/10-timeseries.html#improved-memory-cells",
    "title": "DAT255: Deep learning engineering",
    "section": "Improved memory cells",
    "text": "Improved memory cells\nIn practice, RNNs suffer from vanishing/exploding gradients during training\n Difficult to make them learn long-term dependencies\n\nCan introduce hidden states which are not the same as the output.\n\n\n\n\n\nTwo most used approaces: LSTMs and GRUs."
  },
  {
    "objectID": "slides/12-tools.html#this-week",
    "href": "slides/12-tools.html#this-week",
    "title": "DAT255: Deep learning engineering",
    "section": "This week",
    "text": "This week\n\n\n\nToday:\n\nDeep learning on tabular data\nData preprocessing with Keras\n\nFeature scaling\nConverting text and categorical values\nQuick look at embeddings\n\n\nThursday:\n\nCustom Keras objects\nSome ML experiment monitoring tools"
  },
  {
    "objectID": "slides/12-tools.html#customising-keras",
    "href": "slides/12-tools.html#customising-keras",
    "title": "DAT255: Deep learning engineering",
    "section": "Customising Keras",
    "text": "Customising Keras\nKeras typically have most of what we need, but occasionally we have to add functionality ourselves\nThings we might need to customise:\n\nCallbacks\nMetrics\nLoss functions\nLayers\nModel behaviour undertraining vs inference"
  },
  {
    "objectID": "slides/12-tools.html#custom-callbacks",
    "href": "slides/12-tools.html#custom-callbacks",
    "title": "DAT255: Deep learning engineering",
    "section": "Custom callbacks",
    "text": "Custom callbacks\nA callback is run before/after certain events during training\nUse cases:\n\nSave model checkpoint efter each epoch\nChange learning rate\nWrite results to log file or monitoring tool\n\n\n\n\nclass CustomCallback(keras.callbacks.Callback):\n\n    def on_train_begin(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Starting training; got log keys: {}\".format(keys))\n\n    def on_train_end(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Stop training; got log keys: {}\".format(keys))\n\n    def on_epoch_begin(self, epoch, logs=None):\n      ...\n\n    # and more!"
  },
  {
    "objectID": "slides/12-tools.html#custom-metrics",
    "href": "slides/12-tools.html#custom-metrics",
    "title": "DAT255: Deep learning engineering",
    "section": "Custom metrics",
    "text": "Custom metrics\nNeed some exotic metric you invented yourself, or that is only available in scikit-learn?\nSubclass the abstract keras.metrics.Metrics class and implement the following methods:\nclass MyMetric(keras.metrics.Metric):\n\n  def __init__(self, ..., **kwargs):\n    super().__init__(**kwargs)\n\n    # class variables\n    self.result = self.add_variable(shape=(), initializer='zeros')\n\n  def update_state(self, y_true, y_pred, sample_weight=None):\n\n    # this is where the result is calculated\n    # doesn't return anything\n    result = ...\n    self.result.add_assign(result)\n\n  def result(self):\n    return self.result"
  },
  {
    "objectID": "slides/12-tools.html#custom-loss-functions",
    "href": "slides/12-tools.html#custom-loss-functions",
    "title": "DAT255: Deep learning engineering",
    "section": "Custom loss functions",
    "text": "Custom loss functions\nCustom loss functions can either subclassing keras.losses.Loss, or they can simply be regular functions, with two inputs (y_true, y_pred) and one output.\n\nImportant:\nThe loss function must be differentiable.\n\n\n\ndef mean_squared_error(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n    \nmodel.compile(loss=mean_squared_error)\n\n\n\n\n\nAlso important:\nFor performance reasons, avoid loops, conditionals, and most python built-ins.\nWrite vectorisable code, ideally in the backend framework (i.e. TensorFlow) or using keras.ops."
  },
  {
    "objectID": "slides/12-tools.html#custom-layers",
    "href": "slides/12-tools.html#custom-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Custom layers",
    "text": "Custom layers\nThe simplest custom layer: keras.layers.Lambda\n(has no weights)\n\n\n\nexponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n\nmodel = keras.Sequential([\n  keras.layers.Input(shape=input_shape),\n  exponential_layer(),\n  keras.layers.Dense(10)\n])"
  },
  {
    "objectID": "slides/12-tools.html#custom-layers-1",
    "href": "slides/12-tools.html#custom-layers-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Custom layers",
    "text": "Custom layers\nFor stateful layers, subclass the abstract keras.layers.Layer class.\nNeed three methods: __init__, call, and build.\n\n\n\nclass Linear(keras.layers.Layer):\n\n    def __init__(self, units=32):\n        super().__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(\n            shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True,\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"random_normal\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.linalg.matmul(inputs, self.w) + self.b"
  },
  {
    "objectID": "slides/12-tools.html#custom-models",
    "href": "slides/12-tools.html#custom-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Custom models",
    "text": "Custom models\nTypically we compose complicated models by writing reuseable blocks of layers, as a function\nOccasionally, however, we might want to rewrite it as a class.\n\n\n\n\n\ndef conv_block(inputs):\n  x = layers.Rescaling(1.0 / 255)(inputs)\n  x = layers.Conv2D(128, 3, strides=2)(x)\n  x = layers.BatchNormalization()(x)\n  x = layers.Activation(\"relu\")(x)\n\n  return x\n\n\nclass ConvBlock(keras.layers.Layer):\n\n  def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n\n    self.conv2d = layers.Conv2D(128, 3, strides=2)\n    self.batchnorm = layers.BatchNormalization()\n\n  def call(self, inputs):\n    x = layers.Rescaling(1.0 / 255)(inputs)\n    x = self.conv2d(x)\n    x = self.bathnorm(x) \n    out = layers.Activation(\"relu\")(x)\n\n    return out"
  },
  {
    "objectID": "slides/12-tools.html#the-ml-project-lifecycle",
    "href": "slides/12-tools.html#the-ml-project-lifecycle",
    "title": "DAT255: Deep learning engineering",
    "section": "The ML project lifecycle",
    "text": "The ML project lifecycle"
  },
  {
    "objectID": "slides/12-tools.html#experiment-tracking",
    "href": "slides/12-tools.html#experiment-tracking",
    "title": "DAT255: Deep learning engineering",
    "section": "Experiment tracking 🧑‍🔬",
    "text": "Experiment tracking 🧑‍🔬\nDuring training: Monitor and diagnose\n\nIs my model still improving?\nAre gradients stable?\nIs the model overfitting?\n\nAfter training: Compare and select\n\nWhich of my models performed best?\n…and most importantly: What was the configuration of the best model?\n\nBefore next training: Hyperparameter selection\n\nWhich new models and hyperparameters should I try?"
  },
  {
    "objectID": "slides/12-tools.html#tools-for-experiment-tracking",
    "href": "slides/12-tools.html#tools-for-experiment-tracking",
    "title": "DAT255: Deep learning engineering",
    "section": "Tools for experiment tracking",
    "text": "Tools for experiment tracking\nSome options: TensorBoard\n\n\n\n\n\n+ Integrates well with TensorFlow/Keras - Integrates poorly with other frameworks - Less functionality than other tools"
  },
  {
    "objectID": "slides/12-tools.html#tools-for-experiment-tracking-1",
    "href": "slides/12-tools.html#tools-for-experiment-tracking-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Tools for experiment tracking",
    "text": "Tools for experiment tracking\nSome options: Weights & Biases\n\n\n\n\n\n+ Loads of functionality, including hyperparameter optimisation + Integrates with practically all frameworks - Commercial product (although free for academic use)"
  },
  {
    "objectID": "slides/12-tools.html#tools-for-experiment-tracking-2",
    "href": "slides/12-tools.html#tools-for-experiment-tracking-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Tools for experiment tracking",
    "text": "Tools for experiment tracking\nSome options: AIM\n\n\n\n\n\n+ Loads of functionality + Integrates with practically all frameworks + Open-source, self-hosted (although paid plans exist)\n\nPlus others too: MLflow, Sacred, Comet, Neptune"
  },
  {
    "objectID": "slides/12-tools.html#hyperparameter-optimisation",
    "href": "slides/12-tools.html#hyperparameter-optimisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Hyperparameter optimisation",
    "text": "Hyperparameter optimisation\n\nHow many settings can you tweak in your neural network?\n\nHyperparameters cannot be optimised via gradient descent\nNeed to select them through experimentation.\nSince one experiment (one training run) can be very expensive, new settings should be chosen carefully."
  },
  {
    "objectID": "slides/12-tools.html#hyperparameter-optimisation-grid-scan",
    "href": "slides/12-tools.html#hyperparameter-optimisation-grid-scan",
    "title": "DAT255: Deep learning engineering",
    "section": "Hyperparameter optimisation: Grid scan",
    "text": "Hyperparameter optimisation: Grid scan\nOption 1: Grid scan\n\n\nDivide the range of each hyperparameter into fixed steps, and test all combinations.\n+ Simple\n+ Parallelisable\n– The higher the dimensionality, the bigger the distance between grid points. Need a big number of experiments to have good coverage"
  },
  {
    "objectID": "slides/12-tools.html#hyperparameter-optimisation-random-search",
    "href": "slides/12-tools.html#hyperparameter-optimisation-random-search",
    "title": "DAT255: Deep learning engineering",
    "section": "Hyperparameter optimisation: Random search",
    "text": "Hyperparameter optimisation: Random search\nOption 2: Random search\n\n\nProbabilistic approach: Pick hyperparameter values at random\n+ Simple\n+ Parallelisable\n+ Likely to find a better optimum than grid scan in fewer number of tries\n– Information about one experiment doesn’t help selecting the next\n\n\n\nShould always prefer this over grid search."
  },
  {
    "objectID": "slides/12-tools.html#hyperparameter-optimisation-intelligent-choices",
    "href": "slides/12-tools.html#hyperparameter-optimisation-intelligent-choices",
    "title": "DAT255: Deep learning engineering",
    "section": "Hyperparameter optimisation: Intelligent choices",
    "text": "Hyperparameter optimisation: Intelligent choices\nOption 3: Bayesian optimisation, evolutionary algorithms, and other derivative-free optimisation algorithms\n\n\nLet the result of one experiment influence the choice of the next one\n+ Efficient\n+ Several good frameworks exist\n– Not parallelisable\n\n\n\n\n\n\n\n\nAlways prefer this, except for the most simple cases"
  },
  {
    "objectID": "slides/12-tools.html#tools-for-hyperparameter-optimisation",
    "href": "slides/12-tools.html#tools-for-hyperparameter-optimisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Tools for hyperparameter optimisation",
    "text": "Tools for hyperparameter optimisation\nSome options: KerasTuner, for simple integration with Keras:\n\n\n\ndef model_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n\n  # Tune the number of units in the first Dense layer\n  units = hp.Int('units', min_value=32, max_value=512, step=32)\n  model.add(keras.layers.Dense(units=units, activation='relu'))\n  model.add(keras.layers.Dense(10), activation=\"softmax\")\n\n  # Tune the learning rate for the optimizer -- choose between 0.01, 0.001, or 0.0001\n  learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n  model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n    loss=\"categorical_crossentropy\",\n    metrics=['accuracy']\n  )\n\n  return model\n\n\n\n\ntuner = kt.BayesianOptimization(model_builder, objective='val_accuracy')"
  },
  {
    "objectID": "slides/12-tools.html#tools-for-hyperparameter-optimisation-1",
    "href": "slides/12-tools.html#tools-for-hyperparameter-optimisation-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Tools for hyperparameter optimisation",
    "text": "Tools for hyperparameter optimisation\nSome options:\nOptuna, for integration with any framework\nSeveral optimisations algorithms available.\n\n\n\n  units = trial.suggest_int('units', 32, 512)\n  model.add(keras.layers.Dense(units=units, activation='relu'))\n  ...\nComes with visualisation dashboard, VSCode extension"
  },
  {
    "objectID": "slides/12-tools.html#tools-for-hyperparameter-optimisation-2",
    "href": "slides/12-tools.html#tools-for-hyperparameter-optimisation-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Tools for hyperparameter optimisation",
    "text": "Tools for hyperparameter optimisation\nSome options:\nRay Tune, which interfaces with KerasTuner, Optuna and other optimisation libraries\n\n\n\nsearch_space = {\n  'units': tune.randint(32, 512)\n}\n\ndef model_builder(config):\n  ...\n  units = config['units']\n  model.add(keras.layers.Dense(units=units, activation='relu'))\n  ..."
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#getting-text-into-our-model",
    "href": "slides/14-nlp-embeddings.html#getting-text-into-our-model",
    "title": "DAT255: Deep learning engineering",
    "section": "Getting text into our model",
    "text": "Getting text into our model\n\n\nText vectorisation: Converting text to numeric data.\nTypical approach consists of several steps\n\nStandardisation:Remove diacritics, punctuation, convert to lowercase\nTokenisation:Split text into tokens which can be words, subwords, or groups of words\nIndexing:Convert tokens to integer values\nEncoding:Convert indices into embeddings or one-hot encoding\n\n\n\n\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#tokenisation",
    "href": "slides/14-nlp-embeddings.html#tokenisation",
    "title": "DAT255: Deep learning engineering",
    "section": "Tokenisation",
    "text": "Tokenisation\n\n\n\n\n\nThere are several different tokenisation algorithms\nDifferent language models often use different schemes.\n\n\n\n\nTokeniser playground"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#embeddings",
    "href": "slides/14-nlp-embeddings.html#embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Embeddings",
    "text": "Embeddings\nEmbedding layers are practically the same as one-hot encoding, followed by a linear Dense layer (without the bias term):\n\n\n\n\n\n\nthe  [ 0     1     0     0 ]\n\n\n[ -1.46 -0.86  0.09 ]\n\n\n\n\n\nembedding = keras.Sequential([\n  keras.layers.StringLookup(output_mode=\"one-hot\"),\n  keras.layers.Dense(\n    units=embedding_dim,\n    use_bias=False,\n    activation=None\n  )\n])\n\n\nDifference is that Embedding layers are implemented in a much more efficient way."
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#computing-similarity-in-embedding-space",
    "href": "slides/14-nlp-embeddings.html#computing-similarity-in-embedding-space",
    "title": "DAT255: Deep learning engineering",
    "section": "Computing similarity in embedding space",
    "text": "Computing similarity in embedding space\nWe’ve seen there is a relation between semantics and position in embedding space.\nHow can we measure token similarity?\n\n\nOption 1: Euclidian (L2) norm\n-&gt; distance between positions \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\)\n\n\\[\n\\small\n|| (\\mathbf{a} - \\mathbf{b}) ||_2 = \\sqrt{(a_1 - b_1)^2 + ... \\ (a_n - b_n)^2}\n\\]\n\n\n\nOption 2: Cosine similarity\n-&gt; angular difference between positions \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) (ignores vector lengths)\n\n\\[\n\\small\n\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||}\n\\]"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#other-uses-of-embeddings",
    "href": "slides/14-nlp-embeddings.html#other-uses-of-embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Other uses of embeddings",
    "text": "Other uses of embeddings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncloud.google.com/blog"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#embed-everything",
    "href": "slides/14-nlp-embeddings.html#embed-everything",
    "title": "DAT255: Deep learning engineering",
    "section": "Embed everything",
    "text": "Embed everything\nWe have covered word embeddings, but we can create embeddings for other objects too\n\n\n\nSentences\nImages\nAudio\n…\n\n\n This is what allows us to train multi-modal machine learning models"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#sentence-embedding",
    "href": "slides/14-nlp-embeddings.html#sentence-embedding",
    "title": "DAT255: Deep learning engineering",
    "section": "Sentence embedding",
    "text": "Sentence embedding\n\n\n\nExample: Universal sentence encoder\n\n\n\n\n\n\n\nMeasures similariy between on entire sentences\n\nhttps://arxiv.org/abs/1803.11175"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#image-embedding",
    "href": "slides/14-nlp-embeddings.html#image-embedding",
    "title": "DAT255: Deep learning engineering",
    "section": "Image embedding",
    "text": "Image embedding\n\n\n        \n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor images we can’t take the same direct approach:\nIndividual pixels don’t carry semantic meaning, only groups of pixels do.\n\nBut we already know well how to do feature extraction with CNNs\n\n\n Let the (flattened) output of the last Conv layer be the embedding space"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#audio-embedding",
    "href": "slides/14-nlp-embeddings.html#audio-embedding",
    "title": "DAT255: Deep learning engineering",
    "section": "Audio embedding",
    "text": "Audio embedding\nUse CNNs here too:\n\n\n\n\nWhisper (OpenAI)\n\n\n\n\n\n\n\nLanguage model"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#latent-spaces",
    "href": "slides/14-nlp-embeddings.html#latent-spaces",
    "title": "DAT255: Deep learning engineering",
    "section": "Latent spaces",
    "text": "Latent spaces\n\n\nThe output of embedding layers, convolutional layers, an all feature extraction layers, form a latent feature space\n\nThis latent space can contain knowledge applicable to a variety of tasks, allowing for transfer learning:\nTraining a model on one task, but using it (with little modification) on a different task.\n\n\nThis is nice because we can\n\nPretrain on some task that is easy to set up and evaluate\nFine-tune on a task that we actually want to solve, but is difficult to evaluate\n\n(particularly common for language models )"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#pretrained-embeddings",
    "href": "slides/14-nlp-embeddings.html#pretrained-embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Pretrained embeddings",
    "text": "Pretrained embeddings\nEmbedding leaderboard"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#measuring-embedding-quality",
    "href": "slides/14-nlp-embeddings.html#measuring-embedding-quality",
    "title": "DAT255: Deep learning engineering",
    "section": "Measuring embedding quality",
    "text": "Measuring embedding quality\n\n\nEvaluating semantic textual similarity is generally a difficult thing\n\n\n\nFor this, and other NLP model performance measurements, we rely on benchmark tasks:\n\n\n\nVarious classification / clustering / information retrieval problems with human annotated solutions.\n\nsentences1 = [\n    \"The new movie is awesome\",\n ]\n\nsentences2 = [\n    \"The dog plays in the garden\",\n    \"The new movie is so great\",\n    \"A woman watches TV\",\n]\n\n# Compute embeddings for both lists\nembeddings1 = model.encode(sentences1)\nembeddings2 = model.encode(sentences2)\n\n# Compute cosine similarities\nsimilarities = model.similarity(\n  embeddings1, embeddings2\n)\nThe new movie is awesome\n- The dog plays in the garden   : 0.0543\n- The new movie is so great     : 0.8939\n- A woman watches TV            : -0.0502"
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#contextualised-word-embeddings",
    "href": "slides/14-nlp-embeddings.html#contextualised-word-embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Contextualised word embeddings",
    "text": "Contextualised word embeddings\nAfter training, embedding layers are static – a given token is always mapped to the same embedding vector.\nHowever, a single token can have multiple meanings:\n\nYou are right about this\nMake a right turn at the intersection\n\nOur model can infer the correct meaning from context, as long as we treat the input as a sequence."
  },
  {
    "objectID": "slides/14-nlp-embeddings.html#additional-resources",
    "href": "slides/14-nlp-embeddings.html#additional-resources",
    "title": "DAT255: Deep learning engineering",
    "section": "Additional resources",
    "text": "Additional resources\nPretrained models on KerasHub"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#transformers-basic-idea",
    "href": "slides/16-nlp-transformers.html#transformers-basic-idea",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformers: Basic idea",
    "text": "Transformers: Basic idea\n\n\n\nTokenise and embed the words:\n\n“you are right”\n\n\n“turn right here”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntransform! \n\n\ntransform! \n\n\nEmbedding space\n\n\nNew representation"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#transformers-the-basic-idea",
    "href": "slides/16-nlp-transformers.html#transformers-the-basic-idea",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformers: The basic idea",
    "text": "Transformers: The basic idea\n\n\n\nAim:\n\nTake a sequence of vectors of dimensionality \\(\\small N \\times D\\)\nCompute some relation between the vectors\nUse these relations to transform the vectors into new ones (also \\(\\small N \\times D\\))\nNew representation is better suited to solve the task\n\nCritical operation is to incorporate the relation between vectors\n-&gt; the neural attention mechanism"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#self-attention",
    "href": "slides/16-nlp-transformers.html#self-attention",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\nExample sentence: “The train left the station on time”\n\n\n\n\n\n\nRepeatfor all tokens\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#self-attention-1",
    "href": "slides/16-nlp-transformers.html#self-attention-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#self-attention-2",
    "href": "slides/16-nlp-transformers.html#self-attention-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\n\n\nCompute dot product with all other token vectors"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#self-attention-3",
    "href": "slides/16-nlp-transformers.html#self-attention-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\nScale by input length and apply softmax\nSoftmax ensures all attention scores are in the interval [0, 1]."
  },
  {
    "objectID": "slides/16-nlp-transformers.html#self-attention-4",
    "href": "slides/16-nlp-transformers.html#self-attention-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n\n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\nMultiply each token vector by the score, and sum all of them"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#self-attention-5",
    "href": "slides/16-nlp-transformers.html#self-attention-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Self-attention",
    "text": "Self-attention\n\n\nIn (slow) code, we can write it as\n\n\n\ndef self_attention(input_sequence):\n  output = tf.zeros(shape=input_sequence.shape)\n \n  for i, vector in enumerate(input_sequence):\n    scores = tf.zeros(shape=(len(input_sequence), ))\n\n    for j, other_vector in enumerate(input_sequence):\n      scores[j] = tf.tensordot(vector, other_vector, axis=1)\n\n    scores /= np.sqrt(input_sequence.shape[1])\n    scores = tf.nn.softmax(scores)\n\n    new_representation = tf.zeros(shape=vector.shape)\n    for j, other_vector in enumerate(input_sequence):\n      new_representation += other_vector * scores[j]\n\n    output[i] = new_representation\n\n  return output\n\n\nReturn the new vector representation"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#the-query-key-value-model",
    "href": "slides/16-nlp-transformers.html#the-query-key-value-model",
    "title": "DAT255: Deep learning engineering",
    "section": "The query-key-value model",
    "text": "The query-key-value model\n\n\n\nWe can generalise the attention mechanism by using concepts from information retrieval.\nWhat we computed was basically\n\n\n\n\n\n\n\noutput = sum(inputs * pairwise_scores(inputs, inputs))\n\n\nbut we could be doing this with three different sequences:\n\n\n\n\n\n\n\noutput = sum(values * pairwise_scores(query, keys))\n\n\nFor each element in a query, compute how much it is related to every key, and use these scores to weight a sum of values"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#multi-head-attention",
    "href": "slides/16-nlp-transformers.html#multi-head-attention",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#multi-head-attention-1",
    "href": "slides/16-nlp-transformers.html#multi-head-attention-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\nThe parameters of the Dense layers that form the Q, K, V matrices, is where the attention head actually learns something.\n\nIf \\(\\small D\\) is the dimensionality of the embedding space, and \\(\\small N\\) is the number of tokens in our data matrix \\(\\small X\\),\nthis introduces 3 weight matrices \\(\\small W\\), of dimension \\(\\small D \\times D\\):\n\\[\n\\small\n\\begin{align}\nQ &= X W^{(q)} \\\\\nK &= X W^{(k)} \\\\\nV &= X W^{(v)} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#multi-head-attention-2",
    "href": "slides/16-nlp-transformers.html#multi-head-attention-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Multi-head attention",
    "text": "Multi-head attention\n\n\n\n\n\nIn the end, we can write the scaled dot-product self-attention for a single head as\n\\[\n\\small\n\\mathrm{attention}(Q, K, V) = \\mathrm{softmax}\\left[\\frac{QK^T}{\\sqrt{D}} \\right] V\n\\]\nAnd to get the final output from all the attention heads, we simply concatenate the individial outputs.\n\n\n\n\nDeep Learning: Foundations and Concepts, C. Bishop"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#the-transformer-architecture",
    "href": "slides/16-nlp-transformers.html#the-transformer-architecture",
    "title": "DAT255: Deep learning engineering",
    "section": "The Transformer architecture",
    "text": "The Transformer architecture\n\n\n\n\n\nWith the multi-head attention in place, we add a few extra layers to form a block:\n\nA residual connection (Add & Norm) going around the attention layer\nA stack of Dense (Feed Forward) layers\nA residual connection going around the feed forward layers\n\n\n\nThis almost completes our transformer encoder."
  },
  {
    "objectID": "slides/16-nlp-transformers.html#positional-encodings",
    "href": "slides/16-nlp-transformers.html#positional-encodings",
    "title": "DAT255: Deep learning engineering",
    "section": "Positional encodings",
    "text": "Positional encodings\nSo far, the position of each token in a sentencedoesn’t affect the computation of the attention scores.\n-&gt; If we permute the word order, we still get thesame result.\n\nThe food was bad, not good at all.\n\\[\n\\small\n\\neq\n\\]\nThe food was good, not bad at all.\n\n\n\n\n\nTransformer models solve this by encoding token position into the data itself:\ntoken embedding with position = token embedding + position encoding"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#positional-encodings-1",
    "href": "slides/16-nlp-transformers.html#positional-encodings-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Positional encodings",
    "text": "Positional encodings\n\nOption 1: Count token positions, then embed them\n\n\n\n\n\n\n\n\n\n\n\ntoken\nThe\nfood\nwas\ngood\n\n\ntoken embedding\n\\(\\mathbf{x}_1\\)\n\\(\\mathbf{x}_2\\)\n\\(\\mathbf{x}_3\\)\n\\(\\mathbf{x}_4\\)\n\n\ntoken position\n1\n2\n3\n4\n\n\nposition embedding\n\\(\\mathbf{r}_1\\)\n\\(\\mathbf{r}_2\\)\n\\(\\mathbf{r}_3\\)\n\\(\\mathbf{r}_4\\)\n\n\nfinal embedding\n\\(\\mathbf{x}_1 + \\mathbf{r}_1\\)\n\\(\\mathbf{x}_2 + \\mathbf{r}_2\\)\n\\(\\mathbf{x}_3 + \\mathbf{r}_3\\)\n\\(\\mathbf{x}_4 + \\mathbf{r}_4\\)\n\n\n\n\n\n\n\n\nOption 2: Sinusoidal encoding (see textbook p 613)"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#computational-complexity",
    "href": "slides/16-nlp-transformers.html#computational-complexity",
    "title": "DAT255: Deep learning engineering",
    "section": "Computational complexity",
    "text": "Computational complexity\n\nFor each attention head, we introduce \\(\\small 3 \\cdot (D^2 + D)\\) new parameters(The \\(W^{(q)}), W^{(k)}, W^{(v)}\\) matrices)\nThen we add dense layers on top, adding another \\(\\small D^2 + D\\) parameters per layer\nLayer normalisation adds yet another \\(\\small 2\\cdot D\\) parameters\n\n Quickly adds up to a lot.\n\n\n\n\nStill: Computational cost of a forward pass of a transformer model is\n\n\\(\\small \\mathcal{O}(N^2 D)\\) for the attention layers\n\\(\\small \\mathcal{O}(N D^2)\\) for the dense layers\n\nCompare to \\(\\small \\mathcal{O}(N^2 D^2)\\) for a fully-connected dense network\n Transformers are a lot more efficient for large models"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#transformer-model-structures",
    "href": "slides/16-nlp-transformers.html#transformer-model-structures",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformer model structures",
    "text": "Transformer model structures\n\n\nTypically divide by which parts of the original transformer architecture is being used:\n\nEncoder models:\nUseful for sentiment analysis and similar tasks\nDecoder models:\nMost modern language models are this type\nEncoder-decoder models:\nSequence-to-sequence models, useful for translation and for multimodal tasks"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#encoder-transformers",
    "href": "slides/16-nlp-transformers.html#encoder-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Encoder transformers",
    "text": "Encoder transformers\n\n\nInput: Class token and randomly masked sequence\nOutput: Class prediction and completed sequence\nExample: Bidirectional Encoder Representations from Transformers (BERT), 2019\nCan be fine-tuned for various tasks, but:\n\nNot suited for text generation\nTraining is inefficient\n\n\n\n\n\n\n\n\n\n\n\n\n(positive) The &lt;masked&gt; was great (…)\n\n\n(93% positive) The film was great (…)\n\n\nhttps://arxiv.org/abs/1810.04805 https://huggingface.co/docs/transformers/en/model_doc/bert"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#decoder-transformers",
    "href": "slides/16-nlp-transformers.html#decoder-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Decoder transformers",
    "text": "Decoder transformers\n\n\nDecoder-only transformers can be used as generative models\n\n(like GPT: Generative Pre-trained Transformer, 2018)\n\nInput: A sequence\nOutput: Approximate probabilites for the next token in the sequence\nExamples:\n\nGPT (2018), GPT-2 (2019), GPT-3 (2020), …\nLlaMa versions (2023-2024)\nDeepSeek versions (2023-2025)\nAnd most other LLMs – although they differ in implementation and training\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://openai.com/index/language-unsupervised/ https://huggingface.co/docs/transformers/en/model_doc/openai-gpt"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#encoder-decoder-models",
    "href": "slides/16-nlp-transformers.html#encoder-decoder-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Encoder-decoder models",
    "text": "Encoder-decoder models\n\n\n\n\n\nOriginal transformer model was a sequence-to-sequence model\nCombining the encoder and decoder parts relies on cross-attention\nThe cross-attention mechanism is often used in multi-modal models such as speech-to-text or image-to-text"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#section",
    "href": "slides/16-nlp-transformers.html#section",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Yang, Jingfeng, et al. “Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond.” ACM Transactions on Knowledge Discovery from Data 18.6 (2024): 1-32."
  },
  {
    "objectID": "slides/16-nlp-transformers.html#training-decoder-models",
    "href": "slides/16-nlp-transformers.html#training-decoder-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Training decoder models",
    "text": "Training decoder models\nDecoder-type language models are next word predictors.\n\n\nSo we train them to do exactly this:\n\nMask the end of sentences\nUse the next token as the prediction target\nReveal token and move to next one\nContinue until end of text\n\nUsually call the procedure masked attention or causal attention, when model is only allowed to look “backwards”.\nCan train on large, unlabelled data in a self-supervised approach"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nAs for other classification models, last layer looks like\n\nDense(units=vocabulary_size, activation=\"softmax\")\n\n-&gt; output is a vector of softmax scores for all possible tokens.\n\n\n\nRun the model in an autoregressive loop:\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n(Stop if predicting end-of-sequence token)"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-1",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the _____\n\n\nfloor       7.72% bed         6.82% couch       5.70% ground      4.71% edge        4.66%"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-2",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the floor _____\n\n\n,           25.08% and         13.56% .            7.38% of           7.07% with         6.58%"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-3",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-3",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the floor, _____\n\n\nand         9.41% looking     3.23% the         1.78% he          1.63% his         1.48%"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-4",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-4",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\n\nProcess sequence and predict next token\nAppend predicted token to the sequence\nProcess extended sequence and again append predicted token\nRepeat\n\n\nThe cat sat on the floor, and _____\n\n\nthe         8.40% he          5.72% I           4.55% she         3.65% his         3.56%"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-5",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-5",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\n\nMakes model deterministic, always outputs same output for a given input\nOften get stuck in sequence loops"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-6",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-6",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\n\nComputationally expensive\nAlso gets stuck in loops\n\n\n\n\n\n\n\n\narXiv:1904.09751"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-7",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-7",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\nSampling: Use scores as probabilities and sample randomly\n\nWith large vocabularies the results can be nonsensical"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-8",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-8",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\nSampling: Use scores as probabilities and sample randomly\nTop-K sampling: Sample among the K tokens with highest score"
  },
  {
    "objectID": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-9",
    "href": "slides/16-nlp-transformers.html#generating-text-from-a-decoder-model-9",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating text from a decoder model",
    "text": "Generating text from a decoder model\nSampling strategies for next token:\n\nGreedy search: Always select token with highest score\nBeam search: Keep track of several possible branches of output sequences, and select the sentence with highest probability\nSampling: Use scores as probabilities and sample randomly\nTop-K sampling: Sample among the K tokens with highest score\nAdjusted softmax sampling: Add a parameter T called temperature in the softmax function applied to the output:\n\\[\n  \\small\n  y_i = \\frac{\\exp(a_i/T)}{\\sum_j \\exp(a_j/T)}\n  \\]\n\nLow T: sample among best tokens\nHigh T: sample among all tokens (output is more random)"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#general-transformers",
    "href": "slides/18-multimodal-transformers.html#general-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "General transformers",
    "text": "General transformers\nMotivation behind transformers was to process sequential language data\nTurns out they are great general-purpose models\n\nMake very few assumptions about the input data\n\n\nTransformers are now among state-of-the-art for large-scale models on\n\n\nText\nImages\nVideo\nAudio\nPoint clouds\n\n\n\nIn context of LLMs we call this different modalities"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#visual-attention",
    "href": "slides/18-multimodal-transformers.html#visual-attention",
    "title": "DAT255: Deep learning engineering",
    "section": "Visual attention",
    "text": "Visual attention\nBefore the transformer architecture was established, there were still experiments with visual attention (using a combination of CNNs and RNNs with attention)\nCaption generation model, with attention matrix overlaid:"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#visual-attention-1",
    "href": "slides/18-multimodal-transformers.html#visual-attention-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Visual attention",
    "text": "Visual attention\nUse the attention matrix as an explanation tool:"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#visual-attention-2",
    "href": "slides/18-multimodal-transformers.html#visual-attention-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Visual attention",
    "text": "Visual attention\nUse the attention matrix as an explanation tool:\n\n\n\n\n\n\n\n\n\n\n\n    Mistakes :/"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#transformers-for-computer-vision",
    "href": "slides/18-multimodal-transformers.html#transformers-for-computer-vision",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformers for computer vision",
    "text": "Transformers for computer vision\nChallenge for processing non-textual inputs:\n\nHow to define a token?\n\n\nSimplest approach: Each pixel is a token.\n\n\nProblem: Attention matrix is quadratic in number of tokens\n for big images, the matrix becomes huge\n\nTwo common approches:\n\nCut the image into patches\nFirst apply convolutional layers"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#vision-transformer-vit-for-classification",
    "href": "slides/18-multimodal-transformers.html#vision-transformer-vit-for-classification",
    "title": "DAT255: Deep learning engineering",
    "section": "Vision transformer (ViT) for classification",
    "text": "Vision transformer (ViT) for classification\n\n\nSplit the image in \\(\\small N\\) patches: Instead of\n\nheight \\(\\small\\times\\) width \\(\\small\\times\\) colour channels\n\nnumber of tokens, we get\n\n\\(\\small N \\times\\) (patch size)\\(^2\\) \\(\\small\\times\\) colour channels\n\ntokens\n\nFor each patch:\n\nFlatten it to an 1D vector\nMake embeddings\nAdd positional embeddings\n\n Treat it as any generic sequence and input to a transformer encoder"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#vision-transformer-vit-for-classifiction",
    "href": "slides/18-multimodal-transformers.html#vision-transformer-vit-for-classifiction",
    "title": "DAT255: Deep learning engineering",
    "section": "Vision transformer (ViT) for classifiction",
    "text": "Vision transformer (ViT) for classifiction\n\n\n\n\n\n\nhttps://arxiv.org/pdf/2010.11929"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#positional-embeddings",
    "href": "slides/18-multimodal-transformers.html#positional-embeddings",
    "title": "DAT255: Deep learning engineering",
    "section": "Positional embeddings",
    "text": "Positional embeddings\n\n\nStill need to introduce the position of each patch\n\nOption 1: Handcrafted encodings, like the sinusoidal encoding of the original encoder\n\nGets a little complicated and generally does not perform very well\n\n\n\n\n\n\nOption 2: Learned embeddings\n\nInstead of embedding word 1, 2, … \\(\\small N\\), we trivially extend the procedure to embed patch (1,1), (2,1), … (\\(\\small N, M\\))\nkeras.layers.Embedding can be used for any input dimensionality"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#combined-architecture",
    "href": "slides/18-multimodal-transformers.html#combined-architecture",
    "title": "DAT255: Deep learning engineering",
    "section": "Combined architecture",
    "text": "Combined architecture\n\n\n\n\n\n\n\n\n\nhttps://arxiv.org/pdf/2005.12872"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#inductive-biases",
    "href": "slides/18-multimodal-transformers.html#inductive-biases",
    "title": "DAT255: Deep learning engineering",
    "section": "Inductive biases",
    "text": "Inductive biases\nInductive bias: assumption built into the model architecture\n\nLinear models (\\(\\small y = ax+b\\)): Assume data is linear\nCNNs: Assume translational equivariance, hierarchical structure\nRNNs: Assume meaningful ordering\n\n\n\n\n\n\n\n\n\nTransformers: Assume some relation between input tokens, but that’s about it\nGood: General-purpose architecture\nBad: Requires (a lot) more training data than problem-specific models"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#combining-vision-and-text",
    "href": "slides/18-multimodal-transformers.html#combining-vision-and-text",
    "title": "DAT255: Deep learning engineering",
    "section": "Combining vision and text",
    "text": "Combining vision and text"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#audio-transformers",
    "href": "slides/18-multimodal-transformers.html#audio-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Audio transformers",
    "text": "Audio transformers\n\n\nTypical tasks:\n\nSpeech-to-text\nText-to-speech\nSpeech-to-speech\nText-to-music\n\nTwo common ways to process audio data:\n\nWork on raw waveform data\n(Amplitude as function of time)\n\n\n\nConvert waveforms to spectrograms\n(Frequency content as function of time)"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#audio-transformers-1",
    "href": "slides/18-multimodal-transformers.html#audio-transformers-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Audio transformers",
    "text": "Audio transformers"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#audio-transformers-2",
    "href": "slides/18-multimodal-transformers.html#audio-transformers-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Audio transformers",
    "text": "Audio transformers\nSpeech synthesis: Use an example (acoustic prompt) to determine generated style and tone\n\n\n\n\n\n\nhttps://arxiv.org/abs/2407.08551"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#transformers-for-other-uses",
    "href": "slides/18-multimodal-transformers.html#transformers-for-other-uses",
    "title": "DAT255: Deep learning engineering",
    "section": "Transformers for other uses",
    "text": "Transformers for other uses"
  },
  {
    "objectID": "slides/18-multimodal-transformers.html#multimodal-transformers",
    "href": "slides/18-multimodal-transformers.html#multimodal-transformers",
    "title": "DAT255: Deep learning engineering",
    "section": "Multimodal transformers",
    "text": "Multimodal transformers\nTransformer encoder-decoder structure allows for a single model to operate on selveral different types of data\nIf it can be encoded into an embedding space, it can be used as a context for a decoder."
  },
  {
    "objectID": "slides/20-maintenance.html#section",
    "href": "slides/20-maintenance.html#section",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Application deadline April 15"
  },
  {
    "objectID": "slides/20-maintenance.html#section-1",
    "href": "slides/20-maintenance.html#section-1",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "Next Wednesdayat 14.00"
  },
  {
    "objectID": "slides/20-maintenance.html#the-ml-project-lifecycle",
    "href": "slides/20-maintenance.html#the-ml-project-lifecycle",
    "title": "DAT255: Deep learning engineering",
    "section": "The ML project lifecycle",
    "text": "The ML project lifecycle"
  },
  {
    "objectID": "slides/20-maintenance.html#performance-measures",
    "href": "slides/20-maintenance.html#performance-measures",
    "title": "DAT255: Deep learning engineering",
    "section": "Performance measures",
    "text": "Performance measures\nMetrics to evaluate:\n\nQuality (obviously)\nLatency and throughput\nDevelopment and maintenance time\nUsage cost\nCompliance\n\n\n\n\nTensorFlow Serving + Grafana dashboard"
  },
  {
    "objectID": "slides/20-maintenance.html#monitoring-performance",
    "href": "slides/20-maintenance.html#monitoring-performance",
    "title": "DAT255: Deep learning engineering",
    "section": "Monitoring performance",
    "text": "Monitoring performance\nA typical problem:\nConcept drift (and other types of drift) – when evolutions in data cause your model to perform worse over time\n\n\nNew email spam strategies gets around ML-based spam filter\nNew trends break shopping recommender system\n\n\n\n\n\n\n\nDrift detection can be difficult since it relies on testing with new data\n\nDOI: 10.1016/j.eswa.2022.118934"
  },
  {
    "objectID": "slides/20-maintenance.html#static-vs-dynamic-training",
    "href": "slides/20-maintenance.html#static-vs-dynamic-training",
    "title": "DAT255: Deep learning engineering",
    "section": "Static vs dynamic training",
    "text": "Static vs dynamic training\nTwo approaches to model training:\n\n\n\nStatic (offline) training:\nTrain a single model once, deploy it, and keep using it for a while\nPros:\n\nModel can be thoroughly tested\nDeployment needs only be done once\n\nCons:\n\nData must be unchanging over time\n\n\n\n\nDynamic (online) training:\n(Re)-train models continuously when new data comes in, and serve the most recent\nPros:\n\nModel always up-to-date with shift/fluctuations in data\n\nCons:\n\nPotentially high cost of training and deployment cycle\nEach model may not be thoroughly tested"
  },
  {
    "objectID": "slides/20-maintenance.html#ml-irl",
    "href": "slides/20-maintenance.html#ml-irl",
    "title": "DAT255: Deep learning engineering",
    "section": "ML IRL",
    "text": "ML IRL\n\n\n\n\n\nSculley (2015)"
  },
  {
    "objectID": "slides/20-maintenance.html#mlops-machine-learning-operations",
    "href": "slides/20-maintenance.html#mlops-machine-learning-operations",
    "title": "DAT255: Deep learning engineering",
    "section": "MLOps: Machine Learning Operations",
    "text": "MLOps: Machine Learning Operations\nTake\n\nContinuous integration (CI) and\nContinuous delivery (CD)\n\nfrom DevOps, but adapt it to include data processing and add\n\nContinuous training (CT)\n\nNo hard rules on how to organiseyour ML workflow, but worth thinking about.\nThe MLOps philosophy is one (popular) option.\n\n\n\n\nCool but nonsensical diagram"
  },
  {
    "objectID": "slides/20-maintenance.html#mlops-levels",
    "href": "slides/20-maintenance.html#mlops-levels",
    "title": "DAT255: Deep learning engineering",
    "section": "MLOps levels",
    "text": "MLOps levels\n\n\n\nLevel 0: All processes are done manually\nThe obvious starting point for any ML project.\n\nData preparation, model training, validation etc. performed by hand\nMost code is experimental\nModel development and deployment are completely separate\nInfrequent release iterations"
  },
  {
    "objectID": "slides/20-maintenance.html#mlops-levels-1",
    "href": "slides/20-maintenance.html#mlops-levels-1",
    "title": "DAT255: Deep learning engineering",
    "section": "MLOps levels",
    "text": "MLOps levels\n\n\n\nLevel 1: Pipeline automation\nSet up automated processes for data preparation and model training\nAutomatically…\n\nRetrain models when new data is available (CT)\nRun standarised performance tests\nDeploy new model versions\n\nAt level 0 we deployed a final model, while at level 1 we deploy an entire training pipeline."
  },
  {
    "objectID": "slides/20-maintenance.html#mlops-levels-2",
    "href": "slides/20-maintenance.html#mlops-levels-2",
    "title": "DAT255: Deep learning engineering",
    "section": "MLOps levels",
    "text": "MLOps levels\n\n\n\nLevel 2: Add CI/CD pipeline automation\nAllows new model development to be integrated quickly\nRequires more support structure:\n\nIntegration testing\nAdditional model metrics (latency, throughput, …)\nModel metadata store\nAdvanced pipeline orchestration"
  },
  {
    "objectID": "slides/20-maintenance.html#project-work",
    "href": "slides/20-maintenance.html#project-work",
    "title": "DAT255: Deep learning engineering",
    "section": "Project work",
    "text": "Project work\nNext week is project week\nNo lectures – work on the project\n\n\n\n\nHand-in will open on WiseFlow next week"
  },
  {
    "objectID": "slides/22-diffusion.html#section",
    "href": "slides/22-diffusion.html#section",
    "title": "DAT255: Deep learning engineering",
    "section": "",
    "text": "We already did generative deep learning for text generation - key was to sample instead of deterministically picking top prediction.\nWill look at some deep learning methods for converting sampled values into realistic data:\n\nVariational autoencoders (last week)\nGenerative adversarial networks (Today)\nDiffusion models (Today)"
  },
  {
    "objectID": "slides/22-diffusion.html#autoencoders-revisited",
    "href": "slides/22-diffusion.html#autoencoders-revisited",
    "title": "DAT255: Deep learning engineering",
    "section": "Autoencoders revisited",
    "text": "Autoencoders revisited\nTrain a model to reconstruct its input, by\n\nencoding (compressing) data to smaller dimensionality:the latent space (explore in TensorBoard projector)\ndecoding from latent space to original dimensions\n\n\n\n\n\n\nCan randomly sample values in the latent space to generate new outputs\n(although not all sampled value are meaningful)"
  },
  {
    "objectID": "slides/22-diffusion.html#vectors-in-latent-space",
    "href": "slides/22-diffusion.html#vectors-in-latent-space",
    "title": "DAT255: Deep learning engineering",
    "section": "Vectors in latent space",
    "text": "Vectors in latent space\nFor language models we saw that direction was related to concepts / word meaning\nCan recognise the same for images:\n\n\n\n\n\n\nGenerated faces (arXiv:1609.04468)\n\n\n\n Less smile\n\n\nMore smile \n\n\nOpen mouth\n\n\nClosed mouth"
  },
  {
    "objectID": "slides/22-diffusion.html#latent-space-sampling",
    "href": "slides/22-diffusion.html#latent-space-sampling",
    "title": "DAT255: Deep learning engineering",
    "section": "Latent space sampling",
    "text": "Latent space sampling\nCan generalise to different model types\n\n\n\n\n\n\nIn this and next lecture we look at decoders to go from random points in latent space to realistic data\n Sampling strategy is simply to draw from a normal distribution\nDeep Learning with Python, F. Chollet"
  },
  {
    "objectID": "slides/22-diffusion.html#improving-the-autoencoder",
    "href": "slides/22-diffusion.html#improving-the-autoencoder",
    "title": "DAT255: Deep learning engineering",
    "section": "Improving the autoencoder",
    "text": "Improving the autoencoder\n\n\n\nIn practice, autoencoder latent spaces are not great for generating realistic output.\n\n\n\nCan do a better by imposing some requirements on the structure of the latent space:\n Try to learn a distribution rather than a fixed encoding\n\n\n\nThis will lead us to the variational autoencoder (in a bit)"
  },
  {
    "objectID": "slides/22-diffusion.html#learning-distributions",
    "href": "slides/22-diffusion.html#learning-distributions",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning distributions",
    "text": "Learning distributions\nFor ML in general, we usually only to predict a single best value.\nRequires there to be a functional relationship (one-to-one or many-to-one) between the the input features and the target\n\n\n\n\n\n\n\n\n\nNice relationship between x and y\n\n\n\n\n\n\n\nMany possible y values for each x\n\n\n\n\n\n\n● data points\n‒ prediction"
  },
  {
    "objectID": "slides/22-diffusion.html#learning-distributions-1",
    "href": "slides/22-diffusion.html#learning-distributions-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Learning distributions",
    "text": "Learning distributions\nCan free ourselves from the functional assumption by rather modelling the distribution that data came from\nExample: Mixture density network predicting parameters of normal distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this we can sample realistic new data points"
  },
  {
    "objectID": "slides/22-diffusion.html#variational-autoencoder",
    "href": "slides/22-diffusion.html#variational-autoencoder",
    "title": "DAT255: Deep learning engineering",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\nTwist: Let’s model distributions in latent space\nThis is the aim of the variational autoencoder (VAE).\n\n\n\n\n\n\n\n\n\n\nEncode input data into parameters of a probability distribution (for a normal distribution: mean and variance)\nSample a point from this distribution\nDecode this point back to the input format"
  },
  {
    "objectID": "slides/22-diffusion.html#variational-autoencoder-1",
    "href": "slides/22-diffusion.html#variational-autoencoder-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\n\n\nSince encoder and decoder layers are nonlinear transformations, we can go from complicated input distributions to a normal (Gaussian) looking latent space and back\nTo force this latent space distribution, we modify the loss function:\nCombine\n\nreconstruction error (e.g. MSE), and\ndifference between the latent distribution and a normal distribution (measured by KL divergence)"
  },
  {
    "objectID": "slides/22-diffusion.html#variational-autoencoder-2",
    "href": "slides/22-diffusion.html#variational-autoencoder-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Variational autoencoder",
    "text": "Variational autoencoder\n\n\n\n\n\nThe latent space becomes continuous\n We can sample along the different latent space dimensions to combine (or remove) concepts\n\n\n\n\n\nMNIST digits with 2 latent dimensions"
  },
  {
    "objectID": "slides/22-diffusion.html#generative-adversarial-networks",
    "href": "slides/22-diffusion.html#generative-adversarial-networks",
    "title": "DAT255: Deep learning engineering",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\n\n\n\n\n\n\n\n\nGANs: The coolest thing in 2014 (now superseded by diffusion models)\nPopularised by thispersondoesnotexist.com\n\n\n\n\n\nBased on StyleGAN, arXiv:182.04948"
  },
  {
    "objectID": "slides/22-diffusion.html#generative-adversarial-networks-1",
    "href": "slides/22-diffusion.html#generative-adversarial-networks-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\nHave two models work in tandem:\n\nA generator that decodes random values into an image\nA discriminator that takes in images and predicts whether it is real or made by the generator\n\n\n\n\n\n\n\nTrain so that\n\nGenerator gets better at producing realistic output\nDiscriminator gets better at distinguishing fake from real"
  },
  {
    "objectID": "slides/22-diffusion.html#generative-adversarial-networks-2",
    "href": "slides/22-diffusion.html#generative-adversarial-networks-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Generative adversarial networks",
    "text": "Generative adversarial networks\nTraining procedure is a little tricky, see the textbook for details\nWe can still do vector arithmetic in the latent space:\n\n\n\n\n\n\n\n\n\narXiv:1511.06434\n\n\nNote: GANs are well studied in research but seldom used in practice.\nThey are very sensitive to configuration of the training procedure, and are generally just difficult"
  },
  {
    "objectID": "slides/22-diffusion.html#diffusion-models",
    "href": "slides/22-diffusion.html#diffusion-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Diffusion models",
    "text": "Diffusion models"
  },
  {
    "objectID": "slides/22-diffusion.html#diffusion-models-1",
    "href": "slides/22-diffusion.html#diffusion-models-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Diffusion models",
    "text": "Diffusion models\nIdea:\n\nIn steps, add random noise to the original image\nFor each step, train the model to remove the noise\n\n Denoising diffusion probabilistic model (DDPM)\n\n\n\n\n\n\n\n\nCan technically see the diffusion model as a variational autoencoder, but where the encoder process is just adding noise"
  },
  {
    "objectID": "slides/22-diffusion.html#diffusion-models-2",
    "href": "slides/22-diffusion.html#diffusion-models-2",
    "title": "DAT255: Deep learning engineering",
    "section": "Diffusion models",
    "text": "Diffusion models\n\n\n\n\n\n\n\nTo generate new images, we\n\nSample an images of random noise\nRun stepwise denoising\nConverge at a realistic image"
  },
  {
    "objectID": "slides/22-diffusion.html#forward-encoder",
    "href": "slides/22-diffusion.html#forward-encoder",
    "title": "DAT255: Deep learning engineering",
    "section": "Forward encoder",
    "text": "Forward encoder\nWith the noise addition being the central component, let’s have a closer look.\nLike for the VAE, we want a nicely structured latent space were we can create new data just by sampling from a normal distribution.\n\nThe forward process makes a corrupted image \\(\\boldsymbol{x}_1\\) from an initial example \\(\\boldsymbol{x}_0\\):\n\\[\n\\small\n\\boldsymbol{x}_1 = \\sqrt{1-\\beta_1}\\boldsymbol{x}_0 + \\sqrt{\\beta_1}\\boldsymbol{\\epsilon}_1\n\\]\n\n\\(\\beta_1\\) is the variance of the noise distribution (at step 1), \\(\\small\\beta_1 &lt; 1\\)\n\\(\\boldsymbol{\\epsilon}_1\\) is noise\n\n\n\nThe book writes this transformation as a probability distribution \\(q\\) (eq. 17-5):\n\\[\n\\small\nq(\\boldsymbol{x}_1 | \\boldsymbol{x}_{0}) = \\mathcal{N}(\\sqrt{\\beta_1} \\boldsymbol{x}_0, \\beta_1 \\boldsymbol{\\mathrm{I}})\n\\]\nwhere \\(\\mathcal{N}\\) is the normal distribution and \\(\\boldsymbol{\\mathrm{I}}\\) the identiy matrix"
  },
  {
    "objectID": "slides/22-diffusion.html#forward-encoder-1",
    "href": "slides/22-diffusion.html#forward-encoder-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Forward encoder",
    "text": "Forward encoder\nRun the diffusion step \\(T\\) times, with \\(T \\approx 10^3\\)\n\n\n\n\n\n\n\n\n\n\nThe distribution for an arbitrary step \\(t\\) is then\n\\[\n\\small\nq(\\boldsymbol{x}_{t} | \\boldsymbol{x}_{t-1}) = \\mathcal{N}(\\sqrt{\\beta_t} \\boldsymbol{x}_{t-1}, \\beta_t \\boldsymbol{\\mathrm{I}})\n\\]\n\nTrick: Since the sum of normal distributions is also normal, we can write the entire thing from start to \\(t\\) as (eq. 17-6):\n\\[\n\\small\nq(\\boldsymbol{x}_{t} | \\boldsymbol{x}_{0}) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_{0}, (1 - \\bar{\\alpha}_t) \\boldsymbol{\\mathrm{I}})\n\\]\nwith \\(\\small\\bar{\\alpha}_t = (1-\\beta_1) \\cdot (1-\\beta_2) \\cdot \\ldots \\cdot (1-\\beta_t)\\)\nThe factor \\(\\small\\bar{\\alpha}_t\\) is basically the amount of original image remaining"
  },
  {
    "objectID": "slides/22-diffusion.html#noise-variance-scheduling",
    "href": "slides/22-diffusion.html#noise-variance-scheduling",
    "title": "DAT255: Deep learning engineering",
    "section": "Noise (variance) scheduling",
    "text": "Noise (variance) scheduling\n\\(\\small\\bar{\\alpha}_t\\) should start at 1 (no noise) and end at 0 (all noise).\n\nNaïve approach: Change \\(\\small\\beta_t\\) in fixed steps (linear schedule)\nBetter approach: Start adding noise more slowly (cosine schedule)\n\n\n\n\n\n\n\n\n\n\n\n\nLinear\n\n\nCosine"
  },
  {
    "objectID": "slides/22-diffusion.html#reverse-decoder",
    "href": "slides/22-diffusion.html#reverse-decoder",
    "title": "DAT255: Deep learning engineering",
    "section": "Reverse decoder",
    "text": "Reverse decoder\nThe distributions of the forward encoder \\(q(\\boldsymbol{x}_{t} | \\boldsymbol{x}_{t-1})\\) were easy\nBut computing reverse distribution \\(q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_{t})\\) analytically is close to impossible.\nLuckily we can approximate it with a deep learning model:\n\nBuild a model that learns the approximate data distribution \\(\\small p(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_{t}, \\boldsymbol{w})\\) where \\(\\boldsymbol{w}\\) represents the parameters of the model\n\n\n\nThe original DDPM papers used a U-Net type architecture\n\n\n\nModel choice is independent of the diffusion process itself, as long as\n\nOutput dimensions are equal to input dimensions\nWe can (ideally) encode the time step information"
  },
  {
    "objectID": "slides/22-diffusion.html#generating-new-samples",
    "href": "slides/22-diffusion.html#generating-new-samples",
    "title": "DAT255: Deep learning engineering",
    "section": "Generating new samples",
    "text": "Generating new samples\nWith the trained decoder model in place, we can finally generate new (unseen) images:\n\n\n\nStep 0: Draw random pixel values from a normal distribution\nSteps 1 to \\(\\sim\\) 4000:  Sequentially run the decoder to remove noise\n\n\n\n\n\n\n\n\n\n\n\nProperties of diffusion models:\n\nEasy to train\nSuperb quality of results\nSlow to generate new samples"
  },
  {
    "objectID": "slides/22-diffusion.html#modern-diffusion-models",
    "href": "slides/22-diffusion.html#modern-diffusion-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Modern diffusion models",
    "text": "Modern diffusion models\nThe newest and best image generation models are variants of latent diffusion:\nRun the diffusion in latent space, rather than pixel space\nTo construct the latent space, use a variational autoencoder:"
  },
  {
    "objectID": "slides/22-diffusion.html#conditional-generation",
    "href": "slides/22-diffusion.html#conditional-generation",
    "title": "DAT255: Deep learning engineering",
    "section": "Conditional generation",
    "text": "Conditional generation\nSo far we generated data starting from entirely random noise: We do unconditional sampling.\nOutputs are realistic representations of training data\n\nBut we can’t control what output we get.\n\n\nEnter guided diffusion:\nCondition the generation on additional input data (class, text prompt, …)\nGets complicated, but relies mainly on two things:\n\nEncoding the input prompt through e.g. a language model, and concatenating with input to the denoising network\nCross-attention layers allowing denoising network to attend to prompt tokens"
  },
  {
    "objectID": "slides/22-diffusion.html#open-source-diffusion-models",
    "href": "slides/22-diffusion.html#open-source-diffusion-models",
    "title": "DAT255: Deep learning engineering",
    "section": "Open-source diffusion models",
    "text": "Open-source diffusion models\n\n\nHugging Face text-to-image"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#project",
    "href": "slides/old-08-timeseries-cont.html#project",
    "title": "DAT255: Deep learning engineering",
    "section": "Project",
    "text": "Project\nOn Canvas:\n\nLook at the project catalog(unless you decided on topic already)\nAdd your group\n\n\n\n\n\nThursday next week: Project kick-off\n\nWe go through requirements and expectations\nStart planning\n\n\n\n\nWeek after next week: Project work\n\nDecide on topic, data, and models\nGet help/hints/feedback\nGet project approval\nStart working!\n(no ordinary lectures)"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#sequence-prediction-tasks",
    "href": "slides/old-08-timeseries-cont.html#sequence-prediction-tasks",
    "title": "DAT255: Deep learning engineering",
    "section": "Sequence prediction tasks",
    "text": "Sequence prediction tasks\n\n\n\n\nClassification:\n\nSpeech recognition\nFraud detection, network intrusion detection\nFault detection and predictive maintenance\nMedical diagnostics\nSentiment analysis\nTopic classification\n\n\n We already know (most of) the tools needed\n\n\n\n\nForecasting (regression of future values)\n\nPredicting weather, energy prices, stock prices\nText generation\n\n\n Need a model that can remember the past\n\n\n\nSequence-to-sequence learning\n\nLanguage translation\nImage captioning\nText summarisation\n\n\n Need a model that can remember the context"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#recurrent-neural-networks-rnns",
    "href": "slides/old-08-timeseries-cont.html#recurrent-neural-networks-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way:\nLet each node store its previous output"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#recurrent-neural-networks-rnns-1",
    "href": "slides/old-08-timeseries-cont.html#recurrent-neural-networks-rnns-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Recurrent neural networks (RNNs)",
    "text": "Recurrent neural networks (RNNs)\nOur neural networks up until now have no state (can’t remember anything)\nIntroduce a state in the simplest way:\nLet each node store its previous output"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#improved-memory-cells",
    "href": "slides/old-08-timeseries-cont.html#improved-memory-cells",
    "title": "DAT255: Deep learning engineering",
    "section": "Improved memory cells",
    "text": "Improved memory cells\nIn practice, RNNs suffer from vanishing/exploding gradients during training\n Difficult to make them learn long-term dependencies\n\nCan introduce hidden states which are not the same as the output.\n\n\n\n\n\nTwo most used approaces: LSTMs and GRUs."
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#the-long-short-term-memory-lstm-cell",
    "href": "slides/old-08-timeseries-cont.html#the-long-short-term-memory-lstm-cell",
    "title": "DAT255: Deep learning engineering",
    "section": "The long short-term memory (LSTM) cell",
    "text": "The long short-term memory (LSTM) cell\nAdd long-term memory by having two states in each cell:\nA short-term state \\(\\small\\boldsymbol{h}_t\\) and a long-term state \\(\\small\\boldsymbol{c}_t\\)\n\n\n\n\n\nGates determine data flow – add small networks inside the cell to act as gate operators\n\nkeras.layers.LSTM"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#the-gated-recurrent-unit-gru",
    "href": "slides/old-08-timeseries-cont.html#the-gated-recurrent-unit-gru",
    "title": "DAT255: Deep learning engineering",
    "section": "The gated recurrent unit (GRU)",
    "text": "The gated recurrent unit (GRU)\nSimplified and somewhat more effective variant:\n\n\n\n\n\nkeras.layers.GRU"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#stacking-recurrent-layers",
    "href": "slides/old-08-timeseries-cont.html#stacking-recurrent-layers",
    "title": "DAT255: Deep learning engineering",
    "section": "Stacking recurrent layers",
    "text": "Stacking recurrent layers\nAs usual, we can increase the capacity by stacking layers.\nNote when building a deep RNN: Intermediate layers should return the entire sequence\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\nx = layers.GRU(32, return_sequences=True)(inputs)\nx = layers.GRU(32, return_sequences=True)(x)\nx = layers.GRU(32)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#bonus-trick-1-cnn-processing",
    "href": "slides/old-08-timeseries-cont.html#bonus-trick-1-cnn-processing",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #1: CNN processing",
    "text": "Bonus trick #1: CNN processing\nEven with the previous tricks up out sleeve, getting RNNs to learn patterns over &gt;100 time steps is difficult.\nCan extract small-scale patterns with convolutional layers first, then apply recurrent layers:\nmodel = keras.Sequential([\n  keras.layers.Conv1D(filters=32, kernel_size=4, strides=2, activation=\"relu\"),\n  keras.layers.GRU(32, return_sequences=True)\n  keras.layers.Dense(14)\n])\n(add stride &gt; 1 to downsample)"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#bonus-trick-1-cnn-processing-1",
    "href": "slides/old-08-timeseries-cont.html#bonus-trick-1-cnn-processing-1",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #1: CNN processing",
    "text": "Bonus trick #1: CNN processing\nEven with the previous tricks up out sleeve, getting RNNs to learn patterns over &gt;100 time steps is difficult.\nCan extract small-scale patterns with convolutional layers first, then apply recurrent layers\nOr maybe skip the recurrence altogether? WaveNet architecture:\n\n\n\n\n\nkeras.layers.Conv1D(..., padding=\"casual\")  # Look only backwards"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#bonus-trick-2-bidirectional-rnns",
    "href": "slides/old-08-timeseries-cont.html#bonus-trick-2-bidirectional-rnns",
    "title": "DAT255: Deep learning engineering",
    "section": "Bonus trick #2: bidirectional RNNs",
    "text": "Bonus trick #2: bidirectional RNNs\nFor time series we expect the most recent data points to be most important\n Chronological ordering makes sense\n\nSometimes this is not entirely the case - for instance for text\n\n\n\nI arrived by bike.\n\n\n\nIch bin mit Fahrrad angekommen.\n\n\n\n\n\n\nCan process sequences both forwards and in reverse by using a bidirectional recurrent layer:\n\n\n\ninputs = keras.Input(shape=(...))\nx = layers.Bidirectional(layers.LSTM(16))(inputs)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)"
  },
  {
    "objectID": "slides/old-08-timeseries-cont.html#quiz",
    "href": "slides/old-08-timeseries-cont.html#quiz",
    "title": "DAT255: Deep learning engineering",
    "section": "Quiz",
    "text": "Quiz"
  },
  {
    "objectID": "slides/old-10-kickoff.html#project-kickoff",
    "href": "slides/old-10-kickoff.html#project-kickoff",
    "title": "DAT255: Deep learning engineering",
    "section": "Project kickoff",
    "text": "Project kickoff\n\n\n\nToday:\n\nA second look at project topics\nSome cool examples by Vegard and Jobjørn\nDetails about:\n\nProject scope\nRequirements and recommendations\nReport and material"
  },
  {
    "objectID": "slides/old-10-kickoff.html#project-topic",
    "href": "slides/old-10-kickoff.html#project-topic",
    "title": "DAT255: Deep learning engineering",
    "section": "Project topic",
    "text": "Project topic\nUp to you! Be creative but reasonable\n\nWe have one rule: Relevant data must exist and be readily available\n(IRL it never is, but this is not our concern right now)\n\n\n\nand one guideline:\nPick something you find interesting and motivating\n\n\n\nProjects are approved by sending in short description on Canvas, deadline next Friday, 28 Feb"
  },
  {
    "objectID": "slides/old-10-kickoff.html#demo",
    "href": "slides/old-10-kickoff.html#demo",
    "title": "DAT255: Deep learning engineering",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "slides/old-10-kickoff.html#assignment-description",
    "href": "slides/old-10-kickoff.html#assignment-description",
    "title": "DAT255: Deep learning engineering",
    "section": "Assignment description",
    "text": "Assignment description\n… is now on Canvas:\n\n\n\n\n\nNote: The final submission will be on WiseFlow. Details to follow later"
  },
  {
    "objectID": "slides/old-10-kickoff.html#formalities",
    "href": "slides/old-10-kickoff.html#formalities",
    "title": "DAT255: Deep learning engineering",
    "section": "Formalities",
    "text": "Formalities\n\n\nProject counts for 50% of the final grade\nExam counts for the other 50%\nYou need a passing grade on the project in order to take the exam\n\n\n\n\n\nThe deadline is 25 April\n\nSince the project is graded and goes into the strict WiseFlow system, thereis no possibility for an extension. (Zero, 0, none)"
  },
  {
    "objectID": "slides/old-10-kickoff.html#scope-and-expectations",
    "href": "slides/old-10-kickoff.html#scope-and-expectations",
    "title": "DAT255: Deep learning engineering",
    "section": "Scope and expectations",
    "text": "Scope and expectations\n\n\nMandatory to do:\n\nIdentify a use case for deep learning methods\nConstruct and train a deep neural network\nMake an objective evaluation of model performance\n\n\n\nRecommended to do:\n\nCompare different methods and strategies\nMake use of tools for effective optimisation/monitoring/experimentation\nInvestigate quality of the model prediction\n\n\n\n\nVery recommended: Deploy our model as a web app\n\n\n\n\n\nIn short: Show what you’ve learnt in the course"
  },
  {
    "objectID": "slides/old-10-kickoff.html#submission-format",
    "href": "slides/old-10-kickoff.html#submission-format",
    "title": "DAT255: Deep learning engineering",
    "section": "Submission format",
    "text": "Submission format\nWe want to see two things:\n\n\nA report\nMotivate your choices, describe the implementation, document your experiments, and analyse the outcome. Was it a success? What could have gone better?\nA template for the report will be provided.\n\n\n\n\nThe code\nEverything needed to reproduce your results must be put in a GitHub repo and made accessible. The code must be understandable\n\nThe exact evaluation criteria are listed on Canvas"
  },
  {
    "objectID": "slides/old-10-kickoff.html#faq",
    "href": "slides/old-10-kickoff.html#faq",
    "title": "DAT255: Deep learning engineering",
    "section": "FAQ 💬",
    "text": "FAQ 💬\n\n\nCan I copy code from StackOverflow? Yes, but you must always cite your sources\n\n\n\n\nCan I use ChatGPT / Cursor / &lt;some AI tool&gt; for coding and report writing? Yes, but you must always cite your sources\n\n\n\n\nCan I use pretrained models? Yes, a long as you modify and retrain at least one model\n\n\n\n\nCan I use API calls to OpenAI / DeepSeek / &lt;some service&gt; instead of training my own model? No, but you can use it in addition to your own model"
  },
  {
    "objectID": "slides/old-10-kickoff.html#next-week-project-week",
    "href": "slides/old-10-kickoff.html#next-week-project-week",
    "title": "DAT255: Deep learning engineering",
    "section": "Next week: Project week 🙌",
    "text": "Next week: Project week 🙌\n\n\n\n\nNo lectures, work on the project\n\n\n\n\n\nRemember to submit project description\n\n\n\n\n\nHelp / hints / questions:\n\nI will be available both on Zoom and in physical form in lecture & lab hours\nSend me a message to find a time\n\n\n\n\n\n\nUsual lectures again the week after"
  },
  {
    "objectID": "slides/old-10-kickoff.html#remember-to-think-about-your-future",
    "href": "slides/old-10-kickoff.html#remember-to-think-about-your-future",
    "title": "DAT255: Deep learning engineering",
    "section": "Remember to think about your future",
    "text": "Remember to think about your future\n\n\n\nMany good options for doing more advanced deep learning"
  }
]