{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qk0ngwL7ye_"
      },
      "source": [
        "# Weather forecasting\n",
        "\n",
        "This notebook follows Chapter 13 of the textbook. Here we will try to predict the predict the temperature using data recorded at the Weather Station of the Max Planck Institute for Biogeochemistry in Jena, Germany. In this case we are looking at data recorded from 2009 to 2016, but if you also want to try with more updated data (and also additional data sources besides temperature), you can find it here: www.bgc-jena.mpg.de/wetter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C8lIxWXi7yfA"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg5EE8ql7yfB"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOGU3nc17yfC"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_5aNgq-7yfD"
      },
      "source": [
        "Read the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOp1skKe7yfD"
      },
      "outputs": [],
      "source": [
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIMLtEny7yfE"
      },
      "source": [
        "There are 14 features in total. Let's convert them to numpy arrays and extract the temperature to a separate variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cej7b1U97yfE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LopS__907yfF"
      },
      "source": [
        "Check what the temperature looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE4gknuS7yfF"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(temperature)), temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVRGvJiN7yfF"
      },
      "source": [
        "A lot of data points here, so make a plot that focusses on the first 10 days. There is a measurement every 10 minutes, so we get 24 × 6 = 144 data points per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZr-9yDt7yfG"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1440), temperature[:1440])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDzct7_u7yfG"
      },
      "source": [
        "For our our experiments, we use the first 50% of the data for training, the following 25% for validation, and the last 25% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8JraZYs7yfG"
      },
      "outputs": [],
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17T_mV6S7yfG"
      },
      "source": [
        "To prepare the data, we normalise it by subtracting the mean and dividing by the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hih3yov7yfG"
      },
      "outputs": [],
      "source": [
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24flvzGQ7yfG"
      },
      "source": [
        "\n",
        "## Create the datasets to use\n",
        "\n",
        "We’ll use `timeseries_dataset_from_array()` to instantiate three datasets: one for training, one for validation, and one for testing. We’ll use the following parameter values:\n",
        "\n",
        "- `sampling_rate = 6`: Observations will be sampled at one data point per hour: we will only keep one data point out of 6.\n",
        "- `sequence_length = 120`: Observations will go back 5 days (120 hours).\n",
        "- `delay = sampling_rate * (sequence_length + 24 - 1)`: The target for a sequence will be the temperature 24 hours after the end of the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvBobZJ_7yfH"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 6\n",
        "sequence_length = 120\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples,\n",
        ")\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples,\n",
        ")\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFpLt7Wr7yfI"
      },
      "source": [
        "Try out the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFcHpop77yfI"
      },
      "outputs": [],
      "source": [
        "for samples, targets in train_dataset:\n",
        "    print(\"samples shape:\", samples.shape)\n",
        "    print(\"targets shape:\", targets.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDxFAXOm7yfI"
      },
      "source": [
        "## A simplistic model\n",
        "\n",
        "First, for the simplest approach -- 100% autocorrelation from day to day. This means we predict the temperature 24 hours from now will be equal to the temperature right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9_XXnDD7yfI"
      },
      "outputs": [],
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.0\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNdKTTgr7yfI"
      },
      "source": [
        "## A fully-connected (dense) machine learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-0hGAFp7yfI"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Reshape((sequence_length * raw_data.shape[-1],))(inputs)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_dense.keras\", save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"jena_dense.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSGcawhQ7yfI"
      },
      "source": [
        "Define a utility function for plotting the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVCUn3zx7yfI"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curve(history):\n",
        "    loss = history.history[\"mae\"]\n",
        "    val_loss = history.history[\"val_mae\"]\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, \"r--\", label=\"Training MAE\")\n",
        "    plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
        "    plt.title(\"Training and validation MAE\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curve(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQIRRehh7yfI"
      },
      "source": [
        "## A 1D convolutional model\n",
        "\n",
        "Let's see how a straight-up convolutional model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcCNItwB7yfJ"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 12, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 6, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_conv.keras\", save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"jena_conv.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(history)"
      ],
      "metadata": {
        "id": "wL3pjJcFBjst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5gznZkB7yfJ"
      },
      "source": [
        "## Compare to a recurrent model: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4e_ZS5I7yfJ"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(16)(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\", save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"jena_lstm.keras\")\n",
        "print(\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(history)"
      ],
      "metadata": {
        "id": "1A5cOD8GCEZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYnzWSD7yfK"
      },
      "source": [
        "Maybe we need to regularise it. We add `recurrent_dropout`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXbw4OvI7yfK"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"jena_lstm_dropout.keras\", save_best_only=True\n",
        "    )\n",
        "]\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKu-nN0k7yfL"
      },
      "source": [
        "## Stacked LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtZd6Bnd7yfL"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"jena_stacked_gru_dropout.keras\", save_best_only=True\n",
        "    )\n",
        "]\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ4k_Pmt7yfL"
      },
      "source": [
        "## Try a bidirectional LSTM\n",
        "\n",
        "This one processes the input sequences twice: Once in chronological order, and once in reverse order.\n",
        "\n",
        "Does it help in predicting our weather data?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAaKX4387yfL"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Bidirectional(layers.LSTM(16))(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPHAerrV7yfM"
      },
      "source": [
        "## Open exercises\n",
        "\n",
        "1. Adjust the number of units in each recurrent layer in the stacked setup, as well as the amount of dropout. (The current choices are largely arbitrary and probably suboptimal.)\n",
        "\n",
        "2. Try using a stack of Dense layers as the regressor on top of the recurrent layer, instead of a single Dense layer.\n",
        "\n",
        "3. Improve the input to the model: try using longer or shorter sequences or a different sampling rate\n",
        "\n",
        "4. Implement a simple [WaveNet](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/) model -- that is, a fully convolutional network with strides larger than one, and padding set to \"`causal`\", so that the layers can only look backwards in time.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}