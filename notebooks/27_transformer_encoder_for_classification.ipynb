{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB25Vwil-hrj"
      },
      "source": [
        "# Transformer encoder for classification\n",
        "\n",
        "We revisit the IMDb sentiment analysis dataset, but now try out the famed Transformer. Since this is a sequence-to-vector task (and not sequence-to-sequence), we need only one part of the proposed architecture, which is the encoder. We will use the encoder to make (hopefully good) feature subspaces, and put a classification layer on top.\n",
        "\n",
        "\n",
        "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZuCQdi2Nl9s"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTOjXcCC1wNB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import tensorflow_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1OHEya7Yc8"
      },
      "source": [
        "## Load and vectorise the data\n",
        "\n",
        "We load the IMDb movie review data through TensorFlow Datasets, for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jTrTJHJ7Z2z"
      },
      "outputs": [],
      "source": [
        "dataset, info = tensorflow_datasets.load(\n",
        "    'imdb_reviews',\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        "    split=['train[:80%]', 'train[80%:]', 'test']\n",
        ")\n",
        "\n",
        "train_ds, val_ds, test_ds = dataset[0], dataset[1], dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYXvE-NA8T95",
        "outputId": "24d07501-f5c6-4ba5-a55b-9503ba2de7a2"
      },
      "outputs": [],
      "source": [
        "for example, label in train_ds.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgAfCiIT-wQR"
      },
      "source": [
        "Let's vectorise the data in the usual fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PetbKTNj-xUP"
      },
      "outputs": [],
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "data_without_labels = train_ds.map(lambda x, y: x)\n",
        "\n",
        "text_vectorization.adapt(data_without_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH9QM0n2BP8n"
      },
      "source": [
        "Apply the vectorisation layer to the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuHGisnPBSoy"
      },
      "outputs": [],
      "source": [
        "def vectorise(inputs):\n",
        "    x = text_vectorization(inputs)\n",
        "    return x\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4oJIQAOMmL"
      },
      "source": [
        "Batch and prefetch, for performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSO4SsRHC9av"
      },
      "outputs": [],
      "source": [
        "batchsize = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "int_train_ds = int_train_ds.batch(batchsize).prefetch(AUTOTUNE)\n",
        "int_val_ds = int_val_ds.batch(batchsize).prefetch(AUTOTUNE)\n",
        "int_test_ds = int_test_ds.batch(batchsize).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbLojb-7DYy1",
        "outputId": "7ea18a09-0887-442a-d929-dccfc6178038"
      },
      "outputs": [],
      "source": [
        "for x, y in int_train_ds.take(1):\n",
        "    print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt-EywdDDgIA"
      },
      "source": [
        "## Train an LSTM, for comparision\n",
        "\n",
        "Do we really need these Transformers, anyway? Let's train a good old LSTM to form our baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "cbRD0xgEDjZV",
        "outputId": "8aaf5f22-d85f-4dd1-cddc-d22f3dcacb95"
      },
      "outputs": [],
      "source": [
        "lstm_model = keras.Sequential([\n",
        "    keras.Input(shape=(None, max_length), dtype=\"int64\"),\n",
        "    keras.layers.Lambda(lambda x: tf.one_hot(x, depth=max_tokens)),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJiaMG5uO033"
      },
      "source": [
        "Train the model.\n",
        "\n",
        "Let's add a callback to save the best model, and then load it again before we measure the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "E9JN2Q0sDkQB",
        "outputId": "c761b696-ffe4-4d87-9c3a-24a54dcfb068"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"one_hot_bidir_lstm.keras\",\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "lstm_model.fit(\n",
        "    int_train_ds,\n",
        "    validation_data=int_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "lstm_model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "\n",
        "print(f\"Test acc: {lstm_model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYXvPjhk_PiN"
      },
      "source": [
        "## Define our Transformer model\n",
        "\n",
        "For our encoder model we need a couple of different layers:\n",
        "\n",
        "- `keras.layers.MultiHeadAttention`: The critical part, that adds the attention mechanism in parallel \"heads\".\n",
        "- `keras.layers.Embedding`: Embeddings are great, so we will put our attention layer on top of an embedding layer.\n",
        "- `keras.layers.LayerNormalization`: A normalisation layer that will improve the training.\n",
        "- `keras.layers.Dense`: The classic dense layer, which will need to process the output features from the attention layers. Technically, we will say that the `Dense` layers compute a _projection_ of the features.\n",
        "\n",
        "\n",
        "To make it all work in an efficient manner, we subclass the abstract `layer.Layer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdSssYvI_O7W"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        \"\"\"\n",
        "        Initalise our encoder\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim  # Embedding dimensions\n",
        "        self.dense_dim = dense_dim  # Dimensions of the following Dense layer\n",
        "        self.num_heads = num_heads  # Number of attention heads\n",
        "\n",
        "        # The important bit: The MultiHeadAttention layer\n",
        "        self.attention = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # Our projection part: Two Dense layers\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # Normalisation layers, one for each Dense layer.\n",
        "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        The forward computations\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply a mask to ignore padded inputs (if any).\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        # Compute attention\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "\n",
        "        # Compute the rest\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        To save the model, we add a config method.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQy1XrCRBE9f"
      },
      "source": [
        "Cool. Let's instantiate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "68KLz7kyBGjG",
        "outputId": "3794fcda-0ca2-4bee-b43d-3d63344f1e93"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "first_encoder_model = keras.Sequential([\n",
        "    keras.Input(shape=(max_length,), dtype=\"int64\"),\n",
        "    keras.layers.Embedding(vocab_size, embed_dim),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    keras.layers.GlobalMaxPooling1D(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "first_encoder_model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "first_encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSMP9g25BJCJ"
      },
      "source": [
        "Train the encoder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "uCiB-zIHBJvM",
        "outputId": "d79d9386-9d47-42e7-b169-415c731b2e9b"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"transformer_encoder.keras\",\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "first_encoder_model.fit(\n",
        "    int_train_ds,\n",
        "    validation_data=int_val_ds,\n",
        "    epochs=20, callbacks=callbacks\n",
        ")\n",
        "\n",
        "first_encoder_model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "\n",
        "print(f\"Test acc: {first_encoder_model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHdK3ecAS8-X"
      },
      "source": [
        "Hmm. Maybe not entirely great yet?\n",
        "\n",
        "We are missing a vital piece: So far, we are not really treating the inputs as a sequence, but rather just a set.\n",
        "\n",
        "We need a mechanism for adding in the positions of the words in the input text. This mechanism is the _positional encoding_ scheme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXVjsjQUD1-t"
      },
      "source": [
        "## Positional embeddings\n",
        "\n",
        "Let's again create a custom layer, this time doing both the job of creating embeddings, and applying the positional encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es9wO01uD6vZ"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Embeddings for the input tokens\n",
        "        self.token_embeddings = keras.layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim\n",
        "        )\n",
        "        # Positional encoding. Notice the input dimensiond is the sequence length.\n",
        "        self.position_embeddings = keras.layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # For computing the padding mask.\n",
        "        self.not_equal = keras.layers.Lambda(lambda x: tf.math.not_equal(x, 0))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "\n",
        "        # Encode position just as an integer\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        # Embed the positions.\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "\n",
        "        # Embed tokens\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "\n",
        "        # Sum the two\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return self.not_equal(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN8vYBM5D8zc"
      },
      "source": [
        "## Build the final transformer encoder\n",
        "\n",
        "It's time to build and train the complete encoder. This will be almost identical to the previous one, except that we replace the single `Embedding` layer with our new, custom `PositionalEmbedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "CPh-908TD8dD",
        "outputId": "69bde290-17d8-433d-c4df-506fc81647e0"
      },
      "outputs": [],
      "source": [
        "sequence_length = 600\n",
        "\n",
        "second_encoder_model = keras.Sequential([\n",
        "    keras.Input(shape=(max_length,), dtype=\"int64\"),\n",
        "    PositionalEmbedding(sequence_length, vocab_size, embed_dim),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    keras.layers.GlobalMaxPooling1D(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "second_encoder_model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "second_encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOrmJpiwgvPQ"
      },
      "source": [
        "Train it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc5K1DmUgLHF"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"full_transformer_encoder.keras\",\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "second_encoder_model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "second_encoder_model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\n",
        "        \"TransformerEncoder\": TransformerEncoder,\n",
        "        \"PositionalEmbedding\": PositionalEmbedding\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Test acc: {second_encoder_model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmY1jUXdhciR"
      },
      "source": [
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "With the `TransformerEncoder` layer in place, let's go ahead with our deep learning approach and stack several of them. The OG attention paper used 6 transformer blocks in their encoder, but maybe our performance on this dataset maxes out at 2 or 3? Try it out!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
