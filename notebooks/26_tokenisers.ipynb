{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5UnXWX0keOR"
      },
      "source": [
        "# Tokeniser algorithms\n",
        "\n",
        "In this notebook we get a little more serious about tokenisation.\n",
        "\n",
        "While splitting text on whitespaces is a good start to get individial tokens, all modern language models split on subwords. So while \"indivisible\" is written as a single word, GPT-4 will see it as three separate tokens: \"ind\", \"iv\", and \"isible\". To test out how different LLMs implement their tokenisation step, have go at the tokeniser [playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground).\n",
        "\n",
        "If we are to make our own language model, we can either make a custom tokeniser, or use a pretrained one. Let's investigate both options.\n",
        "\n",
        "Modern tokenisers are actually running the text through a pipeline of several steps:\n",
        "\n",
        "1. **Normalisation**: Clean the text by replacing diacritics, accents, and potentially convert to lower-case.\n",
        "2. **Pre-tokenisation**: Do a first split of the text into smaller pieces -- typically whitespace-separated words.\n",
        "3. **Subword tokenisation**: The difficult part -- find good ways to split words into subwords that can be combined in different ways.\n",
        "4. **Post-processing**: Sometimes we want to insert special tokens, like \"start-of-sentence \\[SOS\\]\" or \"end-of-sentence \\[EOS\\]\". This is done in the post-processing step.\n",
        "\n",
        "We can get pretrained tokenisers from several useful Python libraries, such as Keras Hub, TensorFlow Text, or Hugging Face. For this notebook we try the ones provided by Hugging Face [Tokenizers](https://huggingface.co/docs/tokenizers/en/index).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es1IRocdy0mo"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEtFFp0ZyTBW"
      },
      "source": [
        "Download some data. In this case, we use the _Wikitext-103_ dataset, which contains selected articles downloaded from WikiPedi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mBygtf4yShd",
        "outputId": "39aa9c66-0fb9-4687-c5ed-2efd5a9342b5"
      },
      "outputs": [],
      "source": [
        "! wget -N https://wikitext.smerity.com/wikitext-103-raw-v1.zip\n",
        "! unzip wikitext-103-raw-v1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sqi3qYwdVPc"
      },
      "source": [
        "Check what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp5NiKPd0BRr",
        "outputId": "2a4d1563-9bf2-4f93-b5e5-0bc47714537a"
      },
      "outputs": [],
      "source": [
        "! head wikitext-103-raw/wiki.train.raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFYfF_F-2UTO"
      },
      "source": [
        "## Normalisation\n",
        "\n",
        "Depending on which language your data is written in, you might get a lot of \"non-standard\" characters that should be replaced or removed. The approach to do so is standised in a very serious fashion: https://unicode.org/reports/tr15\n",
        "\n",
        "Luckily this is implemented for us in `tokenizers.normalizers.NFD()`. If we want to add multiple normalisation methods, we can do that in a sequential manner, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEIjUFei2Zse"
      },
      "outputs": [],
      "source": [
        "import tokenizers\n",
        "from tokenizers.normalizers import NFD, StripAccents\n",
        "\n",
        "normalizer = tokenizers.normalizers.Sequence([NFD(), StripAccents()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmliGNfW4VD0"
      },
      "source": [
        "Try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xj1DUeQz2UAt",
        "outputId": "8d5862c5-6365-478a-c6f2-185d9fe30c26"
      },
      "outputs": [],
      "source": [
        "normalizer.normalize_str(\n",
        "    'Here is s√¥m√® f√ºnn√ø t·∫Ωxt with b√¥th Norwegi√•n (√∏√¶√•), Greek (Œ¥ŒπŒ±Œ∫œÅŒØŒΩœâ) and Arabic (ŸáŸêÿ¨Ÿéÿßÿ¶ŸêŸäŸë) characters.'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FchNqrN5zUu"
      },
      "source": [
        "## Pre-tokenisation\n",
        "\n",
        "Let's start by splitting text into words. The most common word dividers are of course spaces and newlines, but sometimes we also have contractions of two words that should be split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShNMg7MP5zDk",
        "outputId": "c7660a8f-3014-4e60-bf40-315dbf0af5f5"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "pre_tokenizer = Whitespace()\n",
        "pre_tokenizer.pre_tokenize_str(\"Hey, what's up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwVpaGba7qnk"
      },
      "source": [
        "Note that the `Whitespace` class will also split on punctuation, and returns the positions of each word in the original sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh3svHb179W6"
      },
      "source": [
        "Maybe we also wany to split numbers into separare digits?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecMa7T_R8Fer",
        "outputId": "58ab9f6c-fce5-4e9c-d671-07b486972775"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Digits\n",
        "pre_tokenizer = tokenizers.pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
        "pre_tokenizer.pre_tokenize_str(\"Call 911!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84FWKhAj8FB-"
      },
      "source": [
        "## BPE subword tokenisation\n",
        "\n",
        "Let's first try out the tokenisation algorithm used by OpenAI's GPT models: _Byte-Pair Encoding_ (BPE). For a tutorial (with video) on the details about bytepair encoding for tokenisation, have a look at the Hugging Face NLP [tutorial](https://huggingface.co/learn/nlp-course/en/chapter6/5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xf39Vpg7Q90"
      },
      "outputs": [],
      "source": [
        "gpt_tokenizer = tokenizers.Tokenizer(BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWdiG2tu7SRl"
      },
      "source": [
        "A special thing about this one, is that it builds spaces into the beginning of tokens, in case the token is the start of a word. This is part of the pre-tokenisation -- let's see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZsIX7-N86cJ",
        "outputId": "3b0df8ad-3fc4-483e-d35c-4be818919616"
      },
      "outputs": [],
      "source": [
        "gpt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "gpt_tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test byte-pair encoding pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYRjI9MO8Fny"
      },
      "source": [
        "We see spaces are encoded as the character \"ƒ†\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB0fI0xj9Z5I"
      },
      "source": [
        "Now we can start training the tokeniser on our dataset. The `tokenizers` library comes with a `BpeTrainer` class, where we can set options such as the vocabulary size. Note that the training process can be memory hungry, so while a vocabulary size of 40-50k is common, we dial it down to avoid out-of-memory crashes. You can try to turn it up as far as it goes.\n",
        "\n",
        "The GPT family of models technically don't use any normalisation, but we can add it anyway, just for good measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWhLmatE9aR4"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "\n",
        "\n",
        "gpt_tokenizer.normalizer = tokenizers.normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "\n",
        "gpt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(vocab_size=10000, special_tokens=[\"<|endoftext|>\"])\n",
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"train\"]]#, \"train\", \"valid\"]]\n",
        "gpt_tokenizer.train(files, trainer)\n",
        "gpt_tokenizer.save(\"bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_lfhW-2ls1t"
      },
      "source": [
        "Let's try out the trained tokeniser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F248CW939-4N",
        "outputId": "e324e30f-c1cb-4e79-f642-a5798e1f78f3"
      },
      "outputs": [],
      "source": [
        "output = gpt_tokenizer.encode(\"This is my awesome tokenised text. If it contains emojis, will it still work ü§®?\")\n",
        "print(output.ids)\n",
        "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
        "#gpt_tokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])\n",
        "# \"Hello , y ' all ! How are you ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JtrImFLLm8Rl",
        "outputId": "3fe2efeb-9d8b-449e-847c-3d2f0a66f511"
      },
      "outputs": [],
      "source": [
        "gpt_tokenizer.decode(output.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsM2u9i7wcWe"
      },
      "source": [
        "Try the real decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n9PhPVAywfMD",
        "outputId": "68cfb164-4561-4966-ebe7-e9f17caa6efb"
      },
      "outputs": [],
      "source": [
        "gpt_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "gpt_tokenizer.decode(output.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWaL5W1ayJTn"
      },
      "source": [
        "To visualise how the text is split into subwords, we can use the `EncodingVisualizer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "NWb1lA0YyJlH",
        "outputId": "b932644b-82eb-4ff0-bb0c-8901176ac9b0"
      },
      "outputs": [],
      "source": [
        "from tokenizers.tools import EncodingVisualizer\n",
        "\n",
        "viz = EncodingVisualizer(gpt_tokenizer)\n",
        "viz('Here is some text to visualise. We have some eloquent words, some boring words, and also some typgin eroorrs.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O35pjGs3oSjt"
      },
      "source": [
        "Next, we let you give it a go with the WordPiece algorithm, which was made popular with the BERT family of language models.\n",
        "There is a tutorial and video about this method too: https://huggingface.co/learn/nlp-course/en/chapter6/6.\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Train a WordPiece tokeniser, and make the same visualisation as above, to identify similarities and differences."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
