{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0iRz-Yfg2vq"
      },
      "source": [
        "# Text generation with transformers\n",
        "\n",
        "In this notebook we train a decoder-only model that can run in generative mode, just like modern LLMs. We will however train it on a rather specific type of text -- the IMDb reviews we have been classifiying in the past. Now we will not be classifying anything, but rather generate new reviews.\n",
        "\n",
        "The text generation is an _autoregressive_ process, and there are different strategies one can inplement in order to obtain natural-looking text. We will try to implement several ones, and see how they compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy9F-p_g-gbn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import tensorflow_datasets\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_eV5hVx23Zq"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswTxEPy3Wng"
      },
      "outputs": [],
      "source": [
        "dataset, info = tensorflow_datasets.load(\n",
        "    'imdb_reviews',\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        "    split=['train', 'test']\n",
        ")\n",
        "\n",
        "train_ds, test_ds = dataset[0], dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsERs2LZBnrG"
      },
      "source": [
        "Quick check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtpO9sH53i_F"
      },
      "outputs": [],
      "source": [
        "for example, label in train_ds.take(1):\n",
        "  print('text: ', example)\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIUVMO0S8gdK"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "We need to make som choices on hyperparameters and sequence lengths. You can change these if you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWYLxnoS8iBD"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "sequence_length = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y85jpFbS23Rc"
      },
      "source": [
        "## Text vectorisation\n",
        "\n",
        "The usual process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ7VK79_3qyT"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"([{punctuation}])\", r\" \\1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp497GB03u8t"
      },
      "outputs": [],
      "source": [
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        ")\n",
        "\n",
        "text_only_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "text_vectorization.adapt(text_only_ds)\n",
        "vocabulary = text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNoJ3Cpb_DPO"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "We want our decoder to predict the next token of the input sentence -- hence our labels will be the next true token in the sentence.\n",
        "\n",
        "Create a dataset where the labels are shifted by one position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx6kvlRn_C5L"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_inputs_labels(text, labels):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "\n",
        "    Discard the original labels, which we don't need.\n",
        "    \"\"\"\n",
        "    #text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = text_vectorization(text)\n",
        "    x = tokenized_sentences[:-1]\n",
        "    y = tokenized_sentences[1:]\n",
        "    return x, y\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.map(prepare_lm_inputs_labels, num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(prepare_lm_inputs_labels, num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlQobOobCtTv"
      },
      "source": [
        "Batching and prefetching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaT__swa_J2h"
      },
      "outputs": [],
      "source": [
        "batchsize = 64\n",
        "\n",
        "train_ds = train_ds.batch(batchsize).prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.batch(batchsize).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4GGJq4VCwY-"
      },
      "source": [
        "Verify that the targets are in fact the original sequence, but shifted one position to the right:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI-hsTyd_PvD"
      },
      "outputs": [],
      "source": [
        "for example, label in train_ds.take(1):\n",
        "    print('text.shape:', example.shape)\n",
        "    print('text: ', example[0].numpy())\n",
        "    print('label: ', label[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xaqx-VMv32Py"
      },
      "source": [
        "## Model components\n",
        "\n",
        "We need positional embeddings, and we need a transformer decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdOndVZ636A1"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = keras.layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = keras.layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.not_equal = keras.layers.Lambda(lambda x: tf.math.not_equal(x, 0))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(self.positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        length = input_shape[-1]\n",
        "        self.positions = tf.range(start=0, limit=length, delta=1)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return self.not_equal(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = keras.layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = keras.layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = keras.layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVoov3Ve4AHH"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "We set up our model to output the logits, and not the score after softmax, so that we can add temperature scaling to the softmax later.\n",
        "\n",
        "In this case we need to match out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYO0qdt24Dzk"
      },
      "outputs": [],
      "source": [
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = keras.layers.Dense(vocab_size, activation=None)(x)    # no softmax, apply it later\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"rmsprop\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at_e-RAMEezu"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV5PoPnlEftO"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds, epochs=15, validation_data=test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GhCBxlp3_8p"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG88lfJN6jYb"
      },
      "source": [
        "Approach 1: Select most probable token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2lMWNtE6imZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def most_probable(predictions):\n",
        "    \"\"\"\n",
        "    Return index of the most probable token\n",
        "    \"\"\"\n",
        "    return np.argmax(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC89ZumT8Qm8"
      },
      "source": [
        "Get token indices from vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w_DakT88QIT"
      },
      "outputs": [],
      "source": [
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0tw--GK5_W3"
      },
      "outputs": [],
      "source": [
        "prompt = \"This movie\"\n",
        "generate_length = 50\n",
        "\n",
        "sentence = prompt\n",
        "for i in range(generate_length):\n",
        "    tokenized_sentence = text_vectorization([sentence])[:, :sequence_length]\n",
        "    predictions = model(tokenized_sentence)\n",
        "    next_token = most_probable(\n",
        "        predictions[0, i, :]\n",
        "    )\n",
        "    sampled_token = tokens_index[next_token]\n",
        "    sentence += \" \" + sampled_token\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfWyuVTv23IT"
      },
      "source": [
        "Approach 2:\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Implement top-K sampling.\n",
        "\n",
        "For the sampling itself (after selecting the top token scores), you can use\n",
        "\n",
        "```\n",
        "samples = np.random.multinomial(1, predictions, 1)\n",
        "return np.argmax(samples)\n",
        "```\n",
        "\n",
        "([NumPy docs](https://numpy.org/doc/2.2/reference/random/generated/numpy.random.multinomial.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPnH1BcmKV_q"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeaOAFWe5-Ub"
      },
      "source": [
        "Approach 3:\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span>:\n",
        "\n",
        "Compute token scores using softmax with temperature.\n",
        "\n",
        "The equation is\n",
        "\n",
        "$$\n",
        "y = \\frac{\\exp(a_i / T)}{\\sum_j \\exp(a_j /T)} \\,,\n",
        "$$\n",
        "\n",
        "where $T$ is the temperature, $a_i$ is the logit of the token in question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhaUk9O6MyjY"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EfCIA-v5-rY"
      },
      "source": [
        "Approach 4:\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span> (more difficult)\n",
        "\n",
        "Implement beam search. For this you will need to manage several (let's say 3 to 5) parallel branches of outputs up to a certain length, and then compute the probabilities of each branch, before selecting the most likely one:\n",
        "\n",
        "![](https://d2l.ai/_images/beam-search.svg)\n",
        "\n",
        "For more information about beam search, have a look at https://d2l.ai/chapter_recurrent-modern/beam-search.html, or other sources."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
