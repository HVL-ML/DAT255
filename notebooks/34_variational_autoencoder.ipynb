{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALMh80sXdl9s"
      },
      "source": [
        "# Variational autoencoder\n",
        "\n",
        "In this notebook we try out one of the classic generative deep learning models, the variational autoencoder, and use it to generate images og MNIST-like handwritten digits.\n",
        "\n",
        "The difference from the \"regular\" autoencoder, which we tested in previous notebooks, is that we try to model a _distribution_ in latent space. So instead of associating an image of a the number \"2\" to an exact encoding in the latent space, we acknowledge that \"2\"s can look different, with an average pattern (the mean) and some deviation (the variance). If we can model these two parameters we have a probabilityh distribution, and if we have probability distribution, we know how to _sample_ realistic values. The we can decode, and thereby generate entirely new images.\n",
        "\n",
        "This figure from the Hugging Face [course](https://huggingface.co/learn/computer-vision-course/unit5/generative-models/variational_autoencoders) on variational autoencoders illustrate what we want to achieve:\n",
        "\n",
        "![](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/generative_models/vae.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF-XkEoiELpe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUSG7g4eEaD3"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "First we build the encoder side of the model, which sequentially downsamples the dimensions of data. End the convolution layers with a `Flatten()`, so that we get a single vector of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "26YkOd1hB2s0",
        "outputId": "6011ac2e-9f1a-4ddb-aefe-92bdbe107233"
      },
      "outputs": [],
      "source": [
        "latent_dim = 2  # A very small latent dimension, but easy to visualise.\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = keras.layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = keras.layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "\n",
        "z_mean = keras.layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = keras.layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
        "\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM_OuQIkEcG8"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "The decoder part has to first rearrange the latent space into the shape of an image, and then upsample using `Conv2DTranspose` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOzVCYZNEc4X"
      },
      "outputs": [],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = keras.layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = keras.layers.Reshape((7, 7, 64))(x)\n",
        "x = keras.layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = keras.layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = keras.layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T8xkkoWEgCv"
      },
      "source": [
        "## Latent space sampler\n",
        "\n",
        "The variational bit comes from sampling random values. The plan is to structure the latent space into normal-looking (Gaussian) distributions, which we can sample from using `tf.random.normal`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiCjEkHOEh2X"
      },
      "outputs": [],
      "source": [
        "class Sampler(keras.layers.Layer):\n",
        "    def call(self, z_mean, z_log_var):\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        z_size = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, z_size))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6p2nGwVF_LE"
      },
      "source": [
        "## The complete model\n",
        "\n",
        "Let's piece together the encoder, sampler, and decoder parts. Because of the sampling, we need to customise the training loop, and the most elegant way to do so is to define it as part of the model class itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PDWRVsBGAmN"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sampler = Sampler()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encoder(data)\n",
        "            z = self.sampler(z_mean, z_log_var)\n",
        "            reconstruction = decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"total_loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amf2bMRTGJFO"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFE72eY9GKZJ",
        "outputId": "64306683-3aa7-4e6c-b2ec-6e4eb6c8cadb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n",
        "vae.fit(mnist_digits, epochs=1, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZHOgKKMGNZK"
      },
      "source": [
        "## Sample new images\n",
        "\n",
        "With the model in place, we can start investigating the latent space by picking different location and running the decoder for all of them. Here we sample a grid, so that we can systematically plot along the axes of the latent space. Since we have only two dimensions in the latent space, we can plot dimension 1 along the _x_-axis and dimension 2 along the _y_-axis and have a complete overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pO3ecqeUGM9v",
        "outputId": "43d9d59c-f114-41f5-bcd8-def876c18755"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 30\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "grid_x = np.linspace(-1, 1, n)\n",
        "grid_y = np.linspace(-1, 1, n)[::-1]\n",
        "\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = vae.decoder.predict(z_sample, verbose=0)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[\n",
        "            i * digit_size : (i + 1) * digit_size,\n",
        "            j * digit_size : (j + 1) * digit_size,\n",
        "        ] = digit\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "start_range = digit_size // 2\n",
        "end_range = n * digit_size + start_range\n",
        "pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "sample_range_x = np.round(grid_x, 1)\n",
        "sample_range_y = np.round(grid_y, 1)\n",
        "plt.xticks(pixel_range, sample_range_x)\n",
        "plt.yticks(pixel_range, sample_range_y)\n",
        "plt.xlabel(\"z[0]\")\n",
        "plt.ylabel(\"z[1]\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(figure, cmap=\"Greys_r\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
