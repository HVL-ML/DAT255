{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SslbwP8vxWcI"
      },
      "source": [
        "# Sequence-to-sequence transformer for language translation\n",
        "\n",
        "In this notebook we will try to do _machine translation_ as a sequence-to-sequence task, where we need both parts of the original transformer model -- the encoder as well as the decoder.\n",
        "\n",
        "In this case we need translated sentences that can be used for training data. Short English sentences with their respective translations into may different languages can be downloaded from [Anki](https://www.manythings.org/anki/). You can choose which language to train on yourself -- but maybe choose one you have some understanding of, in case you want to evaluate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTGhglgux9z5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REmcrlELxVR2"
      },
      "source": [
        "## Download data\n",
        "\n",
        "EXERCISE:\n",
        "\n",
        "Use `wget` and `unzip` to download an interesting language file from https://www.manythings.org/anki/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tj3SKD0xf90"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyZK0KqS3QiG"
      },
      "source": [
        "Have a look at the file (insert the correct file name):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3AlkzYa0QiJ"
      },
      "outputs": [],
      "source": [
        "!head <downloaded file>.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGkO4xXn3dy3"
      },
      "source": [
        "We see it contains three tab-separated columns: English, the translation, and an attribution. The last column we discard when reading in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgNBY8vYxjvo"
      },
      "outputs": [],
      "source": [
        "text_file = \"<downloaded file>.txt\"     # Insert correct name\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, translation, _ = line.split(\"\\t\")\n",
        "    translation = \"[start] \" + translation + \" [end]\"\n",
        "    text_pairs.append((english, translation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtLY_1t5xrET"
      },
      "source": [
        "Look at some samples, to verify the data is correctly read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CylX8388xlPi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JaEm7TKxwZY"
      },
      "source": [
        "Now, split the text in test and train sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8-U8aSgxnxk"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTv8cbvmx0sZ"
      },
      "source": [
        "## Vectorise the text\n",
        "\n",
        "Like before we remove punktuation. In case you are using text for a language with different punctuation than English, you should the extra punctuation characters in `strip_chars` below. (Example: If translating Spanish, add the \"Â¿\" character)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1-fjz2Wx2m8"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")      # Don't remove brackets, since we use them for [start] and [end] tokens.\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_translated_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_translated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ5PphmAyFkr"
      },
      "source": [
        "Prepare TensorFlow datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnHU4pXpyG2s"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, tran):\n",
        "    eng = source_vectorization(eng)\n",
        "    tran = target_vectorization(tran)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"translated\": tran[:, :-1],\n",
        "    }, tran[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, tran_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    stran_texts = list(tran_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, tran_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zmhIvuby3X4"
      },
      "source": [
        "Check the shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tS-RKuLy41t"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['translated'].shape: {inputs['translated'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8dUGb2qzQ5k"
      },
      "source": [
        "## Test a GRU-based variant (optional)\n",
        "\n",
        "For comparison, train a bidirectional GRU-based network.\n",
        "\n",
        "This takes a long time, so you don't really have to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOAOimBSzhto"
      },
      "source": [
        "The encoder part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04Z5ujPza3z"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = keras.layers.Bidirectional(\n",
        "    keras.layers.GRU(latent_dim),\n",
        "    merge_mode=\"sum\"\n",
        ")(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnWUjreUzi4F"
      },
      "source": [
        "The decoder part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODdBpB1SzhGm"
      },
      "outputs": [],
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"translation\")\n",
        "x = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = keras.layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "target_next_step = keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1prZFXDezqIg"
      },
      "source": [
        "Train the GRU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXysC7Kfzq_5"
      },
      "outputs": [],
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFlchEpRzt9S"
      },
      "source": [
        "Try out some translations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_YMh0cSzx5q"
      },
      "outputs": [],
      "source": [
        "translation_vocab = target_vectorization.get_vocabulary()\n",
        "translation_index_lookup = dict(zip(range(len(translation_vocab)), translation_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence]\n",
        "        )\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = translation_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MF8jDjEz3UZ"
      },
      "source": [
        "## The sequence-to-sequence transformer\n",
        "\n",
        "Let's first build the **decoder**. There's a good bit to things required, so we just give the entire code, but try to understand the different parts.\n",
        "\n",
        "Unlike the encoder from last week, we will in this case use _causal attention_, meaning that at each step in the sequence, the model can only look backwards, and not forwards, when trying to complete the next token of the translated text. To accomplish this, we create a causal attention _mask_, which is a diagonal matrix where the upper half is 0 and the lower half is 1. This \"removes\" the end of the sentence, but reveals the next token one at a time, for each step in the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsLO47350cqR"
      },
      "source": [
        "### Define the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S9NNEXYz6Xf"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # We use two attentions heads:\n",
        "        # One for self-attention\n",
        "        self.attention_1 = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim\n",
        "        )\n",
        "        # And one for cross-attention (taking in the eoncoder output)\n",
        "        self.attention_2 = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # The dense layer following the attention heads\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # Layer normalisation after each on the three layers\n",
        "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = keras.layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "\n",
        "        # First, get the causal attention mask\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "\n",
        "        # Compute self-attention on inputs\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask\n",
        "        )\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        # Compute cross-attention with encoder outputs\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "\n",
        "        # Dense layer\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZjbDCqt0ghJ"
      },
      "source": [
        "### Define the encoder\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Define a `TransformerEncoder` class.\n",
        "\n",
        "<details>\n",
        "    <summary>Hint</summary>\n",
        "    Check the past notebooks we did\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZciRru8Z_WwO"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx0dvnBHz8OR"
      },
      "source": [
        "### Add positional embedding\n",
        "\n",
        "Her we use what we had from the previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCT0mIAiz-lB"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = keras.layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = keras.layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.not_equal = keras.layers.Lambda(lambda x: tf.math.not_equal(x, 0))\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return self.not_equal(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgcZbn5J0B4f"
      },
      "source": [
        "### Build the complete model\n",
        "\n",
        "We can in principle add multiple encoder and decoder blocks, but first we try a more minimal model.\n",
        "\n",
        "The encoder takes in the English text, while the decoder takes in the translation. The `keras.Input` layer can (as we tried in previous notebooks) select input from a TensorFlow dataset by name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n5P-KjP0AvJ"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"translation\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbZ_1T40EiG"
      },
      "source": [
        "Time to train it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZbmZIsn0Fjr"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyWnEOgF0IUv"
      },
      "source": [
        "## Try translating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCZvNf7T0JZa"
      },
      "outputs": [],
      "source": [
        "\n",
        "translation_vocab = target_vectorization.get_vocabulary()\n",
        "translation_index_lookup = dict(zip(range(len(translation_vocab)), translation_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = translation_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
