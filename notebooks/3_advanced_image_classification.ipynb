{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5YUewyMUIGd"
   },
   "source": [
    "# More advanced image classification\n",
    "\n",
    "In this notebook we will try out some more fancy variations of convolutional networks. First we try training from scratch, then we will have a look at _fine-tuning_ pre-trainined models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bdVYy3edUIGf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZrSJXOjUIGg"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28oY1C65UIGh"
   },
   "source": [
    "For this task we will do binary classification of images containing either cats or dogs.\n",
    "The data are available on Kaggle (as a 786MB zip archive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jU_RdTbYUIGh",
    "outputId": "79e92cbd-19f7-4e12-90f3-b88b28c59cab"
   },
   "outputs": [],
   "source": [
    "!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVPZUT1xUIGi",
    "outputId": "627df259-b4c3-4781-b16b-3bd39f24adc5"
   },
   "outputs": [],
   "source": [
    "!unzip -q kagglecatsanddogs_5340.zip\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kp5BaD6UIGj"
   },
   "source": [
    "Here you should see a folder named `PetImages`, which contains two subfolders: `Cat` and `Dog`.\n",
    "Occationaly we will come across data which is corrupted or otherwise broken, so let's go through all the files and delete those with broken headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyjvRM61UIGj",
    "outputId": "0d2ae318-93ca-47d0-e7e0-b11923373740"
   },
   "outputs": [],
   "source": [
    "num_skipped = 0\n",
    "for folder_name in (\"Cat\", \"Dog\"):\n",
    "    folder_path = os.path.join(\"PetImages\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = b\"JFIF\" in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(f\"Deleted {num_skipped} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo7fiGRjUIGk"
   },
   "source": [
    "Since out data are now a set of files rather than an array of numbers, we have to do an extra step compared to the previous notebooks. Luckily, `keras.utils` contains various convenience functions to help us out. The code below creates TensorFlow `Dataset`s ([`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)), which will lazy-load the image files once they are needed during training.\n",
    "\n",
    "The documentation for the `image_dataset_from_directory` function can be found [here](https://keras.io/api/data_loading/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCYf7MCbUIGl",
    "outputId": "67c12738-2ea0-4bf4-e5c4-abee6bdd1b3d"
   },
   "outputs": [],
   "source": [
    "image_shape = (180, 180, 3)\n",
    "batch_size = 128\n",
    "\n",
    "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H8wfK-mUIGl"
   },
   "source": [
    "Let's visualise the first few images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "3_1EA6b-UIGm",
    "outputId": "c271829a-5ab7-4f2c-b2a9-900017f6427f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eptn3IYmUIGm"
   },
   "source": [
    "### Artificially increase the dataset size by _augmentation_\n",
    "\n",
    "If we rotate an image of a cat by 45 degrees, is it still an image of a cat? We know the answer is yes, but to your neural network, it might not be that obvious. We will now try to improve generalisation by dynamically adding small modifications to the data, which preserves the information of each image but alters the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qADCpzi5UIGm"
   },
   "outputs": [],
   "source": [
    "data_augmentation_layers = [\n",
    "    keras.layers.RandomFlip(\"horizontal\"),      # Randomly flip (mirror) horizontally\n",
    "    keras.layers.RandomRotation(0.1),           # Add a random rotation\n",
    "]\n",
    "\n",
    "\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzHxWRVdUIGm"
   },
   "source": [
    "Pick a single image and apply the augmentation nine times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "_xQziH7vUIGn",
    "outputId": "2c2d1ebe-f883-410b-e1c0-856ecb2dce39"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx0TPFcWUIGn"
   },
   "source": [
    "### <span style=\"color: red;\">Exercise:<span>\n",
    "Change the arguments in the data augmentation (like replace \"horizontal\" with \"vertical\") and see how the images change. Add new types of augmentation by looking at the relevant [Keras documentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jescdc7pUIGn"
   },
   "source": [
    "To apply the augmentation during training, there are two options. The first and the simplest is to make it part of the model -- notice that our `data_augmentation` function contains Keras layers, after all. The second option is to manipulate the TensorFlow `Dataset` objects. We will do the latter, to get some experience with doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P8BYaEbUIGo"
   },
   "source": [
    "Apply the augmentation to each image by using the `map` function, which behaves like Python's `map`, but is parallelised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-t_qmUyLUIGo"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    lambda img, label: (data_augmentation(img), label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqhw-8RPUIGo"
   },
   "source": [
    "Notice the use of `tf.data.AUTOTUNE`, which will set number of parallel jobs to be optimal for our system. Magic!\n",
    "\n",
    "Now another magic trick: Tell TensorFlow to prefetch images and put them in GPU memory, which speeds up things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "23R4YNBfUIGo"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPrG9cGTUIGo"
   },
   "source": [
    "### Build a simple model\n",
    "\n",
    "With the technicalities of data processing in place, we can now build a convolutional network. We start with the sequential model API, which we know already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H7b_-4vtUIGp"
   },
   "outputs": [],
   "source": [
    "sequential_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=image_shape),\n",
    "        keras.layers.Rescaling(1.0/255),    # Standardise the images on-the-fly\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj0t75WEVvJT"
   },
   "source": [
    "We can visualise the model, with layer shapes and all, using `plot_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5RAO-5S2UIGp",
    "outputId": "f383bbe7-2b6a-4f81-ef53-5934b271b16f"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(sequential_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkTVA17CUIGp"
   },
   "source": [
    "#### The functional model API\n",
    "\n",
    "Towards the end of the notebook we will try out models that have complicated layer connections. In this case, the sequential way of building a model (like above) is a too limiting, and we need to work with the _functional_ API.\n",
    "\n",
    "As the name says, we will treat each layer of the network as a function, which operates on the output of a different layer. The code following code defines a model identical to the one above. Just to avoid having too many global variables floating around, let's get used to defining our models in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZgxCxdVRUIGq"
   },
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "functional_model = make_model(image_shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWUboCaJUIGq"
   },
   "source": [
    "Verify that this model does in fact look like the sequential model from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DNP2QmPeUIGq",
    "outputId": "3790b87a-6a69-45ea-ae51-32a46fbd63c6"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(functional_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYT_XnbwUIGq"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Even though we had to do some tricks to the data loading, the training step works with TensorFlow datasets without much fuss.\n",
    "\n",
    "However, since we are now using considerably more data than last time (both bigger images, and a lot more images), training will start to take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5mbmyEBUIGq",
    "outputId": "89b08ce3-3539-45ef-f57d-417fc02790d4"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "functional_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(3e-4),\n",
    "    loss=\"binary_crossentropy\",             # notice we use binary_crossentropy (not categorical_crossentropy)\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "functional_model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH7kZW5gUIGq"
   },
   "source": [
    "Now we can test the model on a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "7jCd_N3HUIGr",
    "outputId": "d23e2c7f-4f5a-4f1b-af38-6ccdda38644d"
   },
   "outputs": [],
   "source": [
    "img = keras.utils.load_img(\"PetImages/Cat/6779.jpg\", target_size=image_shape[:2])\n",
    "plt.imshow(img)\n",
    "\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "predictions = functional_model.predict(img_array)\n",
    "score = predictions[0][0]\n",
    "print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7sS1hxuUIGr"
   },
   "source": [
    "### <span style=\"color: red;\">Exercise:<span>\n",
    "Create a new and improved model using the functional API, which has an additional `Conv2D` layer, followed by a `MaxPooling2D` layer. Remember to insert these at meaningful places in the model!\n",
    "\n",
    "Then train and try out the model like we did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3tROuS8UIGr"
   },
   "source": [
    "### Build a complicated model\n",
    "\n",
    "The code below showcases how the functional API can be used to create non-sequential models with arbitrary connections between layers, using a small version of the [Xception](https://arxiv.org/abs/1610.02357) architecture as an example.\n",
    "\n",
    "This model uses layers and techniques you haven't seen yet, so you are absolutely not expected to understand everything, but it's fun to get to play with the more advanced stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "abYUCIN3UIGs",
    "outputId": "80484acc-4f62-49d3-c93a-a13a531d91d8"
   },
   "outputs": [],
   "source": [
    "def make_xception_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = keras.layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = keras.layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = keras.layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = keras.layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        units = 1\n",
    "    else:\n",
    "        units = num_classes\n",
    "\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(units, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "xception_model = make_xception_model(input_shape=image_shape, num_classes=1)\n",
    "keras.utils.plot_model(xception_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YeIK0KfUIGs"
   },
   "source": [
    "### <span style=\"color: red;\">Optional:<span>\n",
    "\n",
    "Train the network defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQh0z33LUIGs"
   },
   "source": [
    "### Fine-tuning existing models\n",
    "\n",
    "Often we want to solve a rather specific problem (like classifying images of cats and dogs), and we already have access to an existing model that solves a similar or more general problem (like classifying images of other things). In this case it can be very efficient to use the existing model, but change the last layer -- the classification layer -- while keeping the knowledge in the convolutional layers in front. Then we only need to train the last layer (or potentially the couple of last layers), which is known as _fine-tuning_.\n",
    "\n",
    "Loads of high-performing computer vision models are available in [Keras Applications](https://keras.io/api/applications/). Let's pick the full-scale version of the Xception network, and download the pre-trained version _without_ the final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eziBvdPlUIGs",
    "outputId": "0ffa77dc-f21b-4210-ef19-5f9a0edc2089"
   },
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=image_shape,\n",
    "    include_top=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qvcL-k0UIGt"
   },
   "source": [
    "Some technicalities are required when putting together out new model, and these are most likely dependent on the type of pre-trained model we use. Remember to read the documentation! In this case, the relevant docs are listed [here](https://keras.io/api/applications/xception/).\n",
    "\n",
    "First thing is to preprocess the images according to what the model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "s-o1_WeDUIGt"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    lambda img, label: (keras.applications.xception.preprocess_input(img), label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWb5n0NlUIGt"
   },
   "source": [
    "Now we add our new classification layer on top of the pre-trained Xception model, which we will call our `base_model`. An important thing to note here is that we want to keep the existing convolutional layers unmodified -- this is done by setting `training=False` in `base_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qj3HmIbqUIGu"
   },
   "outputs": [],
   "source": [
    "# Specify how the input images look\n",
    "inputs = keras.Input(shape=(image_shape))\n",
    "\n",
    "# Create the base model, with fixed parameters\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Convert the output from the last convolutional layers into an array\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add the final classification layer\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Piece it all together into the final model\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FNjYoI6UIGu"
   },
   "source": [
    "Now let's train the final model on our own data. This is relatively fast since we only need to optimise the parameters of the single classification layer. We say \"relatively\" because the cost of computing the forward pass of a big model is still the same, but training time until convergence is a lot less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HZa0dexUIGu",
    "outputId": "d74b468e-1cf7-4af4-94dc-daa3a7b8f5ac"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(3e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9njoM493UIGu"
   },
   "source": [
    "Do a performance test on the image from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "uqinP9GbUIGv",
    "outputId": "f7903c2f-fe7c-45e3-95ef-c52de780c901"
   },
   "outputs": [],
   "source": [
    "img = keras.utils.load_img(\"PetImages/Cat/6779.jpg\", target_size=image_shape[:2])\n",
    "plt.imshow(img)\n",
    "\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis\n",
    "img_array = keras.applications.xception.preprocess_input(img_array)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = predictions[0][0]\n",
    "print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-UzvX5yUIGv"
   },
   "source": [
    "### <span style=\"color: red;\">Exercise:<span>\n",
    "\n",
    "Try out fine-tuning different models available in Keras, in particular [ResNet50](https://keras.io/api/applications/resnet/#resnet50v2-function) and [EfficientNetB0](https://keras.io/api/applications/efficientnet/#efficientnetb0-function). If you are feeling adventurous, try one of the bigger ones, like those in the [ConvNeXt](https://keras.io/api/applications/convnext/#convnextbase-function) family (but be aware the memory requirements of these ones).\n",
    "\n",
    "Read more about fine-tuning in Keras here: [Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
